{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zJtPCCk6fVqB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This notebook is focused on presenting the process of building a RAG application.**\n",
        "\n",
        "**The goal of the project: to create a RAG application to create a dedicated assistant for a group based on the information exchanged between group members during various conversations. This chatbot should be able to answer questions that are asked in the group.**\n",
        "\n",
        "**This project was written exclusively for Howsam Academy and the main focus was to achieve the desired result in this task.**\n",
        "**Therefore, minor changes may be required for use in another task.**"
      ],
      "metadata": {
        "id": "cmX3JD0VTKdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setting up"
      ],
      "metadata": {
        "id": "UVXqJSIJIVnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Installing the dependencies"
      ],
      "metadata": {
        "id": "NuUgfUPPG3bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install telethon pandas tiktoken scipy openai"
      ],
      "metadata": {
        "id": "-zBjU4_W7l0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 Setting the enviroment variables"
      ],
      "metadata": {
        "id": "WGMASgE17sz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from telethon import TelegramClient\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "TELETHON_API_KEY = userdata.get('TELEGRAM_API_ID')\n",
        "TELETHON_API_HASH = userdata.get('TELEGRAM_HASH_ID')\n",
        "TELETHON_PHONE_NUMBER = userdata.get('TELEGRAM_PHONE_NUMBER')\n",
        "TELETHON_GROUP_ID = userdata.get('TELEGRAM_GROUP_ID')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "uAy1dOdoPJvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3 Setting up the clients"
      ],
      "metadata": {
        "id": "qglm3oi59JO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "openai_client = OpenAI()\n",
        "\n",
        "from telethon import TelegramClient\n",
        "telethon_client = TelegramClient('session0', TELETHON_API_KEY, TELETHON_API_HASH)"
      ],
      "metadata": {
        "id": "lRFjHBFIQcXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Collecting Data"
      ],
      "metadata": {
        "id": "HR3plZiJSHXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re"
      ],
      "metadata": {
        "id": "PF8B0ruA9vV4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "SNU97QlVAFAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Embedding & Text cleaning utils\n",
        "\n",
        "here, there are two functions :\n",
        "- clean_text:\n",
        "  * this function applies the necessary tranformations to a given text.\n",
        "  - such as:\n",
        "    * removing emojies\n",
        "    * removing URLs\n",
        "    * removing extra white spaces\n",
        "\n",
        "- get_embedding:\n",
        "  * this function generates the embedding vector for a given text using OpenAI API\n"
      ],
      "metadata": {
        "id": "SDEHSBeo98IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove emojis\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    # Remove URLs and @mentions\n",
        "    text = url_pattern.sub(r'', text)\n",
        "    # Remove any remaining special characters or excessive whitespace\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = ' '.join(text.split())  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "def get_embedding(text):\n",
        "    response = openai_client.embeddings.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\"  # Choose the model you want to use\n",
        "    )\n",
        "    return response.data[0].embedding"
      ],
      "metadata": {
        "id": "W6IoGkGRAgcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Patterns"
      ],
      "metadata": {
        "id": "CXBNem0J-v3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 1.1.1 Emoji Patterns\n",
        "\n",
        "here, i set the emoji patterns unicode IDs for future to remove them"
      ],
      "metadata": {
        "id": "GhmTXgCp-Ic1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_pattern = re.compile(\n",
        "    u'[\\U0001F600-\\U0001F64F'  # emoticons\n",
        "    u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "    u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "    u'\\U0001F700-\\U0001F77F'  # alchemical symbols\n",
        "    u'\\U0001F780-\\U0001F7FF'  # Geometric Shapes Extended\n",
        "    u'\\U0001F800-\\U0001F8FF'  # Supplemental Arrows-C\n",
        "    u'\\U0001F900-\\U0001F9FF'  # Supplemental Symbols and Pictographs\n",
        "    u'\\U0001FA00-\\U0001FA6F'  # Chess Symbols\n",
        "    u'\\U0001FA70-\\U0001FAFF'  # Symbols and Pictographs Extended-A\n",
        "    u'\\U00002702-\\U000027B0'  # Dingbats\n",
        "    u'\\U000024C2-\\U0001F251'  # Enclosed Characters\n",
        "    ']+', re.UNICODE)\n"
      ],
      "metadata": {
        "id": "AlA-LccN_gzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 1.1.2 URL Pattern\n",
        "\n",
        "i define the url pattern for removing them in future"
      ],
      "metadata": {
        "id": "rGTUZ6SZ-rXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|@\\w+', re.UNICODE)"
      ],
      "metadata": {
        "id": "0fMR6LTrg4yV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Message Crawling"
      ],
      "metadata": {
        "id": "yi4LopybA2-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "U49CftnQBKE1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1 Message fetching function\n",
        "\n",
        "Here, designed this function to crawl information and extract conversations from a public/private telegram group chat. in this case ('دانش آموختگان هوسم').\n",
        "\n",
        "This function uses the group id to access that group chat.\n",
        "This function should:\n",
        "  * iterate through group messages\n",
        "  * extract messages that have atleast one reply\n",
        "  * check the replies of messages that contain atleast 1 reply\n",
        "  * fix the order of the replies coming to that messages.\n",
        "  * save the conversations in a csv file\n",
        "\n",
        "\n",
        "\n",
        "  This is how you can find the group ids:\n",
        "\n",
        "\n",
        "```\n",
        "from telethon import TelegramClient\n",
        "import os\n",
        "\n",
        "TELETHON_API_ID = ''\n",
        "TELETHON_HASH_ID = ''\n",
        "TELETHON_PHONE_NUMBER = '+'\n",
        "\n",
        "client = TelegramClient('session', TELETHON_API_ID, TELETHON_HASH_ID)\n",
        "\n",
        "async def main():\n",
        "    await client.start(TELETHON_PHONE_NUMBER)\n",
        "    print(\"Client started successfully\")\n",
        "    \n",
        "    found_group = False\n",
        "    async for dialog in client.iter_dialogs():\n",
        "        if dialog.is_group:\n",
        "            print(f\"Group: {dialog.name}, ID: {dialog.id}\")\n",
        "            found_group = True\n",
        "\n",
        "    if not found_group:\n",
        "        print(\"No groups found.\")\n",
        "\n",
        "with client:\n",
        "    client.loop.run_until_complete(main())\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pztl4JsGA__9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def fetch_messages():\n",
        "    try:\n",
        "        await telethon_client.start(TELETHON_PHONE_NUMBER)\n",
        "        print('Client started.')\n",
        "\n",
        "        group = await telethon_client.get_entity(int(TELETHON_GROUP_ID))\n",
        "        conversations = []\n",
        "\n",
        "        print(\"Fetching group messages.\")\n",
        "        message_count = 0\n",
        "\n",
        "        async for message in telethon_client.iter_messages(group, limit=50):\n",
        "            try:\n",
        "                if message.text:\n",
        "                    message_count += 1\n",
        "\n",
        "                    sys.stdout.write(f\"\\rFetching message {message_count}...\")\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "                    replies = []\n",
        "\n",
        "                    if message.replies and message.id:\n",
        "                        try:\n",
        "                            async for reply in telethon_client.iter_messages(group, reply_to=message.id):\n",
        "                                try:\n",
        "                                    cleaned_reply = clean_text(reply.text.strip())\n",
        "\n",
        "                                    replies.append({\n",
        "                                        'date': reply.date,\n",
        "                                        'text': cleaned_reply\n",
        "                                    })\n",
        "                                except Exception as e:\n",
        "                                    print(e)\n",
        "                                    continue\n",
        "\n",
        "                            if replies:\n",
        "                                cleaned_message = clean_text(message.text.strip())\n",
        "                                conversation = [f'-> {cleaned_message}']\n",
        "\n",
        "                                replies.sort(key=lambda r: r['date'])\n",
        "\n",
        "                                for reply in replies:\n",
        "                                    conversation.append(f\"-> {reply['text']}\")\n",
        "\n",
        "                                conversations.append(\"\\n\".join(conversation))\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(e)\n",
        "                            continue\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                continue\n",
        "\n",
        "        print(\"\\nAll messages fetched.\")\n",
        "\n",
        "        try:\n",
        "            df = pd.DataFrame({'conv': conversations})\n",
        "            df.to_csv('conversations.csv', index=False)\n",
        "            print('Done')\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "irygUWKuikiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await fetch_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUO97TQd3i35",
        "outputId": "795af5d1-9b40-455e-e41e-1062184175e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client started.\n",
            "Fetching group messages.\n",
            "Fetching message 48...\n",
            "All messages fetched.\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Generating the embeddings"
      ],
      "metadata": {
        "id": "XH6d1p6FESr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i manually generated the embeddings for all the conversations in the data using this code:\n",
        "test_df['embedding'] = test_conv_df['conv'].apply(lambda x: get_embedding(x))\n",
        "test_df.to_pickle('test_embeddings.pkl')"
      ],
      "metadata": {
        "id": "3OrQjfFHOuja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_df = pd.read_csv('conversations.csv')\n",
        "embeddings_df = pd.DataFrame(columns=['embedding'])\n",
        "embeddings_df['embedding'] = conversation_df['conv'].apply(lambda x: get_embedding(x))\n",
        "embeddings_df.to_pickle('embeddings.pkl')"
      ],
      "metadata": {
        "id": "TCphW2v6Edae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 Gathering the Conversations and Embeddings together"
      ],
      "metadata": {
        "id": "1y2FQzLoFIZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.concat([conversation_df, embeddings_df], axis=1)\n",
        "df.to_pickle('conversations_embeddings.pkl')"
      ],
      "metadata": {
        "id": "4dcsdxDMFZaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. RAG"
      ],
      "metadata": {
        "id": "HURJKD_vfaVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0 Set up"
      ],
      "metadata": {
        "id": "SXVjJmsABqzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial # for similarity search\n",
        "import tiktoken # for counting tokens in a string"
      ],
      "metadata": {
        "id": "agVNztzYiqsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = 'text-embedding-ada-002'\n",
        "gpt_model = 'gpt-4o-mini'"
      ],
      "metadata": {
        "id": "0j_SDLUxi8C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Search\n",
        "  Now i'll define a search function that:\n",
        "\n",
        "Takes a user query and a dataframe with text & embedding columns\n",
        "\n",
        "*   Takes a user query and a dataframe with text & embedding columns\n",
        "*   Embeds the user query with the OpenAI API\n",
        "*   Uses distance between query embedding and text embeddings to rank the texts\n",
        "  - Returns two lists:\n",
        "    * The top N texts, ranked by relevance\n",
        "    * Their corresponding relevance scores"
      ],
      "metadata": {
        "id": "XuEDDra6Bwgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strings_ranked_by_relatedness(query: str, df: pd.DataFrame, relatedness_function = lambda x, y: 1 - spatial.distance.cosine(x, y), top_n: int = 3):\n",
        "  q_embedding = openai_client.embeddings.create(\n",
        "      input = query,\n",
        "      model = embedding_model\n",
        "  ).data[0].embedding\n",
        "\n",
        "  strings_and_relatedness = [\n",
        "      (row['conv'], relatedness_function(q_embedding, row['embedding']))\n",
        "      for i, row in df.iterrows()\n",
        "  ]\n",
        "\n",
        "  strings_and_relatedness.sort(key = lambda x: x[1], reverse = True)\n",
        "\n",
        "  strings, relatednesses = zip(*strings_and_relatedness)\n",
        "\n",
        "  return strings[:top_n], relatednesses[:top_n]"
      ],
      "metadata": {
        "id": "xKxBI575khw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# strings, relatedness = strings_ranked_by_relatedness('دوره بینایی کامپیوتر', df)\n",
        "# for stri, relatei in zip(strings, relatedness):\n",
        "#   print(f\"{relatei=:.3f}\")\n",
        "#   display(stri)"
      ],
      "metadata": {
        "id": "8i4a78zY0cFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Ask\n",
        "Below, i define a function ask that:\n",
        "\n",
        " * Takes a user query\n",
        " * Searches for text relevant to the query\n",
        " * Stuffs that text into a message for GPT\n",
        " * Sends the message to GPT\n",
        " * Returns GPT's answer"
      ],
      "metadata": {
        "id": "HXZt3Ab_DGIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "zIaFoLbFD6q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.1 Counting Tokens"
      ],
      "metadata": {
        "id": "XUydVt_RD61a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens(text: str, model: str = gpt_model):\n",
        "  \"\"\"\n",
        "  This function returns the number of tokens in a text.\n",
        "  \"\"\"\n",
        "  encoding = tiktoken.encoding_for_model(model)\n",
        "  num_tokens = len(encoding.encode(text))\n",
        "  return num_tokens"
      ],
      "metadata": {
        "id": "uJ0GAtey4Q2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.2 Making the input prompt ready for GPT"
      ],
      "metadata": {
        "id": "vAJHCBDID_u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_message(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str,\n",
        "    token_budget: int\n",
        "):\n",
        "  \"\"\"\n",
        "  This returns a message ready for GPT.\n",
        "  \"\"\"\n",
        "  strings, relatedness = strings_ranked_by_relatedness(\n",
        "      query,\n",
        "      df\n",
        "  )\n",
        "\n",
        "  introduction = 'Use the below conversation to answer the subsequent question. If the answer cannot be found in the text below, write \"I could not find an answer.'\n",
        "\n",
        "  question = f'\\n\\nQuestion: {query}'\n",
        "\n",
        "  for string in strings:\n",
        "    next_conv = f'\\n\\nConversation: \"\"\"{string}\"\"\"'\n",
        "\n",
        "    if (\n",
        "        num_tokens(introduction + next_conv + question, model=model)\n",
        "        > token_budget\n",
        "    ):\n",
        "      break\n",
        "    else:\n",
        "      introduction += next_conv\n",
        "\n",
        "  return introduction + question"
      ],
      "metadata": {
        "id": "9if1qjfS90hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3 Ask"
      ],
      "metadata": {
        "id": "XUuW0O6PEKjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    model: str = gpt_model,\n",
        "    client: openai.OpenAI() = openai_client,\n",
        "    token_budget: int = 5000,\n",
        "    print_message: bool = False,\n",
        "):\n",
        "  \"\"\"\n",
        "  Answers a query using GPT and a df of relevant text and embeddings\n",
        "  \"\"\"\n",
        "\n",
        "  message = query_message(query, df, model, token_budget)\n",
        "\n",
        "  if print_message:\n",
        "    print(message)\n",
        "\n",
        "  messages = [\n",
        "      {'role':'system', 'content': 'You answer question in farsi about The Howsam Academy, An AI Academy with multiple courses. you will use the info you will be given to answer questions.'},\n",
        "      {'role':'user', 'content': message},\n",
        "  ]\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "    model = model,\n",
        "    messages = messages\n",
        "  ).choices[0].message.content\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "t4JYzvQH91xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask('دوره جدید بینایی کامپیوتر چه زمانی اماده میشه', df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AniOFmhC-pQ1",
        "outputId": "765078e9-9e40-4a22-a39a-eb9a36f1646d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'دوره جدید بینایی کامپیوتر هنوز زمان دقیقی برای آماده شدن ندارد، اما استاد اشاره کرده\\u200cاند که در حال حاضر درگیر دوره پردازش تصویر هستند و امیدوارند که بتوانند به زودی از آن شروع کنند.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    }
  ]
}