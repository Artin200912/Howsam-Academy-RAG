conv
"-> سلام دوستان vpn خوب پولی هم باشه اکیه سراغ دارید
-> سلام"
"-> سلام دوستان عزیز ممنون میشم اگه بتونید منو راهنمایی کنید من میخوام برای تصاویر ماهواره ای یک دیتاستی تشکیل بدم و این عکس ها رو طوری آموزش بدم که بتونم به یک سری وزنهای بهینه برسم کاری که imagenet کرده آیا باید عکس ها رو دستی Anotate کنمتصاویر ماهوارهای پر از جزییاته و خیلی عکسها رو به سختی میشه anotate کردلیبل گذاری ممنون از وقت و توجه شما
-> سلام حس میکنم ساختن یک شبکه عصبی و آموزش دادن اون روی یک دیتاست معمولی رو تجربه نکردید اگه درست گفتم پیشنهادم این هست که قبل از اینکه پروژه خودتون رو جلو ببرید حتما یک شبکه عصبی مثلا کانولوشن بسازید و روی یک دیتاست عمومی آموزش بدید درمورد تهیه دیتاست و لیبل گذاری هم باید هدف انجام کار مشخص باشه احتمالا منظورتون سگمنتیشن هست بازهم برای اینکه مشخص بشه که نیاز به لیبل گذاری هست یا نه باید به روشهای مرجعتون نگاه کنید اونها چیکار کردن کلا خوب نیست بدون روشهای مرجع و پایه مسیر تهیه دیتاست و انجام پایان نامه طی بشه
-> درست حدس زدید من مبتدی هستم ولی دو سه تا پروژه با شبکه عصبی انجام دادم از من برای بهتر شدن کارشون خواستن یک دیتاست جدید با لیبل ایجاد کنم تا با دقت بیشتری در پروژه هاشون استفاده کننددیتاست عمومی به اندازه کافی جوابگوی کارشون نیست هدف کار هم بله سگمنتشن هست و در حال حاضر سمنتیک سگمنتشن"
"-> سلام ممنون میشم دوستان سینیوری که تجربه دارند به این سوالم پاسخ دهند بنده با توزیع های مختلفی از لینوکس کار کردم ابونتو مانجارو کالی مینت دبیان هر کدوم نقطه قوت و ضعف هایی داشتتند یکی کند بود یکی کامیونیتی ضعیفی داشت یکی مدام اپدیت میداد و ولی من دنبال پایداری و stability بهتر برای کارهای ماشین لرنینگ و کلا به طور خلاصه هوش مصنوعی هستم به نظر شما کدام توزیع لینوکس با توجه به ویژگی هایی که برشرمدم بهتره و سازگاری بهتری داره همچنین استیبل تر هست برای کارهای پروداکشن
-> در واقع برای همه دولوپ شده برای همه توزیع ها چه ارچ کار کنید چه اوبونتو چه اوپن سوزه اوبونتو همه استفاده میکنن کامیونیتی قوی ای داره ولی توزیعی مثل ارچ کار هر کسی نیست تمامی کانفیگا باید دستی انجام بشه حداقل علم lpic1 نیاز داره برای ماشین لرنینگ بکند و توصیه اوبونتوعه دبیان جالب نیست اصلا
-> "
"-> سلام استاد خسته نباشید وقتتون بخیر ببخشيد مزاحمتون شدم استاد ببخشید من ی پروژه ای دارم که باید بر اساس دیتای ورودی از دوربینو لیدار تصویر سه بعدی محیط رو بسازم میخواستم بدونم برای این کار باید چه کلید واژه ای سرچ کنم این 3d image reconstruction میشه
-> سلام آره البته بهتر هست بخش 3d رو در papers with code نگاه کنید
-> از آموزشهای هوسم چه اونایی که تاحالا اومده و چه اونایی که بعدا قراره بیاد موردی هست که مستقیما در ارتباط باشه با این موضوع ی مورد دیگه اینکه تو این حوزه به معنای واقعی صفر کیلومترم چه پیش نیازی باید داشته باشم به نظرتون
-> فصل سهبعدی دوره بینایی کامپیوتر مقاله باید بخونید
-> سلام استاد وقتتون بخیر سوال این دوست عزیز سوال من هم هست علیرغم دوره های عالی که برگزار کردیدالبته تموم نکردم هم یادگیری ماشین هم عمیق اواسط دوره هستم هنگام مقاله خوندن اکثر مطالبش رو نمیفهمم برای کسی که بخواد به صورت آکادمیک وارد بینایی ماشین بشه چه ریاضیاتی باید گذرونده باشه و اینکه مسیرش بعد از ماشین لرنینگ به چه شکل هست به طور مثال اول شبکه های نورونی یا یادگیری عمیق یا مستقیم خود بینایی ماشین
-> 
-> استاد ساخت کورسش دست خودتونو میبوسه
-> بدم نمیاد وقت ندارم
-> سلام وقت بخیر استاد من چندتاسوال دارم چطور میتونم وارد اتاق گفتگو بشم باهاتون
-> "
"-> دوستان لطف کنن به جای شوخی و مسخره اگه جوابو بلدن جواب بدن
-> دیگه همه داریم جواب میدیم دیگه"
"-> من قبلا تو آناکوندا ازش استفاده میکردم ولی آناکوندا خیلی سنگینه نمیدونم چطوری توی vscode ازش استفاده کنم اصلا میشه جداگانه نصب بشه
-> بله میشه باید اکستشن ژوپیتر رو توی vs code نصب کنید"
"-> اصولا استفاده از ژوپیتر توی vscode کار اشتباهیه
-> اصلا خیلی رابط کاربری بهتری بهتون میده نسبت ب خودش من از روز اول داخل vscode با ژوپیتر کار کردم خیلی فضای بهتری داره
-> نه نیست سرچ کنید vscode خیلی راحت فایلشو دانلود و نصب کنید
-> خیلی کاربرد های زیادی داره vscode اتفاقا خوب هم هست کار کنید باهاش بحثی که شد شوخی های متداوله درباره ابزار هاست"
"-> سلام دوستان من یه سوال پایتونی دارم آیا میشه توی vscode با ژوپیتر لب کار کرد
-> سلام بله میشه نوتبوک در وی اس کد باز کرد فک کنم باید افزونش رو نصب کنید
-> بله کاملا شدنیه وقتی هم می خواید فایل بسازید مثلا اگر اسم فایل نوتبوک A هست کافیه به جای Apy بنویسید Aipynb خودش براتون نوتبوک می سازه
-> دلم برای vscode سوخت
-> چرا بوگاتی در نقش تراکتور
-> همه میگن نسبت به ابزارها بدون تعصب باشید اما واقعا vscode بوگاتی نیست
-> بوگاتی نباشه درحد که شاسی هست بدبخت
-> شاسی در نقش فرغون بگین بهتره
-> اصلا متوجه شوخیهای بعدش نشدم"
"-> سلام آقای دکتر خروجی من تو ایپاک اخر هستش حس میکنم دارم یه اشتباه به شدت مسخره میکنم
-> سلام به جای اینکه لاس تست رو جمع کنی ریپلیس کردی به جای باید بذاری"
"-> فقط یک پیشنهادی که دارم از برنامه ای جز اسپات پلیر برای انتشار استفاده کنید چون هربار میخوام از اسپات استفاده کنم باید تمام سیستم رو ایزوله کنم وگرنه بخاطر باز بودن برنامه هایی که اصلا نمیدونستم روی سیستم وجود دارن دائما بسته میشه
-> دقیقا و یک مشکل عمده برای ما لینوکس کار ها و کسایی که عاشق دنیای اپن سورس هستند اینه که اسپات پلیر برای لینوکس توسعه داده نشده باور کنید اگر پروژه کارشناسی و برنامه هایی که روی اسپات پلیر داشتم که اکثرش برای هوسم هست نبود یک لحظه هم روی ویندوز نبودم
-> دقیقا"
"-> سلام دوستان لطفا نظر بدید همیشه من تاکید زیادی رو کگل برای افزایش مهارت و تقویت رزومه داشتم مدتی هست که تیم ما روی یک مینی دوره آشنایی با کگل کدنویسی یک مسابقه کگل وقت گذاشته الان به مرحلهای رسیده که میتونیم جمعبندی کنیم و کار ضبط رو شروع کنیم احتمالا حدود 10 ساعت آموزش میشه قبل از ورود به ضبط میخواستیم نظر شما رو بدونیم آیا شما چنین آموزشی رو مناسب میدونید فکر میکنید بدردتون میخوره نظرتون برای ما مهم هست
-> سلام وقت بخیر آقای دکتر قطعا من که به شخصه استقبال می کنم و چقدر خوب که همچین محتوای با ارزش و کاربردی ای تولید بشه من حتی پیشنهاد می کنم اگر میشه گیتهاب و کلا نحوه ساخت پروژه از صفر تا صد روی گیتهاب و نکات کاربردی مربوط به آن هم باشه حالا چه با این دوره چه به صورت دوره ای مجزا
-> سلام استاد وقت بخیر برای من بسیار عالی هست چون رشته و کارم در این حوزه نبوده آشنایی با کگل ندارم و همیشه برام سوال بوده که دقیقا چی هست جدای از اون الان تازه کمی یادگیری ماشین کار کردم نمیدونم کی باید شروع به ورود به کگل کنم متشکرم از اموزش های خوبتون
-> خیلی خوبه سعی بشه که مسابقه ی باشه به فضای واقعی کار نزدیک باشه خیلی بهتره
-> چیزی که من دیدم اینه که هر کسی میتونه با انجام یک پروژه کگل و گذاشتن کدش تو گیتاب شخصی خودش به چند تا هدف برسه یک پروژه رو از ابتدا تا انتها انجام بده هر چند این پروژه با پروژه هایی که در عمل باهاش روبرو میشیم چالشهای عملیاتی کمتری داره اما از ابتدا تا انتها بردن یک پروژه خیلی دید آدم رو گسترش میده روی تمیز کد نوشتن کار کنه این کد رو داره توی ویترین خودش میگذاره پس ناگزیر لازمه کد تمیزی بنویسه همین روند توانایی کدنویسی شخص رو بهبود میده مهارت تکنیکی و بعضی از مهارتهای نرم خودش رو نشون بده در مورد رزومه شدن اینو بگم که خیلی وقت ها موقعی که من با کسی مصاحبه میکنم اگه کدی که شخص خودش نوشته رو ببینم و ببینم که تمیزه حداقل نصف نمره مصاحبه رو بهش میدم چرا چون از روی کامیتهای کد میشه فهمید که یک نفر گیت رو میشناسه یا فقط یه چیزی زده بیاد بالا چون از روی فولدر بندی پروژه میشه فهمید یک نفر منظم کار میکنه یا نه چون از روی انتخاب اسم متغیرها میشه حدسهایی در مورد نحوه تعامل با افراد زد چون با دیدن کدی که یک نفر روی یک پروژه کگل زده میشه تا حد خوبی تشخیص داد که شخص مفاهیم پایهای یادگیری ماشین رو میدونه یا نه مثلا از جدا کردن داده های train و test از انتخاب معیار ارزیابی از انجام ارزیابی در انتهای پروژه از مراحلی که برای تحلیل و بررسی داده طی میکنه و در آخر اگه پروژه های خوبی روی گیت گذاشتید میتونید پروژههای همدیگه رو بخونید و به هم دیگه ستاره بدید
-> سلام سعید جان ممنونم خوشحال شدم پیامت رو دیدم دوستان آقای سعید کریمی یکی از دوستان قدیمی من هستن که سالهاست در حوزه هوش مصنوعی کار میکنن
-> سلام اقای دکتر وقت بخیر این اموزش به درد بنده میخوره و اخیرا دنبالش بودم"
"-> سلام دوستان از چه ابزاری میشه برای رسم شبکه های عصبی که توشون ترنسفورمر و اتنشن داره استفاده کرد به خصوص با لاتکس
-> Tikz drawio visio"
"-> سلام وقت بخیر ببخشید من به راهنمایی شما نیازمندم با توجه به سرفصل های دوره دیپ لرنینگ کدام فصل ها پیش نیاز پردازش تصویر هست و اگر بخوام وارد nlp بشم کدام فصل ها مهم هستن شاید سوال ابتدایی باشه اما بسیار به توصیه و راهنماییتون نیازمندم سپاس از توجهتون
-> بنظر من همه رو باید ببینید جلسات خیلی بهم مرتبط هستند و اصلا از جایی نمیشه وارد وسط مباحث شد
-> چشم بسیار ممنونم از راهنماییتون گفتم شما که تجربه دارین شاید قسمتی نیاز نباشه پس اگر همشون مرتبطن حتما همینکار میکنم
-> خواهشمندم بله بهترینش همینه از اول شروع کنید
-> سلام نه فصل اول رو در هر صورت ببینید
-> مرسی از راهنماییتون"
"-> سلام ظهر بخیر استاد یه راهنمایی میتونم ازتون بگیرم یه سری دستورا که کدهای گیت رو میتونیم بیاریم داخل کولب رو شما داخل پروژه ها استفاده میکنین مثل git clone pwd این دستورا رو بخایم بیشتر بفهمیم شما سورسی سراغ دارین یا مثلا چی سرچ بزنیم بنظرتون بهتره تا کمک کننده باشه سپاس
-> سلام باید کمی لینوکس و کار با ترمینال لینوکس رو یاد بگیرید البته خیلی کم درحالت ساده به چت جی پی تی بگید لیستی از دستورات مهم رو همراه با کارکردشون بهتون میده وبلاگ و آموزش هم که زیاد هست
-> خیلی ممنونم"
"-> سلام وقت بخیر دوستان کتابی برای کامپیوتر ویژن پیشرفته میشه معرفی کنید
-> سلام ریچارد شلیسکی برای چه کاری میخواید خوندن این کتابها کار هرکسی نیست
-> ممنون از معرفی کتاب برای آمادگی بیشتر برای پایان نامه ام
-> پس احتمالا کتاب به کارتون نمیاد بیشتر روی استراتژیتون فکر کنید"
"-> سلام وقتتون بخیر دوستان کسی میدونه وقتی کدی رو میخوایم اجرا کنیم و زمان بر و سنگین هست چه راهی میشه بکار برد که موقع قطع و وصل نت نخواد کد از اول اجرا بشه
-> سلام اگه با ژوپیتر اجرا کنین که قطع نمیشه کلاتو کولب هم تا وقتی کرش نکنه یا gpu رو نگیره از اول اجرا نمیشه فقط موقع قطع و وصل نت یه پاوس داره انگار بعد نت که اومد خودش ادامه میده
-> اهان ممنونم"
"-> سلام خداقوت دوره بینایی ماشین پیشرفته چه زمانی شروع میشود
-> سلام پاییز
-> یکی سوال شرعی درمورد بینایی پرسیده بود ولی حذف کرد تغییرات دوره بینایی جدید نسبت به قبلی تفاوتش زیاده چون حوزه بینایی از دوره قبل تا این دوره تغییر و تحول بزرگی داشته ضمن اینکه استایل ما هم در ارائه مطالب تغییر و تحول داشته
-> همچنان اون حالتی که الان داره رو حفظ میکنه و پروژه های سطح بالاتر رو میزنیم یا پروژه ها در راستای اموزش قرار میگیرن مثه چیزی که الان تو پردازش تصویر هست
-> به نظرم یه ربات تلگرام بسازیم و یه مدل زبانی هم بذاریم واسش که خودش بیاد و سوالات تکراری رو جواب بده
-> یادم میمونه حداقل برای فان
-> رو من حساب کنین
-> مثل پردازش تصویر میریم جلو دیگه پروژههای سطح بالا رو میبریم توی ویژن کاتالیست چون دفعه قبلی عدهای میگفتن کدها براشون سخته
-> به به حتما همچنین
-> اتفاقا کاربردی هست از توی تاریخچه چت گروه میشه سوال هایی که زیاد تکرار شدن رو پیدا کنیم
-> حالا اینکه شوخی بود ولی کاش اگر امکان و فرصتش بود به جای تلگرام فروم یا تالار گفتگو درست میشد برای سوال و جواب اینجوری جواب سوالی که قبلا پرسیده شده بهراحتی پیدا میشه سوال تکراری مطرح نمیشه و جستجو هم خیلی راحتتره به نظرم برای مدرس هم خیلی راحتتر باشه و وقت کمتری ازش گرفته میشه
-> بیا پیوی پس صحبت کنیم که چه روشی برای حل این مساله میتونیم ارائه بدیم شاید واقعا بتونیم به نتیجه برسیم"
"-> 
-> این doc هاش هست"
"-> سلام وقت همگی بخیر من دانشجوی ارشد هستم موضوع پایان نامه ام مرتبط با مبحث یادگیری تقویتی عمیق است من تا حدودی مباحث ماشین لرنینگ و میدونم ولی برای یادگیری تقویتی عمیق هر چی سرچ میکنم کمتر منبعی پیدا میکنم که بتونم دانشم و بیشتر کنم که از پس پایان نامه بربیام از دوستان کسی هست که بتونه در این مورد راهنماییم کنه
-> سلام به نظرم هاگینگ فیس خیلی خوب و ساده مطالب تقویتی رو جا میندازه ولی شما فرمودید دانشم رو بیشتر کنم نمیدونم این منبع برای شما مناسب هست یا نه
-> خیلی ممنون در حد خیلی جزئی تو نت سرچ کردم و یه چیزایی میدونم
-> سلام وقتتون بخیر روبوتک یه دوره یادگیری تقویتی داره از پایه شروع میکنه و تا ابتدای یادگیری تقویتی عمیق DQN میبره کیفیت دورم خوبه و با کد زنیه توی آپارات دکتر رهبان استاد دانشکده کامپیوتر شریف کلاس امسالش کامل آپلود کرده از پایه هست تا مطالب واقعا پیشرفته با ریاضیات تا حد خیلی خوبهوم ورک و کد های اینو میتونم در اختیارتون بزارم دوره دانشگاه استنفوردم توی یوتیوب هست البته این درواقع دوره شریف از همین دوره گرفته شده این کانالم اعضاش کتابخونی دارن کتاب ساتن که مرجع اصلی تمام دوره هاستو دارن میرن جلو فیلم هاش رو هم آپلود میکنن
-> پس هاگینگ فیس خیلی خوبه مختصر و مفید گفته
-> کلا اگر از OpenAI gym که کتابخانه مخصوص یادگیری تقویتی هست استفاده کنید خوبه توی gitir میتونید اموزشهاسو پیدا کنید
-> ممنون از توضیحات کاملتون"
"-> برای ارشد
-> چقدر خوب نمیدونم والا شاید سقف ما هم بیشتر بوده و حرفی نمیزنن و الا خیلی عجیبه یه رشته دیگه ده سال پیش 6 میلیون بدن ولی الان به یه رشته دیگه یک سوم اون مبلغ در نظر گرفته بشه"
"-> سلام بزرگواران آموزش خوب فارسی tourch میخواستم
-> سلام دوره پایتورچمون که رایگانه همون رو برای شروع ببینید"
"-> سلام من چند تا سوال دارم من میخوام دوره های دیپ لرنینگ و دیپ کاتالیست و ماشین ویژن رو تهیه کنم سوال اول اینکه چه زمانی میتونم از طرح های تخفیف تون استفاده کنم سوال دوم اینکه چرا دوره ماشین ویژن رو بازنشسته کردین من سر فصل هاش رو دیدم خیلی خوب بود دوره جدید کی آماده میشه ممنون میشم پاسخ بدین
-> سلام درمورد تخفیف لطفا با پشتیبانی صحبت کنید به همین اکانت پیام خصوصی بدید یا توی سایت توی چت آنلاین مطرح کنید بینایی کامپیوتر به خاطر بخش کدنویسی که گاهی خطاهایی به خاطر ورژن لایبرریها برای دوستان پیش میومد بازنشست شده احتمالا دو ماه دیگه دوره جدیدش شروع میشه پیشنهاد من این هست که الان دوره دیپ رو ببینید و خودتون رو برای بینایی کامپیوتر آماده کنید
-> بسیار عالی ممنون من دوره دیپ رایگان رو دیدم ولی خوب ظاهرا دیپ جدید تغییراتی داره و میخوام تهیه کنم امیدوارم دوره بینایی ماشین زودتر تموم بشه دیپ کاتالیست هم پروژه های خوبی تعریف کردین ممنون
-> سلام و وقت بخیر من اخیرا دوره بینایی رو تهیه کردم ایا از دوره بینایی جدید به صورت رایگان یا با تخفیف میتونیم استفاده کنیم
-> سلام ما هنوز درمورد جزئیاتش تصمیم نگرفتیم فعلا در این حد میتونم بگم که قطعا دسترسی رایگان نخواهید داشت اما قطعا تخفیف خوب درنظر گرفته میشه البته اگه 99 درصد هم تخفیف بدیم بعضی دوستان میگن که تخفیف بیشتری نمیشه بدید یک غری هم میزنن درمورد اینکه چرا دوره جدید برای افرادی که یک دوره مشابه رو قبلا تهیه کردن قبلا توضیح دادم توی این گروه دنبالش گشتم ولی متاسفانه پیداش نکردم
-> سلام منظور کدوم دوره هست بینایی کامپیوتر حرفهای
-> بله
-> یکسری خطای روی لایبرریهایی که آپدیت شدن گاهی وجود داره
-> برا ما که داریم این دوره رو و کامل ندیدیم آیا کامل ببینیم بعد دوباره هنگام دوره جدید اون رو هم ببینیم یا صبر کنیم جدیده رو ببینیم
-> هشتگ سوالات شرعی مستحبه که بمونی تا دوره بعدی تو که دوره پردازش تصویر رو الان داری تازه پروژه پینگ پنگ هم داری
-> سلام استاد وقت بخیرمن دانشجوی دوره دیپ کاتالیست هستم و پروژه تخمین سن رو با استریم لیت نوشتم و یه قسمتی داره که کاربر عکس بارگزاری میکنه و مدل باید تخمین سن کنه من تو لوکال که مینوشتم میومدم اول عکسو تو لوکال سیو میکردم بعد به مدل میدادم ولی الان که تو hugging دیپلوی کردم دیگه طبیعتا عکسو نمیشه سیو کرد تو لوکالپیشنهادی میتونین بدین که چیکار کنم که بتونم عکسو به مدلم بدم
-> ای شیخبزرگوار بر من بخشای که کوتاهی اندیشهام را بهجا نیاوردم و بر شکیبایی و بردباریات سپاس میفرستم
-> در اقدامی جهادی ریکشنها رو از محدودیت خارج کردیم
-> چه کار خوبی من متاسفانه نمیدونم امیدوارم سایر دوستان راهنمایی کنن میدونی
-> درود به شما اگر بزارید مطالب گروه رو سیو کنیم برای خودمون بعدا بخونیم هم باور کنید هم ثواب دنیوی دارد هم اخروی
-> در اقدامی جهادی 2 این هم آزاد شد اصلاحات به این میگن الل
-> واییی باورم نمیشه خیلی لطف کردین
-> آره خاستم راهنماییشون کنم خودم کدتون رو بفرستید لطفا براتون درستش میکنم
-> سلام ممنونم فرستادم براتون"
"-> سلام وقت به خیر این لب تاب با این مشخصات برای ران کردن مدل های دیپ لرنینگ مناسب هست
-> سلام فکر کنم gpuش رده پایین هست منظورتون از مناسب بودن چیه مثلا میخواید باهاش چه کارهایی انجام بدید
-> سلام دوست من امروزه همه چی روی کلاد هست به ران کردن الگوریتمهای هوش مصنوعی رو سیستم لوکال فکر نکنید اصلا
-> ببینید ایران شرایطش واقعا متفاوته خیلی از بچهها برای کار دانشگاهیشون میخوان به نظرم این پلن برای همه کار نمیکنه
-> خوب منم ایران هستم ولی واقعا نیاز نشده تا حالا بخوام روی لوکال کار کنم البته درست می فرمایید برای پروژه های دانشجویی ممکنه قضیه فرق کنه و یا اینکه اگر در سازمانی یه پروژه خصوصی باشه شاید قضیه فرق کنه
-> یعنی از سرویس پولی استفاده میکنید مثلا چه چیزهایی رو انتخاب کردید
-> من از همون گوگل کولب استفاده میکنم بیشتر اما نسخه پولی ش هم تایم بیشتری به من میده و هم رم بیشتر البته نسخه های مختلف جی پی یو هم میده مثلا جی پی یو V100 رو من خیلی اوکی هستم باهاش اما برای دانشگاه به نظرم دانشگاه باید امکانات لازم رو بده به دانشجو ها برای ران این نوع پروژه ها نه اینکه دانشجو از جیب بخواد خرج کنه یه لب تاب بگیره اونم در نهایت همون کار کولب رو ممکنه انجام بده نهایتا حداقل اینترنت درست حسابی بده به دانشجوها
-> بتونم باهاش مثلا کار پایان نامه م ۱۰۰۰ تا تصویره و الگوریتم unet رو اجرا کنم و در آینده هم بتونم پروژه بگیرم کارهای دیپ لرنینگ رو انجام بدم میخام یک بار هزینه کنم ولی تا دو سه سال بتونم باهاش کار کنم gpu ش در چه رده ای باشه خوبه
-> ممنون درسته ولی نت یاری نمیکنه گاهی اوقات
-> اگر میتونی یه مک بوک بگیر با پردازنده سری M2
-> سلام حتی از اینکه نمیشه با لپتاپ ترین طولانی گذاشت و فشار خیلی اورد بهش هم بگذریم با 4 گیگ اذیت میشی و هر لحظه به ارور cuda out of memory میخوری و عملا پولت رو ریختی دور
-> باید دانشگاه بده ولی خب نمیده
-> پس پیشنهاد من همون google colab pro هست اگر ابعاد پروژه خیلی بزرگ نباشه به نظرم برای پروژه های دانشجویی خوب هست
-> به نظرم برای این کاری که شما میخواید نهایت این هست که روی این لپتاپ کدنویسی کنید و بعد هم یک ترین خیلی محدود و سبک بذارید بعد برای ترین اصلی این سیستم جوابگو نیست به نظرم این لپتاپ معمولی هست درکنارش به سیستمی مناسب برای ترین نیاز دارید حالا یا کلود هست یا موارد دیگه کلا بهتر هست روی ترین با لپتاپ حساب باز نکنید پی سی میشه ولی لپتاپ مناسب ترین کردن نیست دقت کنید مناسب ترین نیست وگرنه میشه کد زد و ترینهای کوچیک و محدود گذاشت
-> آره من کولب پرو رو مناسب میدونم ولی باید فرد پول داشته باشه که بتونه هزینه کنه یعنی ماهی یکی دو میلیون ممکن هست نیاز باشه که البته با دلار هم تغییر میکنه یکی از بچهها runpod رو معرفی کرده بود lightning هم به نظر خوبه
-> runpod هم خوبه
-> اها ممنون
-> اها ممنونم بابت راهنمایی تون
-> داخل پروپوزال یه بخش هزینه هم وجود داره که اگر اشتباه نکنم دانشجو میتونه تا سقف 2 میلیون تومن استفاده کنه هر چند مبلغ خاصی نیست ولی برای ترین اصلی و نهایی بنظرم میتونه جواب گو باشه من خودم این بخش رو پر کرده بودم ولی چون به سیستم های ابری علاقه نداشتم روی لوکال اجرا کردم و دیگه دنبال کارهاش نرفتم
-> آفرین چه پیشنهاد خوبی دانشگاهها یک چیزهایی رو هم مخفی میکنن و صداش رو در نمیارن ولی پیگیر باشید به جاهای جالبی میرسید مثلا دانشگاه به کسانی که پروژه ساخت دارن پول میدن شاید با رایزنی بشه این کارها رو هم در رده ساخت در نظر بگیرن
-> بله دقیقا همینطوره
-> در علوم وزشکی خیلی بیشتر ازین رقم هاست
-> ما مهندسا همیشه خدا مظلومیم
-> من سال 94 دفاع کردم اون موقع 4 یا 6 تومن بود"
"-> ببخشید استاد مشکل من اینه که مجبور شدم برای پایان نامم یه پروژه یادگیری ماشین انجام بدم و اطلاعات رو از سایتهای مختلف گرفتم و بالاخره پایان نامه رو با موفقیت انجام دادم ولی بعضی مطالب برام بدرستی تفهیم نشده بود چون با عجله یاد گرفته بودم و الان که همزمان هم دارم دوره آموزشی شما رو میبینم و هم پایان نامه رو تبدیل به کتاب میکنم سوالاتی توی پایان نامه برام پیش میاد که هنوز توی دوره بهش نرسیدم عذر میخوام اگر گاهی سوالاتی میپرسم که توی دوره هست دلیلش اینه که هنوز به ویدویوی مربوط به اون نرسیدم و مربوط به پایان نامم هست
-> خواهش میکنم مشکلی نیست موفق باشید"
"-> سلام کسی میدونه یه دیتاستی که نیاز به متوازن سازی و نرمال سازی داره آیا فقط رو داده های ترین متوازن سازی و نرمال سازی انجام میدیم یا باید رو داده های تست هم انجام بشه دیدم تو سایکیت لرن نوشته بود باید نرمال سازی بعد از تقسیم داده ها انجام بشه تا نشت نکنه
-> نرمالیزه فقط روی ترین اینها رو توی فصل انتخاب مدل دوره یادگیری ماشین گفتیم مطالبی که هم در سایکیت خوندید درست هست"
"-> ۴۷ مگ حجم نوت بوکم شده و بالا نمیاره گیر کردم واقعا راه حلی چیزی ممنون میشم ژوپیتر
-> با آپدیت ژوپیتر درست شد
-> دقیقا فقط مشکلی که پیدا کردم بعدش اینه که خروجی ها رنگ سیاه دارن و خوانا نیستن به این صورت
-> فك كنم ميتونيد ژوپيتر در تنظيمات اوكى كنيد
-> نتونستم کاری کنم هرچی گشتم"
"-> سلام دوستان وقتتون بخیر من دارم روی یه پروژه ای کار میکنم که مربوط به فشن هستش و کاربر اسم استایلی که میخواد رو بهش میده و برنامه طبق اون استایل کاربر و لباس هایی که توی فروشگاه موجوده بهش استایل پیشنهاد میدهبه صورت عکس اسم استایلی که کاربر میده هم تک کلمه ای هستش مثلا کژوال یا کلاسیک یا دنبال یه دیتاست میگردم که یه سری عکس از مدلهای مختلف لباس ها رو داشته باشه و توی لیبل هاش اون تک کلمه ای استایل کژوال و هم باشه ولی دیتاست هایی که برای این کار موجوده و من دیدمشون هیچ کدوم تک کلمه ای استایل رو توی لیبل هاشون ندارن مثلا توی دیتاست FashionIQ فقط یه دیسکریپشن یا کپشن برای اون عکسه گذاشته به صورت زیر B009WH4ZQC captions has a v shaped neck with no sleeves and is dark is darker and has a vneck ممنون میشم اگه کسی از دوستان تجربه ای در این زمینه داره بنده رو راهنمایی کنه
-> سلام وقت بخیر من تجربه خاصی ندارم صرفا یه مبحثی به ذهنم رسید گفتم بگم شاید کمک کننده باشه به نظرم caption های تمام سمپل ها رو بردارید و به یه مدل زبانی بدید و ازش بخواید که براتون برچسب اون رو بزنه"
"-> دوستان شب بخیر من لپ تاپ i3دارم میخوام i5 برای برنامه نویسی هوش مصنوعی دوستان کسی تجربه برنامه نويسي دارد راهنمایی کند ممنون
-> 
-> ممنون بابت توضیحاتی که دادید ما اخیرا برای هوسم میخواستیم یک لپتاپ بگیریم یکی از دوستان مورد اعتماد ما گفت که ترجیحا کامپیوتر بگیرید لپتاپهای امروزی ضعیف شدن و مادربردهای قوی ندارن با همون هزینهای که میخواید لپتاپ بگیرید شاید بتونید یک سیستمی ببندید که هم قویتر از لپتاپ باشه و هم قابل ارتقا باشه ما دو سال پیش یک لپتاپ تاف برای هوسم خریدیدم ولی به نظرم ضعیف هست بگذریم از اینکه یکسری مشکلات غیرقابل انتظار مثل پاک شدن رنگ کیبورد و غیره هم پیش اومد اون هم در شرایطی که از یک جای بسیار مطمئن تهیه کرده بودیم"
"-> سلام وقت بخیر یک راهنمایی لازم داشتم بنده دوره ی یادگیری عمیق 2022 را چند ماهی است که تمام کردم و سعی کردم تمارین آن و همچنین خود کدنویسی جلسات را خودم انجام بدهم بعد از آن به سراغ دوره دیپ کاتالیست رفتم تا توی یادگیری عمیق بتوانم تخصص خودم را بیشتر کنم هر چند که دو ماهی است که در پروژه ی صفر باقی ماندم دلیل stop شدنم هم این بود وقتی میخواستم فرا تر از تکرار کدهای جلسات بروم و با استفاده از classification مسئله را حل کنم ناکام ماندم جدای همه ی این موارد از چندین شرکت تسک های ارزیابی جهت ورود به کارآموزی و یا استخدام گرفتم ولی هیچ کدام را نتوانستم به خوبی انجام دهم حتی یک تسک پیش بینی و رگرسیون یه سیگنال مالی بود که فکر میکردم با یک mlp و یا rnn ساده میتونم از پس آن بربیایم هم نتوانستم میخواستم در این مورد راهنمایی بگیرم که ایراد کار کجاست و باید برای رفع مشکل چه اقداماتی انجام دهم
-> سلام چیزهایی که گفتید تقریبا خوب بود و دوست داشتم مهمترین توصیه من این هست که دست از تلاش برندارید و به جزئیات عمیقا توجه کنید دیپ کاتالیست رو رها نکنید و ادامه بدید مهم نیست که اون دستهبندی رو نتونستید برید سراغ پروژههای بعدی و اونها رو انجام بدید شاید بعد از پروژه بعدی یا بعدها مهارتتون بیشتر بشه و بتونید اون پروژه 0 رو هم به شکل دلخواه انجام بدید مشاوره خصوصی هم احتمالا برای شما مفید هست چند جلسه مشاوره ممکن هست بهتون دید خوبی بده و رو به جلو حرکت کنید
-> برای پروژههای کارآموزی و استخدامی که رد شدید و زمانش گذشته هم وقت بذارید نشد نداره حتما میتونید انجامش بدید
-> خیلی ممنونم اینکه روی یک پروژه دیپ کاتالیست تسلط کافی نداشته باشم و skip کنم برم سراغ پروژه ی بعدی مشکلی ایجاد نمیکنه همین الان درخواست مشاوره را به howsammailcom ایمیل کردم
-> بله حتما یک سری از این پروژه ها خروجی گرفتن از یک صفحه ی گیت هاب و سپس فاین تیون کردن مدل بر روی دیتاستی است که مشخص میکردند توی این موارد توی انبوهی از فایل ها و کدها گم میشم و نمیتونم حتی یک مدل pretrain را لود کنم کلی فایل پایتونی config و init که دقیقا یک فرمت مشخصی را دنبال میکنند که حتی با مطالعه ی readme و سایر توضیحاتشون هم سر در نمیارم برای این موارد باید چه کاری انجام دهم
-> به نظرم فعلا برید پروژه 1 ببینید وضعیتتون چطور هست
-> اینها بیشتر تجربی هست یکسری مهارتهای جانبی هست که با پروژههای متعدد کمکم حل میشه توی موردهای اولی خوبه که از کسی بخوایید بهتون هینت بده هینت بده شما تلاش کنید نشد دوباره هینت بده بهتون دوباره تلاش و
-> خیلی ممنونم از راهنماییتون
-> سلام احتمالا ضعف برنامه نويسي صورت مسئله تعيير ميكنه نميتونيد كد بزنيد
-> ضعف برنامه نویسی علی الخصوص در بررسی کدهای دیگران و سعی بر اجرای کدهای گیت هاب به چشم میاد پیشنهادی دارید
-> فکر میکنم اجرای کدهای سادهتر گیتهاب کمک میکنه
-> نا اميد نشيد كلا بررسي كدي كه ديگران نوشتن كار سختيه مفاهيم پايه رو كامل درك كنيد و بفهميد مثل كلاس فانكشن و اينارو كامل متوجه بشيد بيشتر راه رو رفتيد براي درك برنامه نويسي وقت بزاريد مثل پايتورچ كلا بر پايه كلاس نوشته شده شما ندونيد يه كلاس تو پايتون چطوري پياده ميشه هيچ وقت پايتورچ رو درك نميكنيد فقط كدها رو حفظ ميكنيد
-> اینها با تمرین هدفمند و استراتژی مناسب به راحتی حل میشه این مرحلهای هست که همه تجربه میکنن
-> سلام و وقت بخیر میشه یه راهنمایی راجب این موضوع بکنید منظورتون از استراتژی مناسب چی هست و تمرین هدفمند به جه شکلی باید باشه ممنون
-> سلام من یه پیشنهادی براتون دارم قبل از اینکه درباره موضوعی کدشو ببینین یا فیلمی ببینین خودتون سعی کنین انجامش بدین خیلییی خوبه این روش من هم تازه دیپ کاتالیست شروع کردم قبل این خودم سعی داشتم یه پروژه face detection انجام بدم اصلا نمیدونستم چیزی به اسم faster rcnn اینا هست خودم از صفر عکس جمع کردم لیبل زدم مدلشو نوشتم دیتاست ساختم که خیلیم خوب کار نمیکرد البته ولی خیلی چالش داشت و دیدمو گسترش دادبه شمام پیشنهاد میکنم وقت بذارین رو پروژه ها ارزششو داره
-> "
"-> دوستان امیررضا توی یوتوب محتوای ارزشمند و سنگینی میذاره مثلا دیدم که مقاله gpt1 bert و llama 3 رو با جزئیات تشریح کرده انجام این کارها واقعا سخت و طاقت فرسا هست امیدوارم امیررضا ادامه بده و دوستان هم ویدئوها رو ببینن و حمایت کنن من بیش از 10 سال هست که در کار تولید محتوا هستم محتوای رایگان ارزشمند هست ولی اگر مثل هر کاری تولید محتوا با درآمدی همراه باشه فرد رو تشویق میکنه که محتوای بیشتر و باکیفیتتر بسازه سالها قبل وقتی دیدم مخاطب از آموزشهای من خوشش میاد یک میکروفون ساده و قلم نوری از همون درآمد تولید محتوا خریدم محتواهای یوتوب که پولی نیست اما حداقل کاری که میشه کرد این هست که لایک کنید کامنت بذارید و ببینید که درصد میانگین مشاهده ویدئوها بالا بره و الگوریتمهای یوتوب این ویدئوها رو به افراد بیشتری پیشنهاد بده
-> سلام استاد جان خیلی لطف دارین شرمنده می کنین بنده رو لطف و محبت شما و این که تعریف می کنین از این کار برام افتخار بزرگیه و امیدوارم بتونم شایسته لطف تون باشم"
"-> سلام استاد دوره جدید بینایی کامپیوتر کی آماده میشه ما منتظر هستیم
-> سلام درگیر دوره پردازش تصویر هستیم امیدوارم از مهر ماه بتونیم شروع کنیم"
"-> با سلام و درود یه سوال داشتم از حضورتون کسی میدونه که چجوری میشه چینش روابط و فرمولها رو در chatgpt درست کرد
-> احتمالا براش فارسی مینویسی که چینشش درست نیست همان سوال را انگلیسی بنویسی فکر کنم چینشش درست بشه تجربه شا داشتم که میگم
-> اکستنشن ChatGPT RTL
-> سلام وقت بخیر بعضی وقتا مشکل از راست به چپ کردن خروجی هست که اگر برعکس کنید درست میشه اگرم میخواین ازش توی فایل ورد استفاده کنید از چت جی پی تی بخواین فرمول رو بصورت کد لتکس بده"
"-> سلام وقت بخیر یه دیتاست تایم سری دارم که میخوام کلاسیفیکشن انجام بدم و 13 کلاس دارم تعداد لیبل های کلاس ها متعادل نیست و کم و زیاده سعی کردم با روش های under sampling و smote تعداد رو متعادل کنم و از random Forrest و xgboost هم استفاده کردم الان میخوام که از ترنسفورمر استفاده کنم سوالی که دارم اینه که smote تایم سری بودن داده رو زیر سوال نمیبره و عدد window slicing توی time series بر چه اساسی بدست میاد بر اساس ازمون های اماری مثل dkfoller و اون lag ای که بدست میاد
-> اگر روش های متعادل سازی کلاس ها جواب نداد روش دستکاری loss function رو پیشنهاد میکنین"
"-> سلام شبتون بخیر من تازه دوره دیپ لرنینگ رو شروع کردم یه سوال داشتم استاد مطالب رو روی گوگل کلب اجرا میکنن ولی من دارم از ژوپیتر نوتبوک استفاده میکنم همینطوری میشه ادامه داد یا منم باید از گوگل کلب استفاده کنم
-> اکثر جاهای دوره شما نیاز به GPU دارین اگر خودتون GPU ندارین و میخواین رایگان استفاده کنین باید با کولب ادامه بدین
-> سوالهای مرتبط به دوره در گروه اختصاصی دوره و سوالهای خارج از دوره در گروه دانشآموختگان"
"-> نسل 31 لاما تغییرات جذابی رو با خودش همراه کرد میدونیم که این سری از مدل های لاما در سه ورژن 8B و 70B و 405B منتشر شدن و یه تغییر بزرگ رو هم توی جامعه اوپن سورس رقم زدن این تغییر بزرگ به واسطه انتشار بزرگترین مدل این سری لاما یعنی Llama 31 405B هست مدلی با 405 میلیارد پارامتر که بزرگترین مدل اوپن سورس تا به امروز هست تا حالا هیچ کمپانی ای مدلی به این عظمت رو اوپن نکرده بود و این یعنی قراره شاهد اتفاقات بزرگی توی جامعه اوپن سورس باشیم این مدل با 150 تا بنچمارک مختلف سنجیده شده و اتفاق جالب این هست که این مدل توی بنچمارک هایی که متا منتشر کرده به سطحی نزدیک به Claude 35 Sonnet GPT4 GPT4o داره و در یک سری از بنچمارک ها حتی بهتر هم عمل کرده برای مثال توی بنچمارک IFeval که مربوط به instruction follwing هست از هردوی Claude sonnet 35 و GPT4o عملکرد بهتری نشون داده بریم سراغ دوتا مدل کوچیکتر که خیلیم ترکوندن نسخه ها 870 میلیاردی که نسبت به نسل قبل یک سری تفاوت ها دارن 1 Context length 128K 2 Multilingual support زبان ها انگلیسی فرانسوی آلمانی هندی ایتالیایی پرتغالی اسپانیایی و تای این دوتا فیچر های اصلی و جدیدی هستن که به مدل های این نسل اضافه شده و زیاد تغییر پرفورمنس خاصی توی این مدل ها مشاهده نمیشه اما تغییرات نسبتا خوبی بهشون اعمال کردن میشه گفت این دو نسخه کوچیک این نسل صرفا یک اپدیتی بودن از نسل قبلی و غول اصلی همون مدل 405B هست جزییات فنی تر دیتا دیتای آموزش مدل نسبت به نسل قبل خیلی بزرگ شده و کیفیتش خیلی بهتر شده حجم دیتا برای این نسل حدود 156 تریلیون توکن هست و تقریبا 8 برابر دیتای نسل قبلی لاما یعنی نسل 2 هست آموزش مدل بزرگ این سری یعنی مدل 405 میلیاردی با 38 1025 FLOPs روی 156T توکن pretrain شده این مدل چند ماه توی فاز اموزش بوده و روی 16000 تا تراشه H100 اموزش دیده شده که در مجموع جمعشون به 393 میلیون ساعت محاسباتی GPU میرسه و چند صد میلیون دلار هزینشه همچنین مدل های این سری روی 10 میلیون سوال و جواب انسانی و 25 میلیون synthetic data فاین تیون شده ان و توی هر مرحله از supervised fine tuning وdirect preference optimization استفاده کردن که داده های synthetic رو با کیفیت بالاتری ایجاد کنن و عملکرد مدل هم بهبود بدن معماری توی معماری این سری خبری از MoE یا Mixture of Experts نیست و یک معماری ساده Decoder Only رو داریم همچنین همه مدلای این سری از GQA یا Groped Query Attention استفاده میکنن با هدف maximize کردن stability توی پروسه آموزش همچنین خودشون مدل رو BF16 به FP88bit کوانتایز کردن و حجم مدل رو کم کردن و جالبه که میگن اینکار باعث شده بتونن مدل رو روی یه نود سرور واحد اجرا کنن اجرا و تست این مدل ها مشخصا نمیتونین مدل بزرگ رو به صورت لوکال اجرا کنین و باید از پروایدر های third party استفاده کنین که بتونین مدل 405 میلیاردی رو تست کنین از پلتفرم ما میتونین برای تست و استفاده همه این مدلا به صورت رایگان استفاده کنین کنین اما متا با 25 تا سرویس قرارداد داره و اونا به صورت پولی مدل هارو ارائه میدن Accenture Amazon Web Services AMD Anyscale CloudFlare Databricks Dell Deloitte Fireworksai Google Cloud Groq Hugging Face IBM WatsonX Infosys Intel Kaggle Microsoft Azure NVIDIA OctoAI Oracle Cloud PwC Replicate Sarvam AI ScaleAI SNCF Snowflake Together AI and vLLM project developed in Sky Computing Lab at UC Berkeley هاگینک فیس inference API رو برای این مدل فعلا از دسترس خارج کرده و Groq هم طبق گفته خودشون 23 هفته دیگه میتونن مدل رو ارائه بدن اما برای تست نسخه های کوچیکتر به صورت لوکال میتونین از ollama استفاده کنین و همچنین فکر میکنم که سخت افزار مورد نیازتون واسه ران کردن این دو نسخه برای نسخه 8 میلیاردی کوانتایز شده میتونین با 4090 هم اجراش کنین و نسخه کوانتایز شده 70 میلیاردی هم ترجیحا A100 یا L40 یا L40S اوکین نکات اضافی عملکرد مدل های این نسل توی کد زدن خیلی بهتر شده متا گفته که قراره نسل های جدید لاما MultiModal باشن متا قراره به زودی یه سرویس API هم برای این مدلا ایجاد کنه که قراره با Bing همcompatible بشه و میتونین لاما رو وصل کنین به یک موتور جستجو تیم Scale AI یه سری لیدربرد اماده کرده که بر اساس یک سری بنچمارک روی دیتاست های شخصی خود این تیم ساخته شدن لینک توییت تیم ما نسخه های 870 میلیاردی رو از نسل 3 31 رو روی 9 تا سوال تست کرده و نتایجشون رو همینجا قرار میدیم
-> دوستان تو این پست درمورد نسل جدید لاما یه سری توضیحات جامع دادم اگه حوصله ندارین خودتون برین سراغ منابع مختلف و مقاله منتشر شده به نظرم خوبه که این پستو بخونین
-> خیلی خوب بود ممنون"
"-> یه آپدیت بزرگ برای پلتفرممون منتشر کردیم توی این آپدیت تغییرات کلی و تغییرات جزئی هردو دیده میشن که پیشنهاد ما اینه که خودتون تجربشون کنین اما اصلی ترین تغییر نسخه جدید پلتفرم اضافه شدن 23 تا مدل جدید از چند خانواده مدل جدید هست یعنی شما درحال حاضر به ۳۵ مدل open source و close source به صورت رایگان دسترسی دارین با انتشار نسل جدید سری مدل های لاما یعنی نسخه های 31 ما هم دست به کار شدیم و علاوه بر هر 3 مدل جدید منتشر شده توسط متا 20 مدل جدید به پلتفرممون اد کردیم نسخه 405B لاما هم به صورت کاملا رایگان همون شب عرضه مدل ها به پلتفرممون اضافه کردیم اما تصمیم گرفتیم خبرشو همزمان با اپدیت پلتفرم اعلام کنیم لیست مدل هایی که درحال حاضر میتونین ازشون استفاده کنین 1 Dorna AIhugging face text generation model 2 OpenAI Family neuralbrainAlGPT 4 instance gpt4omini GPT4 gpt35turbo gpt35turbo1106 TTS 3 Qwen Family Qwen257B Qwen27B Qwen215B Qwen2O5B Qwen15110B Qwen1532B Qwen1527B 4 Llama Family Llama31405B Llama3170B Llama318B Llama370B Llama38B 5 Cohere Family Aya35B Aya23B CommandRPlus CommandR CommandLight CommandLightNightly CommandNightly Command 6 Gemma Family gemma227B gemma29B gemma7B 7 Zehpyr Family zephyr7B 8 Mistral Family mistralnemo12B mixtral8x7B mistral7Bv2 9 Yi Family Yil534B البته به همینجا ختم نمیشه و ما هر روز داریم فیچر های جدیدی برای بهبود پلتفرم اضافه میکنیم برای استفاده از پلتفرم ما میتونین از طریق این آدرس به صورت مستقیم و از طریق وبسایت ما ویدجت ابی رنگ سمت راست اقدام کنین
-> دوستان خوشحال میشم اگه نظر یا پیشنهادی دارید حتما بگید خیلی به بهبود کیفیت کارم کمک میکنه"
"-> سلام وقتتون بخیر من یک کدی نوشته بودم توی یه قسمت کد اصلی ام برای اینکه یه پوشه ایجاد بشه و توش مثلا یه سری نمودار به فرمت npz ذخیره کنه و بار اول که ران کردم اوکی بود و به همون اسمهایی که باید میبود ذخیره شده بود ولی امروز که دقیقا همون کد رو ران کردم اسم هاش عوض میشه و بهم میریزه باید چیکار کنم دوباره همه ی سل ها رو کپی پیست کنم تو یه قایل کولب دیگه
-> سلام احتمالش هست که فایل هایی با همون نام توی گوگل درایو یا هر محیط دیگه ای که فایل ها ذخیره میشن باشه
-> سلام بله قبلش تو یه کولب دیگه اجرا کرده بودم باید حذفش کنم اون فایل رو
-> بله وقتی با همون اسم موجود باشه فایل مجبوره اسمش رو تغییر بده باز برای اطمینان فایل رو دانلود کنید روی سیستمتون داشته باشید که اگر یه وقت مشکل این نبود از دستش ندید
-> خیلی ممون"
"-> سلام دوستان من از چند ماه پیش در تدارک شرکت در کنکور کامپیوتر برای هوش مصنوعی هستم و با یه مشاور مطرح که صحبت کردم گفت به دو دلیل که رشته کارشناسیت غیر مرتبطه و در طول روز زمان زیادی نداری برای درس خوندن کنکور آی تی شرکت کن و کنارش هوش کار کن خواستم نظرتون بدونم و لطف کنید توصیه ای کنید
-> اینکه از غیر رشته هوش مصنوعی میشه در هوش مصنوعی کار کرد درسته مثلا بچههای رشته مهندسی پزشکی برق و هم اصلا میتونن پایاننامشون رو مبتنی هوش تعریف کنن و حتی درسهای مرتبط با هوش بگذرونن اما اینکه پیشنهاد مشاورتون خوب هست یا نه رو نمیدونم
-> تشکر
-> سلام من نمیدونم مشاورتون کی بوده ولی چون خودم یه زمانی کنکور ارشد کامپیوتر شرکت کردم میدونم بازاره کنکور ارشد کامپیوتر خیلی داغه و اکثر افرادی که تو این حوزه تدریس میکنند سواد آکادمیک به معنای واقعی کلمه ندارند و بیشتر از این که معلم به معنای واقعی باشن showman هستن و مثلا میگن ماییم که خاک رو تبدیل به کیمیا میکنیم و مباحثی از این دست یا مثلا میگن هر سال تک رقمی ها برای موسسه ماست و فلان به نظرم خیلی به این حرفا و بازارگرمیا توجه نکنید اگر میخواید کنکور کامپیوتر بدید بهتره با یه فردی که واقعا تو این حوزه خبره هست مشورت بگیرید برای مثال تیم دکتر حقیقت واقعا افراد با سوادی توش هست و خودشون هم یک استاد به تمام معنا هستند این هم پلتفرمشونه بد نیست یه نگاهی بهش بندازید
-> لطف کردین"
"-> سلام من دارم دوتا دوره رو باهم میبینم بخاطر حجم ویدیوها روی اسپات پلیر درایوc پر شده اگر پاک کنم یکی از دوره هارو میتونم از همون لایسنس استفاده کنم روی سیستمم یا دچار مشکل میشم
-> پیام به پشتیبانی"
"-> سلام خدمت همگی ببخشید من دارم یه مدل رو fine tune میکنم و همه لایه ها رو freeze کردم به جز لایه آخر تعداد sample هام هم 750 تا هست وقتی میرم fine tune کنم loss train بعد از 2 تا epoch یهو زیاد میشه learning rate و batch size رو تغییر دادم ولی باز بهتر نمیشه ممنون میشم از دوستان بتونن راهنمایی کنن
-> انقدر حساسیت نداره ولیدیشن هم که زیاد نشده بذارید بیشتر ترین بشه ببینید چطوری میشه
-> ممنون از پاسختون این با early stopping خودش بعد از 4 تا epoch متوقف شد من early stopping رو برداشتم تا 10 تا epoch هم همین وضعیت بود حقیقتا دارم مدل bertbaseuncased رو فاین تیون میکنم نمیدونم تا چند تا epoch باید بزارم بره تا 10 تا همون وضعیت بود بزارم بیشتر train شه
-> تا ده تا رو بفرستید ببینم
-> 
-> به نظرم ترین شده سخت نگیرید تستهای بعدی رو انجام بدید ببینید در حد انتظارتون هست یا نه"
"-> سلام دوستان وقت همگی بخیر سوالی داشتم در رابطه با این نمودار که مربوط به L1Loss یک تسک رگرسیون هست بر روی ResNet 50 با 323 عدد دادهی تصویر آیا چنین نموداری با توجه به نوسانی بودنش اعتبار کافی برای ذکر در گزارشهای علمی مثل پایاننامه رو داره و اینکه بنظرتون برای تعداد ایپاک بیشتری باید آموزش صورت بگیره ممنونم
-> من در تصویر تجربه ای ندارم و در مورد تعداد epoch ها نمیتونم نظری بدم باید مقالات اون حوزه رو ببینی ولی نوساناتش طبیعی به نظر میرسه و روند نزولیش کاملا مشهوده منظورم اینه که نوسانای شدیدی نداره فاصله بین اموزش و ولیدیشن هم مناسبه به نظر من
-> وجود نوسانات ايرادى به حساب نمياد براى گزارش عملكرد شبكه
-> resnet"
"-> سلام دوستان کسی بازیابی اطلاعات کار کرده
-> بفرمایید"
"-> در قطعه کد خودش یک حلقه به تعداد ایتریشن ایجاد کرده و داخل حلقه اینفرنس کرده
-> گفته trying to measure inference time میخواد زمان اینفرنس رو اندازه گیری کنه که خب این کار مرسوم هست چند بار زمان رو حساب میکنن و بعد میانگین میگیرن و گزارش میدن
-> در دنیای واقعی اینطور نیست یک ورودی داریم و یک خروجی ایشون دو تا ابزار بهینه سازی شبکه های عصبی رو مقایسه کردند و به این نتیجه رسیدن زمانی که تعداد ایتریشن زیاد شود ابزار A بهتر از B است سوالم اینکه در دنیای واقعی ما از هر کاربر یک ورودی دریافت می کنیم و اینفرنس میگیریم در واقع ایتریشن ۱ است نمیدونم خوب توضیح دادم یا نه در واقع چیزی که ذهنم رو درگیر کرده همان کاربرد ایتریشن در دنیای واقعی یا پروداکشن است
-> در دنیای واقعی وقتی محصول درحال سرویس دهی به کاربر هست ایتریشن نداریم درست اما قبل از ارائه نهایی محصول میتونیم تست اندازهگیری و مقایسه انجام بدیم الان فرد میخواد اینفرنس تایم رو بدست بیاره به مدیر پروژه گزارش بده نمیتونه بگه که محصول رو ارائه بدیم وقتی کاربرد اومد ورودی داد زمان اینفرنس رو حساب کنیم
-> واقعیتش اینکه یک فردی ابزار onnx و openVINO رو در لحاظ اینفرنس تایم مقایسه کرده و به این نتیجه رسیده در ایتریشن بالا اینفرنس تایم openVINO بهتر است حال باتوجه به توضیحاتتون که در پروداکشن ایتریشن نداریم میشه اینطوری برداشت کرد که onnx در پروداکشن عملکرد بهتری دارد
-> من این لینک رو ندارم که چک کنم به نظر میرسه فرد یک مشکلی داره و نتایج عجیبی بدست آورده که اومده توی استک مطرح کرده نمیتونم نتیجه گیری شما رو تایید کنم
-> 
-> جوابش رو دادن که Generally CPU Execution Provider works best with small iteration since its intention is to keep the binary size small Meanwhile the OpenVINO Execution Provider is intended for Deep Learning inference on Intel CPUs Intel integrated GPUs and Intel MovidiusTM Vision Processing Units VPUs This is why the OpenVINO Execution Provider outperforms the CPU Execution Provider during larger iterations
-> خب در پروداکشن ایتریشن نداریم من سوالم اینکه این ایتریشن بالا کاربردش چیه
-> realtime applications like video surveillance autonomous driving or interactive AI systems the model continuously processes data as it comes in"
"-> سلام وقتتون بخیرمیخواستم بپرسم آیا به زودی قراره روی دوره دیپ کاتالیست تخفیف بزارین یا نه متاسفانه تخفیف قبلیو از دست دادم
-> سلام لطفا به پشتیبانی پیام بدید"
"-> هر کی میدونه ممنون میشم جواب بدید من تاکنون تجربه اموزش شبکه با تصاویر رو نداشتم و الان کار فرصی در دست دارم میخوام ببینم نیازم با دیدن کامل دوره رفع میشه یا ن
-> هیچ دورهای نیاز شما در این پروژه یا سایر پروژهها رو مرتفع نمیکنه همونطور که برای پزشک قبل یک عمل جراحی فوری هیچ کتاب و آموزشی بدرد نمیخوره"
"-> سلام دوستان کسی اطلاع داره در کگل هم آیا میشه مثلا دوتا اکانت ایجاد و از gpu هردواکانت همزمان استفاده کرد این اجازه رو میده ممنون میشم اگه اطلاعاتی دارین به اشتراک بگذارین
-> مشخصا میشه دوتا اکانت ساخت ولی منظورتون از استفاده همزمان چی هست
-> که بتونم بخشی از داده ای ک دارم ترین میکنم رو با gpu روی یک اکانت و بقیه رو هر اکانت دیگه ترین کنم مثلا در مبحث k_fold validation هر فولد روی یک اکانت ترین بشه
-> یعنی میخواین یه پروسه اموزش disturbed شده ایجاد کنین
-> داده ای دارم از نوع text ک روی ۳ تا ایپاک در کولب نزدیک ۲۴ ساعت ترینش طول میکشه و کولب هربار gpu رو ازم گرفته و نهایتا نتونستم به نتیجه ای برسم با توجه به اینکه کگل gpu بیشتری میده گفتم اگر بشه این شکلی اونجا اجراش کنم شاید به نتیجه ای برسم
-> این سرعت آموزش منطقی هست از نظر خودتون
-> قاعدتا نه واقعا زیاد سرچ کردم اما به نتیجه ای نرسیدم از مدل برت دارم استفاده میکنم ک آنقدر طولانی داره ترین میشه ممنون میشم اگر نظر یا پیشنهادی دارین راهنماییم کنین
-> سرعتش اصلا بهینه نیست به نظرم برای همچین کار سنگینی نه استفاده از کولب درسته نه کگل چون صرفا با یه پروسه نصفه که یهو قطع شده مواجه میشین
-> چند میلیارد توکن داری
-> پس یعنی پیشنهاد فقط استفاده از یه سروره
-> تقریبا ۲۵
-> خب 8 ساعت برای هر ایپاک بد نیست راستش حجم داده ات خیلی زیاده البته باز بستگی به تسکت داره چند گیگه داده ات زیر 3 گیگ باید باشه
-> برای این سوال هم باید توی مرور گرت new in private window باز کنی و با اکانت دیگه ورود کنی یا دوتا مرور گر مختلف استفاده کنی اگه بهت گیر داد بخاطر یکسان بودن ایپی باید روی یکی از مرور گر هات پراکسی بزنی
-> داده خیلی بزرگتر از اینه اما من الان فقط بخشی از اونو برداشتم و روی مقدار خیلی کمی دارم ترین میکنم و همه داده نیس در واقع
-> حجم کل داده نه به گیگ نمیرسه ۳۰ مگه درواقع دیتا حدود 600000 تا لینک url هست ک من روی تقریبا 50000 تا دارم شبکه رو ترین میکنم
-> ممنونم
-> خب چون 25 میلیارد توکن نداری و داده ات 30 مگه احتمالا کدت مشکل داره یا اصلا به gpu وصل نشدی
-> توکن نه ببخشید با پارامترا اشتباه کردم کدم رو ک نمیدونم دقیقا چ مشکلی داره ولی به gpu متصل میشم با batch size ای ک در نظر گرفتم تا تقریبا نیمی از اون هم میره و دیگه gpu رو میگیره
-> ۲۵ میلیارد پارامتر
-> سلام وقت بخیر من هنوز دوره یادگیری عمیق و کاتالیست رو کامل ندیدم سوالم این هست که من در ترین شبکم با تصاویر دچار مشکل کرش میشم و طی ترین رمم کامل پر میشه ورودیم مقادیر پیکسل های تصویر تصویر فلت شدس که میشه 5700 تا ویژگی و خروجیم باز مقادیر پیکسل های تصویر هست که میشه 131044 ویژگی سوالم از شما اینه طبیعیه که کرش میکنه در ادامه راهی گفته خواهد شد که با همان رم کم کولب هم بشه ترین روی تصاویر انجام داد ممنون میشم در حد بله و خیر جوابمو بدید
-> بله خیر
-> گفتن داده تکست پس یعنی مدل کامپیوتر ویژنی نیست احتمال قریب به یقین ترنسفورمر هست که اینها میتونن از 100 میلیون تا چند میلیارد باشن
-> مدل برت رو دادم پیاده میکنم روی حدود۶۰۰۰۰ تا url در مدل lstm حدود ۲۵ میلیارد پارامتر داشتم روی خود برت نمیدونم یعنی امتحان نکردم ببینم میتونم سامری بگیرم و تعداد پارامترها رو ببینم یانه Bach size رو الان ۸ در نظر گرفتم
-> سلام و وقت بخیر استاد ممنون میشم اگر روی مدل و داده من نظر یا پیشنهادی دارین بتونین راهنماییم کنيد روی دیتام مدل lstmهم زدم اما مدل برت هست ک انقدر طولانیه و تاالان نتونستم به نتیجه ای برسم
-> من همه پیامهای شما رو خوندم اما هنوز نفهمیدم مدل شما چقدر پارامتر داره دیتاست شما چقدر حجم داره 60 هزار لینک دارید لینکها چی هستن و اصلا موضوع چی هست
-> دیتاست تقریبا ۳۰ مگ حجم داره حدود ششصدهزارتا لینک کامل و خام url وبسایت هست که من حدود پنجاه هزارتاش رو فعلا درنظر گرفتم و مساله طبقه بندی این لینک هاست که امن هستن یا فیشینگ مدل هم از bert base استفاده کردم
-> بازهم همون شد دیگه نمیدونم متاسفانه
-> کجاش نامفهوم بوده نمیدونم شاید بیان من در توضیحات سخت بوده در هر صورت ممنون از وقتی ک گذاشتین"
"-> سلام دوستان یه سوال کلی داشتم خیلی ذهنم را درگیر کرده ممنون میشم اگه اطلاعی دارید در اختیارم بگذارید خیلی از جاها خصوصا اگهی های شغلی تاکید زیادی روی مهندس پرامت نویسی دارند و خیلی روی این تاکید میکنند حتی تو یه مصاحبه که رفته بودم خیلی روی این مانور میداد هرچند توقع من این بود بیشتر بره سمت مدل ها و الگوریتم های هوش ولی خب یکم که بررسی کردم واقعا به چیز خاصی نرسیدم حتی یه بخشی از یه کورس یودمی دیدم یکم قلق و تکنیک بود که چطور بتونیم منظورمون را بهتر به چت چی پی تی و مثل اون بفهمونیم که خوب اکثرش را هم خودم هم میدونم حتی ناخودآگاه خیلی ها که دارند کار میکنند میدوند سوال من اینه آیا واقعا منظور مهندس پرامت نویسی را درست متوجه شدم آخه این که کار خیلی سختی نیست شاخ غول شکوندن نداره که انقدر همه دارند روش مانور میدن یا من بد منظور از پرامت نویسی را متوجه شدم که خوشحال میشم اگه کسی میدونه اطلاعاتش را در اختیارم بگذاره یا خیلی از کار فرماها و بیقه پرت هستند و رو این چیز هایی که ساده هست انقدر مانور میدن
-> همونه مساله اینه که همین رو خیلیا بلد نیستن و نمیتونن درست استفاده کنن
-> سلام من این پست رو امروز تو لینکدین دیدم فکر میکنم به کارتون بیاد مهندسی پرامپت یکی از تکنیکهای مهم برای دریافت جوابهای بهتر و مفیدتر از مدلهای زبانی بزرگ هست و البته برای همهی حوزهها مثلا حل سوال ریاضی استدلال منطقی سیستم توصیهگر و خلاصهسازی لزوما به یک شکل نیست در مقالهی A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks کاربرد بیش از چهل تحقیق در مورد مهندسی پرامپت در نزدیک به سی حوزهی مختلف بررسی شده
-> درمورد prompt eng گفته میشه که یک شغل آینده دار و طولانی مدت نیست به نظر میرسه به صورت موقت در شرایط کنونی به وجود اومده"
"-> سلام صبحتون بخیر مقاله سایت و میشناسید که برای لیست کامل feature engineering توی دیپ لرنینگ معرفی کنید خیلییییی ممنون میشممممممم
-> دیپ لرنینگ اومد که یادگیری ویژگی و استخراج ویژگی اتوماتیک رو جایگزین مهندسی ویژگی کنه مهندسی ویژگی در یادگیری ماشین مطرح میشه نه در یادگیری عمیق دوره یادگیری ماشین دارید که هفته چهارم رو ببینید کل هفته درمورد همین مهندسی ویژگی هست
-> سلام یک سوالی داشتم یه پروژهای داشتم من برای دانشگاه توی کولب هر ایپاک ۲۴ ساعت طول میکشید شک کردم که داره رو CPU اجرا میشه وزنهای شبکه رو پرینت کردم و نشون داد رو GPU عه پروژه رو بردم تو کگل دقیقا با همون کد بدون تغییر ۵ تا ایپاک شد یک ساعت ممکنه وسط ترین از GPU بره رو CPU
-> سلام اون چیزی که برای من اتفاق افتاده بود این بود که کلا جی پی یو قطع میشد
-> سلام احتمالش کم هست اگه دیوایسها متفاوت باشه اغلب با خطای متفاوت بودن دیوایس مواجه میشید کلا با حساب و کتاب میشه فهمیده که یک ایپوک حدودا چقدر طول میکشه معمولا چنین مواردی ناشی از اشتباهات کدنویسی یا کم تجربگی در انجام پروژه هست مثلا چند مورد پیش اومده که دوستان دیتاستشون رو میذارن توی گوگل درایو و بعد از مانت کردن درایو در کولب مستقیم آدرس همون گوگل درایو رو برای خوندن دیتاست میدن خب این کار خیلی خیلی پروسه آموزش رو کند میکنه از اینجور مسائل ریز وجود داره احتمالا البته استفاده از ماژولهای تایم برای محاسبه مدت زمان اجرای بخشهای مختلف کد هم خیلی خوبه البته profiling هم خوبه
-> این که بدیهی هست مدت زمانی که در روز کولب به شما جی پی یو میده بسیار کمه شاید زیر 4 ساعت باشه کلا کولب رایگان دوست نداره روش ترین سنگین بذارید بهتره کارهای اولیه تا قبل ترین اصلی رو اونجا انجام بدید و اگه ترین طولانی دارید از یک سرویس مطمئن استفاده کنید
-> با کولب پرو هم نمیشه همچین تایمی ترین داد با توجه به تعداد کم واحد های محاسباتی ای که در اختیار کاربر میزاره
-> باید حساب کتاب کنی دیگه من وقتی ازش 100 واحد میگیرم برنامه میریزم که به شکلی بهینه ازش استفاده کنم مثلا برای پروژه مدلسازی زبان دیپ کاتالیست داشتم از همین کولب پرو استفاده میکردم سناریو چیدم و آزمایشهام رو در قالب یک لیست آماده کردم یکی یکی انجام دادم و بعدش نتایج رو ثبت کردم از تکنیکهای خلاقانه هم باید استفاده کرد دیتاست بزرگ هست یک بخشی از دیتاست رو بردار و کار رو جلو ببر وقتی به جای خوبی رسیدی اونوقت آموزش روی کل دیتاست بذار
-> من دیتا رو ریخته بودم تو درایو و بعد از مانت دیتا رو خوندم و بعد بردم رو GPU بعد گذاشتمش train بشه اشنباهه نکته عجیبش برام این بود که کد یکسان بود ولی تو کگل درست اجرا میشد حالا بعدا دقیقتر بررسیش میکنم ممنون از پاسختون
-> آره دیگه به خاطر همین مساله طول کشید کگل دیگه گوگل درایو نداشتید برای همین مشکل خودش رو نشون نداد باید دیتاست رو از گوگل درایو کپی کنید توی هارد سیستمی که کولب میده در واقع همون بخش content که همیشه توی آموزش نشون میدم
-> کاش یه ویدئو راجع به کار با کولب و نکته های در رابطه با استقاده اش هم توی یوتوب داشتید خیلی کمک کننده میشد خیلی از بچه ها با کولب مشکل دارن مثل دانلود کردن فایل با دستور و اکسترکت و کار با درایو و نصب پکیج ها و البته که ساده و پیش پا افتاده به نظر میاد
-> باشه انشالله این کار رو میکنم ممنون
-> البته بگم اکثر این چیزایی که گفتم و موضوعات دیگه ای مثل پایش و مدیریت مقدار استفاده از سخت افزار رو هم خیلی خوب توی دیپ کاتالیست توضیح دادین
-> یک مطلب خوبی آماده میکنیم میذاریم یوتوب که همه بتونن ببینن البته به یکسری ویدئوهای کوتاه زیر 5 دقیقه تک نکتهای هم نیاز هست ایشالا بعد اثاثکشی هوسم به یه جای بزرگتر شرایطش رو فراهم میکنیم
-> ممنون مثه همیشه عالی هستید برای اثاث کشی کمک نمیخاید
-> ممنون
-> آهاااااااا ممنونم"
"-> چرا سازمان هوش مصنوعی تشکیل شد پیوست نوشت تشکیل سازمان هوش مصنوعی با هدف هماهنگی و تنظیم فعالیتهای این حوزه در ایران انجام شده است این سازمان که پیشتر به عنوان مرکز ملی هوش مصنوعی فعالیت میکرد حالا مسئولیتهایی مانند راهاندازی اپراتور هوش مصنوعی ایجاد مزرعه پردازشی بزرگ و مرکز داده را بر عهده دارد دلایل تشکیل سازمان 1 بهبود کاربرد هوش مصنوعی ایران در تولید علم هوش مصنوعی رتبه خوبی دارد اما در کاربرد آن نیاز به پیشرفت دارد 2 هماهنگی بین نهادها نبود هماهنگی بین مراکز مختلف تصمیمگیری منجر به تشکیل این سازمان شد 3 چالشهای اخلاقی و امنیتی هوش مصنوعی چالشهایی مانند حفظ حریم خصوصی و امنیت ایجاد میکند که نیاز به رگولاتوری دارد نقشهای سازمان برنامهریزی و هماهنگی تسهیل در اجرای سند ملی هوش مصنوعی قانونگذاری همکاری با مجلس برای تدوین قوانین هوش مصنوعی توسعه هوش مصنوعی تا پنج سال آینده به گفته کارشناسان هوش مصنوعی به سرعت در حال رشد است و ایران نباید از این روند عقب بماند سازمان هوش مصنوعی میتواند به تدوین قوانین و زیرساختها کمک کند هوش_مصنوعی
-> دقیقا درسته همه ریسرچرا مقالاتشونو از ایران میدزدن
-> دیگه به خاطر مورد 3 زحمت نکشید حریم شخصیخصوصی ما اوپن سورسه
-> معمولا کسی تو رقابتی که توش شرکت نمیکنه عقب نمیمونه اما خب ما که سعادت گفتن این حرفا رو نداریم امیدوارم حداقل تو روند کارایی که الان داره انجام میشه سنگ اندازی نکنن
-> فردا میان یه قانون تصویب میکنن میگن توسعه LLM با بالای ۱۰۰ میلیون پارامتر ممنوعه جلوگیری از مصرف بی رویه پارامتر"
"-> متوجه شدم این روش تحت عنوان weakly supervised مطرحه چرا ویکلی حالا چون شما یسری اطلاعات دارید ولی کامل نیست برای این امر باید مقالات ویکلی سوپروایز رو مطالعه کنید ۴ سال پیش بحث ترندی بوده کارای جالبی انجام شده
-> ممنون از راهنمایی تون"
"-> دوستان نظرتون درمورد آینده LLM ها چیه
-> نظری ندارم
-> نظرات دکتر لکون خیلی عمیق و گیرا هست در رابطه این موضوع پیشنهاد میدم اونارو بررسی کنید"
"-> سلام اقای دکتر یه سوالی دارم ببخشیداگه خیلی پیش پا افتاده هست ولی منو گیج کرده واقعا یه طبقه بندی باینری می خوام انجام بدم که مثلا تصویر سیب رو شناسایی کنه یعنی وقتی یک تصویر رو بهش میدیم بگه که این سیب هست یا نیست حالا سوالم اینه که دیتاست من فقط باید شامل یک دسته از تصاویر سیب باشه یا باید دیتاست دو دسته داشته باشه که شامل تصاویر سیب و غیرسیب هست
-> فکر می کنم فقط تصاویر سیب کافی باشه من ی لحظه ذهنم رفت سمت ریکاگنیشن انگاری یکم مسله تون با دسته بندی متفاوت مثلا اگه تعداد سیب هاتم کمه و تنوع سیب ها بالاعه میتونی از روش های زیرو شات یا وان شات لرنینگ استفاده کنید البته اگه صورت مسله رو درست فهمیده باشم
-> سلام فکر کنم مساله واقعیتون سیب نیست ممکن هست با تغییر ماهیت مساله جواب متناسب با مساله اصلی دریافت نکنید اگه واقعا سیب مساله اصلی هست بگید سناریو چی هست لازم به توضیح بیشتری هست
-> اگه اشکالی نداره پیوی پیام بدم
-> پیوی رو من چک نمیکنم
-> من می خوام در واقع دیتکشن برای سیب انجام بدم یعنی در یک تصویر شلوغ مکان سیب رو شناسایی کنم از روش های یولو و faster rcnn نمی خوام استفاده کنم"
"-> سلام استاد من بخش اول دوره بینایی کامپیوتر رو خریداری کردم بخش دوم رو می خواستم تهیه کنم اما ثبت نامش بسته شده امکانش هست راهنمایی بفرمایید
-> سلام علی جان به پشتیبانی پیام بده لطفا"
"-> سلام وقت بخیر برای آموزش و پیاده سازی spatiotemporal graph convolution network چه منابعی پیشنهاد می کنید من هنوز دوره Deep رو تموم نکردم و نمی دونم چقدر داخل مبحث اشاره شده
-> سلام ما درمورد شبکه گرافی صحبت کردیم این فصل میتونه در درک شبکههای گرافی کمکتون کنه اما بهصورت خاص درمورد موضوع شما در دوره صحبت نشده یادم نمیاد دورهای چنین مبحثی رو گفته باشه اما من موضوع شما رو گوگل کردم یکسری مقاله survey وبلاگ و ویدئو برام آورد شاید در یک کورسی هم این مبحث رو پوشش داده باشن اما بهصورت کلی شخصا وقتی درگیر یک موضوع خاص میشم از وبلاگ و ویدئوهای کوتاه شروع میکنم بعد پیادهسازیهای ساده ازش میبینم نهایتا میرم سراغ مطالب سنگینتر مثل مقاله و survey و احتمالش کم هست که موضوعهای خاص رو در یک دوره بتونید پیدا کنید و فرضا یک فصل از دوره باشه
-> تشکر استاد برای GNN چه کورس یا کتابی پیشنهاد می کنید داخل Udemy دوره زیاد پیدا می کنم از کیفیت دوره ها یا کتاب ها هیچ اطلاعی ندارم
-> فصل یادگیری عمیق برای شروع خوبه دوره نمیدونم شایدم نیاز نباشه"
"-> چند ماه پیش تیم ما موفق شد سایت رسمی ما رو به طور رسمی با هدف قابل دسترس تر کردن ابزار های هوش مصنوعی به طور خاص تر LLM های مختلف راه اندازی کنه ما با هدف اینکه بتونیم به کاربرای ایرانی کمک کنیم که بدون سختی به LLM ها دسترسی پیدا کنن سرویس اوپن سورسی رو راه اندازی کردیم که مدل های مختلفی رو به طور رایگان توش ارائه میدیم به طور کلی ما در حال حاضر 13 مدل رو ارائه میدیم ما در حال حاضر دوتا مدل اصلیمون یکیشون GPT4 فاین تیون شده با دیتای مخصوص مربوط به سایت هست و یکیشون هم یکی از مدل های معروف هاگینگ فیس هست و همچنین در کنار این دو مدل ما مدل های دیگه ای هم ارائه میدیم مدل های openai 1 GPT 4 O mini 2 GPT 4 3 GPT 35 turbo نسخه 1106 4 TEXT TO SPEACHTTS از openai__ __ __ مدل های open source 1 Llama 3 70B 2 Llama 3 8B 3 Aya 13B 4 Gemma 7B 5 Zephyr 7B 6 Mistral 7B برای استفاده از سرویس ما میتونین از طریق سایتمون اقدام کنین وقتی وارد سایت بشین با یه ویدجت ابی رنگ مواجه میشین که میتونین از طریقش از مدل ها استفاده کنین همچنین از طریق این آدرس میتونین به طور مستقیم به مدل ها دسترسی پیدا کنین همونطور که گفتم این پروژه اوپن هست و هنوز درحال توسعه ست ما در تلاشیم که مدلای بیشتری رو به سرویسمون اضافه کنیم و صرفا به مدل های متنی اکتفا نکنیم پیشنهادات و فیدبدک های شما خیلی میتونه به ما تو بهتر کردن سرویسمون کمک کنه
-> لطفا این پیام رو برای من بفرستید"
"-> وقتی dataset کوکو رو ترین میکردم به همین مشکل خوردم و حتی چند تا کارگر که استفاده کردم هم اونقدر بهینه نبود که میخاستم مجبور شدم از روش های دیگه استفاده کنم اگه خودت نمیتونی درست کنی حتما از جمینای بپرس
-> سپاس گزارم لطف کردید"
"-> داخل هر پوشه حداقل ۲۰۰ به بالا فریم هست
-> دسته بندی کلاس هاست فریم که نه تصویر دیگه فریم میگید یعنی دادهتون ویدئو هست
-> قراره کار ردیابی انجام بدم دیتاستی که داریم مثلا پوشه toy1 شامل ۳۷۶ دنباله عکس که همون فریم های ویدیوعه شبکه رو با این دیتاست ترین و تست ترین کنم که بتونم برای استخراج ویژگی استفاده کنم
-> دوستان عزیز کسی ایده ای برای راهنمایی نداره
-> سلام استاد وقتتون بخیر در کدوم دوره و کدوم بخش نحوه ترین یه شبکه vgg رو اموزش دادید حالا اگر خود همینم نبود و چیزی تو مایه های این بود لطفا بگید یه ماه موندم چطوری با دیتاست خودم شبکه رو ترین کنم خواهشا راهتمایی کنید
-> سلام ببینید ترین کردن یک مدل مثل vgg resnet و غیره که مثلا در دوره دیپ لرنینگ دیپ کاتالیست و بینایی کامپیوتر هست اتفاقا توی اینترنت هم به وفور فارسی و انگلیسیش هست اما دو تا نکته هم رو درنظر داشته باشید اول اینکه به صورت خاص دنبال یک کد یا آموزشی باشید که مدل vgg ترین کرده باشه به احتمال خیلی زیاد مشکل شما رو حل نمیکنه باید مفاهیم پایه دیپ لرنینگ رو خوب یاد بگیرید نکته دوم شاید این پروژه شما نیاز به ترین کردن مدل در شرایط خاصی داشته باشه به عنوان مثال ممکن هست ترین کردن به صورت کلاسیفایر براتون مناسب نباشه دقت کنید گفتم ممکن هست که مناسب نباشه من چون پروژه شما رو خوب درک نکردم نمیدونم راه حل کارتون چی هست میگم اینطور نباشه که برید یک مدلی هم ترین کنید و بعد یک ماه بفهمید اصلا نباید مدل را با این سازوکار ترین میکردید من برای شما مشاوره گرفتن از یک فرد متخصص رو مناسب میبینم کسی که بتونه کارتون رو بشکونه و راهنماییتون کنه نمیدونم شاید الانم دارید و طبق توصیههای ایشون دارید عمل میکنید
-> خیلی متشکرم از لطفتون نه متاسفانه کسی رو پیدا نکردم شما کسی رو میشناسید که معرفی کنید
-> دکتر جان کلیت کار اینطوره من میخوام این شبکه رو ترین کنم که وقتی از دیتاست تست میخوام ردیاب ترکر رو اجرا کنیم از قبل ویژگی هاش از این شبکه استخراج شده باشه بعد کمک کنه ک ردیابی مقاوم تری داشته باشه الان این بنظرتون نیاز به ساز و کار خاصی داره
-> مطمئن نیستم ممکن هست به ترین شبکه مبتنی بر سیامیز نیاز داشته باشید
-> متشکرم"
"-> استاد حجم دیتای من ۳۲ گیگ هست کولب پرو جوابگو هست
-> شبکه چیه
-> 
-> از روی دیتا نمیشه فهمید که چه سخت افزاری نیاز دارید علاوه بر اون جنس دیتا شبکه میزان پارامترهای شبکه و بچ سایزهایی که توی کد دارید در تعیین سخت افزار برای ترین نقش داره منم هیچ اطلاعاتی درمورد فاکتورهایی که گفتم ندارم به چت جی پی تی این اطلاعات رو بدید شاید بتونه کمکتون کنه یا از یک فرد متخصص که بهش اعتماد دارید بپرسید
-> ممنون کسی رو نمیشناسم
-> سلام استاد دیتای من ۳۵ گیگ هست و میخواهم با vgg19 ترین کن و برای فیچر استکرشن استفاده کنم کجا پیشنهاد میدید برای ترین کردن
-> کولب بد نیست زمانش محدوده ولی میتونید کار کنید ببینید کلا کارهای شما دو بخش داره یک بخش تحقیق و توسعه و دو بخش آموزش نهایی از اول به اون بخش آموزش نهایی فکر نکنید درصدی از دادهتون رو بردارید و کار رو از اول تا آخر پیاده سازی کنید و یک نتیجه اولیه بگیرید بعدش که مطمئن شدید همه چیز درست هست ترین طولانی بذارید حالا برای اون ترین طولانی نهایتا یک پولی میذارید کنار و یک سرویس پولی میگیرید مثلا من برای پروژه پردازش گفتار کل پروژه رو با سیستمی مثل کولب جلو بردم دیدم همه تستهای اولیه نشون میده که پیاده سازی درست هست و شبکه داره یاد میگیره رفتم یک سرویس داخلی گرفتم که ترین طولانی و حجیم بذارم
-> از كولب بيزارم هرموقع ترين ميكرديم موقع ترين رو از جي پي يو ورميداشت واقعا اذيتم ميكرد
-> یک دغدغهای که دارم این هست که صرف یادگیری عمیق یا یادگیری ماشین بخشی از مشکلات بچهها رو حل میکنه در استراتژی پروژه مشکل زیاد وجود داره کسی هم بالای سر بچهها نیست که هندلشون کنه باید اینا رو در قالب ویدئو یا ویسهای کوتاه یک جایی منتشر کنم
-> اهاان یعنی میفرمایید که نیازی نیست روی کل دیتا شبکه رو ترین کنم بعد یه موردی من یک پوشه دارم که حاوی چندین پوشه هست و خود هر کدوم از این پوشه های فریم های ویدیو من هست که میخوام شبکه رو ترین کنیم ۳۴ تا از این پوشه هارو قرار بدم بعد من فقط تست و ترین دارم برای ولیدیشن از روش کراس ولیدیشن استفاده کنم امکانش هست یه توضیح کلی در مورد نحوه کاری که باید بکنم بدید واقعا گیج شدم در ضمن دیتای من همه گری اسکیل هستند
-> یه پیشنهاد دارم براتون چون حجم داده اتون زیاده باید از هارد لود کنید و این لود کردن زمان زیادی میگیره یه بچ جنریتور طراحی کنید که همیشه درون خودش 10 یا 15 بچ ذخیره داشته باشه و وقتی که فراخوانی میشه از رم برداره نه از هارد این شکلی زمان لود کردن داده اتون باعث نشه ترینتون خیلی طولانی بشه و اگه کولب پرو استفاده میکنید هزینه اتون فقط برای ترین باشه نه لود کردن"
"-> سلام وقت بخیر در دوره دیپ کاتالیست برای اجرای کدها از سرور مجازی استفاده شده در کدوم بخش از ویدیو ها هست
-> سلام کل دوره روی کولب هست
-> من کدی رو میخواستم اجرا بگیرم رو کولب به خاطر کمبود حافظه جواب نمیده برای اجرای کد ممنون میشم راهنمایی داشته باشید که نحوه استفاده از سرور چطور هست اموزشی داره
-> سرور از کجا گرفتید
-> نگرفتم میخوام بگیرم ولی اطلاعاتی ندارم اگه راهنمایی داشته باشید ممنون میشم
-> فعلا اول باید بگیرید و بعدش برسید به نحوه استفاده ازش ایشالا وقتی دیگه صحبت از اجاره سرور میشه بحث هزینه هم مطرحه بد نیست قبلش با یک فرد متخصص درمورد پروژهتون صحبت کنید و مطمئن بشید که کولب دیگه کارساز نیست برای شما من کولب پرو رو پیشنهاد میکنم چون با دردسر کمی میتونید به سختافزار قویتر دسترسی داشته باشید اما آرتین هم قبلا runpod رو پیشنهاد کرده که یک متن طولانی هم نوشته و توی همین گروه هست داخلیها هم من قبلا یک بار استفاده کردم راضی نبودم نمیدونم از اون موقع اوضاع بهت شده یا نه همچنین نمیدونم جاهای دیگه در داخل کشور چطورن بازم آرتین اطلاعات خوبی داره
-> سلام استاد جديدا اين وبسايت lightning ai گرفتم واقعا راضي بودم و همچنان دارم ازشون استفاده ميكنم تونستم دقت خوبي بدست بيارم به نسبت قبلي و چندين مدل هاي مختلف رو تونستم ترين كنم
-> برای من شماره یکبار مصرف قبول نکرد
-> والا واسه منم پيش اومده اين مورد تازه متوجه شدم به خاطره يه سري اي پي حساسن من شماره از رفيقم گرفتم كه المان بود اي پي انگليس رفتم قبول نكرد و اي پي المان رو تست كردم جواب داد بعد از اون جريان ميتونيد هر اي پي استفاده كنيد"
"-> این ادبیات هم واقعا عجیبه برای یک دوره
-> به نظرم آناکوندا بهتر تربیت میکنه
-> میتونیم از تمساح های فلوریدا هم کمک بگیریم"
"-> سلام استاد بنده عادت دارم پست های جالب که نکته ای در اون نهفته هست ذخیره میکنم تا داشته باشم و حتی بعدا خیلی از آنها را بیشتر بررسی کنم و هم یک پلی لیست غنی از نکات داشته باشم ولی چند وقتی است خیلی از مطالبی که در گروه گفته شده را نمیتوانم برای خودم فوروارد کنم یا اسکرین بگیرم اول فکر کردم به خاطر اپدیت تلگرام هست ولی فقط هم همین گروه این طوری شده دلیل خاصی داره اتفاقی افتاده
-> سلام فکر می کنم کلا اجازه فوروارد و کپی گروه بسته شده چون برای من هم همینطوره به عنوان یه راه حل جایگزین برای اینکه مطالب مفید رو از دست ندید می تونید copy link را بزنید و در saved messages بفرستید و برای اینکه یادتون باشه اون پیام یا مطلب راجع به چی بود و بتونید راحت تر پیداش کنید یه ریپلای هم به اون لینک برای خودتون بزنید و مثلا چند کلمه در حد عنوان و موضوع بنویسید اینطوری با زدن لینک هدایت میشید به همون پیام موجود در گروه
-> سلام احتمالا به زودی بازش میکنیم"
"-> ۱ اون دوره چی بوده مگه که ۳۳ میلیون ۲ چرا باید تفکر این باشه که هیچ دوره ای ارزش هزینه رو نداره ۳ چرا کسی که اموزش میده رو قضاوت میکنن ۴ چرا فکرمیکنن دوره موثر نیست ولی یادگیری از یوتوب و منابع رایگان موثره ۵چرا باید دانش مدرسان همه دوره های آموزشی زیر سوال بره توسط کامنت مدنظر
-> اینکه هزینه دوره ای بالاست دلیل بر خوب بودنش نیست معمولا سیلابس و تیم ی که دوره رو برگزار می کنند مهم اینکه چقدر در اون زمینه سابقه فعالیت دارند بنظرم دوره های یوتوب فقط برای قدم اول خوبند طبق تجربه اندکم کارآموزی و چالش های صنعت بهترین دوره آموزشی با فرض اینکه فاندامنتال اون زمینه رو پشت سر گذاشتی است
-> بجز جمله اول و بخشی از جمله دوم تقریبا با بقیه صحبتهای ایشون موافقم من هم توی آموزشها و گروههای مختلف همینها رو میگم ایشون در جمله اول واقعیت رو گفتن ولی همه واقعیت رو نگفتن وقتی میگن خودشون و دوستانشون یک ریال هزینه نکردن این رو هم بگن که کجا بزرگ شدن کجا درس خوندن تا چه مقطعی درس خوندن چه دانشگاههایی بودن و چه رشتهای درس خوندن من هم برای این حوزه تخصصیم یک ریال خرج نکردم اما در بهترین دانشگاههای ایران درس خوندم به واسطه دانشگاه و دوستانم به راحتی اولین شغلم رو پیدا کردم همواره اطرافم افراد باهوش و باسواد بودن و من رو به سمت جلو هل دادن استاد راهنمای ارشدم بود که آینده من رو ساخت اگه بدون این واقعیات بگم ریالی هزینه نکردم یعنی بخش مهمی از واقعیت رو نگفتم همه واقعیت رو نگفتن هزینه داره میتونه منجر به گمراهی افراد بشه و واقعا هم میشه لیبل افراد روی ذهنیت ما در پذیرش حرفهاشون اثر مستقیم داره به نظرم ایرادهای دیگهای هم وارد هست
-> من هم معتقدم که رایگان بودننبودن یک منبع لزوما به معنای ارزشمند بودن نبودن اون نیست بسیاری از دورههای آموزشی با کیفیت حاصل تلاش و تخصص افراد هستن و خب ارائه اونها به صورت رایگان منصفانه نیست حالا هر کسی با توجه به سبک یادگیری و نیاز خودش میتونه از بین منابع انتخاب کنه اینکه به طور کلی همه پکیجهای آموزشی رو بیارزش و بیفایده بدونی ادعای درستی نیست پکیجهای آموزشی باکیفیت میتونن ابزار مفیدی برای یادگیری باشن به شرطی که با دقت انتخاب شده و از منابع معتبر تهیه بشن یکی دیگه از نکتههای بد قضیه اینجا بود که فردی کامنت میذاره و به طور کلی دانش همه افرادی که آموزش ارائه میدن رو زیر سوال میبره که این کار هم منصفانه نبود
-> اگه حضوری باشه منطقی تره چون حضوری گرون تره چون هم دستمزد مدرس هست و هم هزینه مکان برگزاری و هم هزینه های جانبی و سود اموزشگاه توی یه شهر بزرگ مثه اصفهان که باز از پایتخت ارزونتره اموزش پایتون بین 6 تا 8 میلیون به ازای 60 تا 80 ساعت و اموزش هایی که به اسم هوش مصنوعی برگزار میکنن هم بین 12 تا 25 میلیون هست بخایم جمع کنیم احتمالا یه مبلغ نزدیک به اون چیزی که آقا گفته میرسیم
-> خوب حساب کتاب میکنیا
-> آخه همین میشه یه حسابی کنیم یه مدرس بخاد ساعتی 200 تومن بگیره برای برنامه نویسی باید آموزشگاه چقد بگیره واسه یه دوره که ده هفته حداقل فضاشو اشغال میکنه معمولا هم کلاس ها بین 4 تا 5 نفرن حالا برای هوش مصنوعی که مدرسین زیر 400 تومن هم نمیگیرن و بخان حداقل 100 ساعت هم تدریس کنن
-> درسته"
"-> چطوری فرآیند ترین سخت تر میشه یا چطوری باعث میشه مقادیر تو یه بازه مشخص قرار بگیرن میشه استدلال کرد
-> فکر میکنم چون نرمالیزیشن قبل ماژول residual قرار میگیره باعث میشه دامنه مقادیر فیکس بشه و این باعث میشه گرادیان ها هم مقادیرشون معقول تر بشه
-> منم همین طور فکر می کنم چون قبل از این که Transformation اعمال بشه گرادیان ها consistency بهتری دارن و اون اسکیپ کانکشن نتایج نرمالیزه شده رو جمع می کنه و نه نتایج نرمالیزه نشده رو
-> شاید دقیق تر این موضوع این باشه که وابستگی رو مقادیر ورودی کمتر میکنه و نوسان رو توی مقادیر ورودی کم میکنه و نوسان توی پروسه اموزش هم کم میکنه و فکر کنم این امر باعث میشه بتونیم مقادیر لرنینگ ریت رو هم بزرگتر کنیم چون ناپایداری رو کم کردیم"
"-> سلام روز همگی بخیر من میخام کار با شبکه های یادگیری عمیق روبصورت عملی شروع کنمکد هام باید رو جی پیو ران شهالان دانشگاه یه سیستم داره که جی پیو قوی داره ولی این سیستم خام هست وهیچ مناسب سازی نشدهکسی میتونه یه منبعی برای آموزش معرفی کنه که چه مراحلی بایدانجام بدم که سیستم آماده ران گرفتن شه سپاس
-> 
-> سلام به صورت عملی یاد بگیرید یعنی چی شما که یادگیری عمیق رو دارید نیازتون رو رفع نمیکنه نصب و اینجور مسائل رو هم گفتیم دیگه
-> بله استادشما فوق العاده عالی درس دادیدمن تا الان با سیپیو کار کردممنتها تو مباحث خیلی ساده و کم حجمالان که حجم کارم بالاست میخاستم از جیپیو استفاده کنمتو سیستم دانشگاه زدمگفت این پایتورچ باکودا ۱۲۴ همخونی ندارهبه سایت پایتورچ رفتمتمام ورژن های پایتورچ بااین کودا همخانی نداشتبجز ورژن جدید اش که هنوز استیبل نشده که شما توآموزش هاتون فرمودید از آخرین نسخه چون هنوز کاملا باگ هاش مشخص ورفع نشده استفاده نکنیمسراغ یه سیستم دیگه رفتماصلا کودا نداشت و گفت باید نصب کنی وو کلا تو این زمینه کم سواد هستم ونمیدونم میشه کودارو تغییر دادباید براساس چه ویژگی های جیپیو این تصمیم رو گرفت وکلا من گیج شدمگفتم یه آموزش تمیز ببینم که کامل مسلط شمعلاوه بر اون تو کولب مشکلاتی داشتممثلا توابعی که تو کداصلی فراخانی میشدبه تعبیر متلبی ام فایل های فانکشن رو نمیدونستم چطور بهش معرفی کنمیا اینکه بعضی ام فایل ها اسمشون مثلاconfigبود که نمیدونم این اسم رو کولب دیفالت داشت یا مساله چیز دیگه ای بود و خطا میداد آناکونداهم خیلی کوتاه رد شدیم ازش تو دوره یادگیری عمیق رفتم سراغ آموزش پردازش تصویرشمااونجا پایچارم رو مفصل تر گفته بودیدرفتم سراغ پایچارم و یه سوال دیگه استاد تو فصل هایی که من تاالان از دوره دیپ لرنینگ دیدم بیشتر بحث کلاس بندی بوداگر من قسمت سگمنتیشن بینایی کامپیوتر رو ببینممیتونم تعمیم بدمتو بینایی کامپیوتر سورس کد ها رو پیدا نکردم کلا الان دنبال آموزشی هستم که اگر بخام خودم یه سیستم بخرم و صفر تاصد راه اندازی جیپیو به عهده خودم باشهبتونم برای کارهای پایان نامه ام راهش بندازم سپاسگزارم از شما
-> 
-> در ادامه ویس بالا بگم که من خودم با این مساله درگیر بودم شما دو تا متغیر نداری بلکه سه تا داری نسخه کودا نسخه تورچ و نسخه درایور کارت گرافیک خودم وقت گذاشتم برای نصب کودا روی سرور ریموت که نشد و بعد همکارم تلاش کرد و نشد خیلی آموزش و بلاگ و chatgpt رو زیر و رو کردیم و نشد در نهایت این تلاشهای ما باعث شد سرور کرش کنه و بعد از مشورت با cto نهایتا راضی شد که از لینوکس بریم روی ویندوز روی ویندوز با آخرین درایور کارت گرافیک کودا ۱۱۸ و تورچ ۲۰۰ اوکی شد شاید برای شما ترکیب دیگهای نیاز باشه با توجه به کارت گرافیک با این وجود احتمالا همین جواب بده و اينکه من اینقدر توش عمیق شدم قطعا باید راهی برای نصب روی سرور لینوکس هم باشه که من بلد نیستم اگه شما بودید که سگمنتیشن میخواست باید بگم که این مرحله رو که رد کنی وارد نصب دتکترون میشی که خودش داستان داره پیشنهاد من البته دورههای مهندسی پرامپت هست به شما که اون هم حتی شاید با آزمون و خطا به نتیجه برسی و نیاز به دوره نداشته باشه
-> سپاسگزارم
-> ممنون از توصیه ارزشمند شما استاد"
"-> دوستان فرق این دو تا چیه به چه کار آید
-> سلام بر اساس تصویر محل قرار گیری Layer Normalization این که قبل از residual connection یا بعد از residual connection قرار بگیره تفاوت این دو ماژول هست فکر کنم در post normalization جاری شدن گرادیان کمی سخت تر باشه و فرایند ترین کمی سخت تر شه چون بعد از residual connection نرمالیزیشن اعمال می شه توی ترنسفورمر فکر می کنم همون pre norm رو داشتیم و عمدتا شبکه ها از pre norm استفاده می کنن به خاطر gradient flow بهتر شایدم کلا اشتباه نوشته باشم مطمئن نیستم واقعا
-> به نظر میرسه که تو pre norm کلا به پایدار تر بودن مقادیر گرادیان ها کمک میکنه و باعث میشه مقادیر تو یه بازه مشخصی قرار بگیرن و درنهایت اموزش متعادل تری داشته باشیم این چیزیه که من متوجه میشم
-> منم همین طور
-> postnorm residual unit x_l1 LNx_l Fx_l prenorm residual unit x_l1 x_l FLNx_l در شبکههای ترنسفورمری با prenorm residual unit وقتی تعداد لایهها زیاد باشه کمتر مستعد exploding vanisging gradients هستند با نرمالسازی ورودیهای هر لایه قبل از عملیات اصلی مشکلات مربوط به گرادیان رو کاهش میده توی مقاله اصلی ترنسفورمر لایه نرمالیزیشن بعد از مولتیهد اتنشن بود بطور مثال توی این مقاله نشون دادن که لایه نرمالیزیشن قبل از مولتیهد اتنشن باعث بهبود همگرایی میشه
-> مهندس لطف می کنید اسم مقاله رو بفرمایین تا منم بخونم ممنون می شم
-> On Layer Normalization in the Transformer Architecture
-> دست شما درد نکنه
-> نظر من در مورد سوالی که پرسیده بودم"
"-> سلام دوستان وقت بخیر من یک تسک classification دارم که کلاس هام خیلی آنبالانس هست با over sampling و under sampling هم نتونستم روی کلاس های minor ام نتیجه خوبی بگیرم کسی نظری داره چه کار کنم
-> BREAK_TYPE 0 964544 1 4547 2 3041 3 2835 4 2657 5 1216 6 562 7 177 8 152 Name count dtype int64
-> داده چی هست مدل چی هست
-> Tabular Kernel extreme learning machine
-> این رو هم اضافه کنم که وقتی از الگوریتم SMOTE میخوام استفاده کنم داده synthetic بسازم برای کلاس های minor ریسورس کافی ندارم و سشن کرش میشه
-> این روش رو حتما تست کنید حالا چالشهایی هم داره که سعی کنید حلش کنید حدس میزنم مجبور هستید از این مدل استفاده کنید اما اگه مجبور نیستید بوستینگها رو پیشنهاد میکنم xgboost و
-> خیلی ممنونم از پاسختون حتما ببخشید SMOTE رو وقتی دیتا بزرگ هست هم میشه استفاده کرد این ابعاد دیتا من هست 979731 23
-> بزرگ و کوچک که نداره مساله اینجاست که الان رم شما اجازه نمیده 9 تا کلاس دارید که با اورسمپلینگ میخوایید اینها برابر بشن یعنی همشون باید به اندازه کلاس 0 بشن یعنی میشه حدودا 9 میلیون سمپل که خب زیاده باید روشهای مختلف رو تست کنید مثلا میخوایید کارایی smote رو در بهبود عملکرد مدل دسته بندی تست کنید پس یکی از آزمایشهای اولیه شما میتونه این باشه که از کلاس 0 به اندازه کلاس 1 سمپل رندوم بردارید و ببینید در این شرایط smote چیکار میکنه یعنی حجم سمپلهای کلاس 0 رو کم کنید از طرفی یکسری آزمایش دیگه برای سایر روشها میتونید داشته باشید مثلا روش nearmiss یا موارد دیگه نمیدونم این روش smote نسخه مینی بچ هم داره یا نه نسخه مینی بچ هم میتونه مشکل رم رو حل کنه درنظر گرفتن وزن برای تابع اتلاف هم روشی هست که خیلی موثره آزمایش طراحی کنید از ساده تا مشکل یکی یکی انجام بدید نتایج رو یادداشت و تحلیل کنید براساس اینها تحقیق کنید متدها و راهکارهای جدید پیداطراحی کنید آزمایش کنید نتایج رو تحلیل کنید و
-> یک دنیا ممنونم از وقتی که گذاشتید و اطلاعات مفیدی که بهم دادین چشم همین کار رو میکنم
-> من فکر میکنم ترکیب undersampling و oversampling جواب بده امتحانش کنید
-> خیلی ممنونم"
"-> سلام دوستان توی داده های جدولی اگر یک سری ستون ها باینری باشن یعنی فقط صفر یا یک داشته باشن و یکسری های continues باشند و بخوایم نرمالیزیشن انجام بدیم با standard scaler باید فقط همون continues ها رو نرمال کنیم یا باید کل دیتا یعنی هم continues ها و هم باینری ها رو
-> این دست شماست که ببینید چه ستونهایی نیاز به پیشپردازش دارن ستونی که فقط 0 و 1 هست رو میتونید پیشپردازش نکنید
-> ممنون آقای دکتر اینطوری چون اسکیل ها متفاوت میشه باعث نمیشه مدل به یک کدوم ارزش بیشتری بده
-> مثلا چه مدلی
-> اینکودر یک شبکه ترانسفورمر
-> نه آخه فیچرها باهم که مقایسه نمیشن میزان اثرگذاری هر فیچر با سمپلهایی که داره و اثری که روی خروجی میذاره سنجیده میشه تازه ما اون فیچرهایی که به نرمالیزه نیاز دارن رو به این دلیل نرمالیزه نمیکنیم که ارزششون نسبت به بقیه بیشتر نباشه به خاطر مقیاس بزرگ دادهها و بزرگ شدن فضای جستجو و اینجور مسائل هست که نرمالیزه میکنیم
-> خیلی ممنونم آقای دکتر"
"-> 
-> منظور از بازیابی اطلاعات چی هست
-> منظور Infromation Retrieval هست"
"-> سلام دوستان کسی اطلاع داره کدوم یکی از مسابقات سایت kaggle رو شرکت کنیم خوبه که تو گیتهابمون بذاریم برای تقویت رزومه
-> سلام منظورتون از کدوم یکی چی هست
-> اخه سایتش کلی مسابقه داره و خب نمیدونم کدوم مسابقاتش خوبن منظورم اینه که سطح خوبی داره و میشه بعد از شرکت توی رزومه و گیتهاب اشاره کرد بهش
-> 
-> خیلی ممنونم از توضیحاتتون سپاس"
"-> سلام دوستان کسی مقاله در زمینه هوش مصنوعی نوشته من چند تا سوال در مورد نحوه نوشتن و روند آن میخواستم بپرسم
-> سلام شاید بهتر باشه سوالاتتون رو جمع و جور کنید و بفرستید اینطوری افراد بیشتری ترغیب میشن که بهتون کمک کنن
-> سلام وقت بخیر من میخواستم مقاله ای در زمینه هوش مصنوعی بنویسم می خواستم راهنمایی داشته باشید که برای نوشتن مقاله و پیاده سازی اون مقاله چه دوره هایی رو باید گذرونده باشم که بتونم یک مقاله بنویسم چه موقع یک موضوع نواوری حساب میشه و نواوری رو چطور باید پیدا کنیم کلا ممنون میشم در مورد نحوه نوشتن یک مقاله از صفر تا صد توضیحی داشته باشید
-> این چند تا سوال جوابش یک کورس میشه در زمان خالیم سعی میکنم بهش جواب بدم البته دوستان زیادی در گروه هستن که تجربه مقاله دارن امیدوارم اونها لطف کنن و به سوال شما جواب بدن
-> ممنون میشم دوستان راهنمایی داشته باشند"
"-> سلام استاد امروز يه سري با پايتورچ يه مدل mlp تست كردم نتيجه گرفتم چندتا optim رو تست كردم فعلا دوتا رو Adagrad Adadelta اين دوتا رو هايپرپارامتر رو تنظيم كردم و چندتاشو خوب نتيجه داد به نظرم خيلي چيزا مونده كه بايد تست كرد بقيه رو من باور دارم اين پايتورچ بهتر از قبلي كه مدل ساختم باشه اگه پيشنهادي داريد ممنون ميشم
-> سلام منظورتون مسابقه هست دادههای تبولار معمولا قلمروی پادشاهی بوستینگ هست
-> بله منظورم مسابقه بود جالبه
-> معمولا از sgd adam adamw برای ترین شبکه استفاده میشه"
"-> سلام دوستان بخش دیکدر ترنسفورمرز میدونین کدوم قسمت دوره دیپ پیاده سازی شده
-> تو توضیحات اومده ولی تا جایی که یادمه فقط از قسمت انکودرش استفاده شده بود"
"-> سلام بچه ها اين كد من در كگل هستش تا جاى كه تلاش كردم درصد دقت رو رسوندم ٠٨٩١ همچنان تلاشمو ميكنم درهرصورت اين ديتا بسيار چالشى و پر حجم هستش تجربه ى من كه بهتون ميگم يه سرى كتابخانه ها تست كردم همچنين هايپرپارامتر تنظيم كردم به لطف استاد و تنها دوتا مدل رو تونستم خوب نتيجه بيارم يكى lbgm و xgboost اين بار catboost رو تست كردم ولى به نظرم كافى نبود در هرصورت تلاشمو ميكنم از اين بهتر نتيجه بدست بيارم تنها يه مشكلى كه هست sklearn هستش كه كودا نداره من چندين مدل كلاسيفيكشن رو تست كردم بسيار زمان برد پيشنهاد نميكنم اگه بخواييد ديتاى پرحجم رو ترين كنيد و بعد از اينكه ترين كردم درصد دقت اونقدر جالب نبود شايد با هايپرپارامتر نياز داشت كه تنظيم شه ولي اونقدر طول ميكشه كه ارزش نداره اميدوارم اين مورد براتون مفيد باشه لينك كد من رو زير ميزارم براتون با تشكر
-> سلام من میخواستم کدتون رو ببینم ولی انگار private هست لطفا public کنین بتونیم ببینیم
-> مرسى كه گفتيد الان تغيير دادم به عمومى با تشكر
-> ممنونم"
"-> سلام دوستان وقت بخیر میشه کولب رو جایی غیر از مرورگر باز کرد نرمافزار یا ریموت روی ide یا هر روشی بجز خود مرورگر
-> از طریق ssh میشه به vscode وصل کرد من قبلا تلاش کردم موفق نشدم اما راهکارش رو که گفتم بعضیها از دانشجوها از خارج کشور تونستن وصل بشن توی اینترنت هم سرچ کنید مطالب زیادی براش هست"
"-> سلام آقای دکتر شب تون بخیر میخواستم ببینم از xai برای action recognition هم میشه استفاده کرد و اینکه الان چه مباحثی در action recognition ترند هست
-> سلام نمیدونم xai چی هست
-> سلام مخفف explainable ai هست مثلا captum یکی از کتابخونه های این حوزه هست که روی مدل های پایتورچی کار میکنه
-> والا تعریفی که من از Explainable AI میدونم این هست Explainable AI is a set of tools and frameworks to help you understand and interpret predictions made by your machine learning models بحث تفسیر درک و اطمینان به مدل در تصمیمگیری هست همون captum هم شعارش Model Interpretability for PyTorch هست نمیدونم جز تفسیرپذیری کاربردی دارن یا نه در اکشن هم نمیدونم
-> تاجایی که من میدونم بیشتر به دنبال درک و تفسیر وقایع هست مثلا در حوزه پزشکی اگر تصاویر دسته می شوند به کمک XAI میشه به این درک رسید که چه عواملی باعث شده این اتفاق بیوفته
-> منظورم همینه میخوام بدونم مدل های اکشن رو هم میشه تفسیر کرد
-> نمیدونم ولی منطقم میگه که وابسته به تسک نباید باشه شاید بسته به تسک پیچیدگیها فرق کنه"
"-> دوستان علت اینکه تو کولب بعضی وقتا saving failed میزنه چیه
-> سلام بعضی وقتا بخاطر ضعیف بودن اینترنت یا روشن بودن فیلتر شکن هست چندبار کنترلs رو بزنین اوکی میشه
-> آها من معمولا DNS عوض میکنم میرم chatgpt برای اونه احتمالا ممنون"
"-> سلام وقت بخیر برای داشتن یک شبکه که بشه گفت خوب اموزش دیده تقریبا به صورت تجربی باید با چند تا دادده اموزش ببینه برای مثال هزار تا به بالا و میدونم عوامل زیادی موثر است است در پاسخ گویی میخواستم یه دید داشته باشم که برای ترین شبکم چند تا داده باید دردسترس داشته باشم
-> سلام بستگی به سایز مدل پیچیدگی مساله و داره نمیشه عددی گفت در هر لحظه از انجام یک پروژه باید خودتون رو سوال بارون کنید مثلا در این موقعیت سوالهای من از شما مساله چی هست دسته بندی mnist هست یا تخمین سن افراد یا چرا باید دیتاست جمع آوری کنید یعنی دیتاست آماده نیست اصلا یا دیتاست مشابهی وجود نداره که بتونید اون رو بیارید در کار خودتون مناسب سازی کنید مدل چی هست چند میلیون پارامتر داره اگه نمیدونید روشهای پایه شما چند میلیون پارامتر دارن هدف چی هست کار دانشگاهی هست اگه کار دانشگاهی هست وقتی خودتون میخواید دیتاست جمع کنید اونوقت چطوری کارتون رو میخواید با دیگران مقایسه کنید و این سوالتون رو در گروههای تلگرامی نپرسید باید درباره مسالهتون به افرادی مطمئن با جزئیات توضیح بدید و بعد نظرشون رو جویا بشید حالا اگه هم نمیتونید کسی رو پیدا کنید مشکلی نیست سوالتون رو با جزئیات بپرسید چون الان من ناشناس به شما بگم 100 تا ممکن هست شما همین رو مبنا قرار بدید و برید جلو و بعد ببینید جواب نمیده
-> سلام پروژه من دانشگاهی است مسالم ساخت تصویر هست از داده های سینوگرام مسئله به شکل yAx که ورودی داده های سینوگرام هستند و داده خروجی ام پیکسل های تصویر بازسازی شده هستند من تازه میخوام شروع کنم به طراحی مدل و شبیه سازی من هنوز مدل را طراحی نکردم فقط صرفا چند دیتاست پیدا کردم میخواستم دانلودش کنم به همین سبب این سوال برام پیش امد چون حجم دیتاست های تصویر بالاست نمیدونستم چه تعداد سمپل کافیه برای شروع و حالا در ادامه برای تعمیم پذیری دیتاست اماده هست و حتی شاید بشود چند تا دیتاست را ترکیب کرد در کار سایر دوستان دیدم که برای تعیمیم پذیر شدن مدلشون از چند دسته دیتاست مختلف استفاده میکنند ولی من با توجه به محدودیت سیستم و زمان ای که پیش بینی میکنم باهاش مواجه شم میخواستم بدونم مثلا حداقل با چند هزار داده میتوان انتظار داشت شبکه تعمیم پذیری داشته باشد به صورت نسبی نه دقیق برای ابتدای کار کار دانشگاهی است به نکته خوبی اشاره کردید برای مقایسه با سایر روش ها باید دیتاست مشترک باشه ولی خب خیلی از مقالات دیتاستشون در دسترس نیست در این صورت کاری نمیشه کرد"
"-> سلام استاد وقتتون بخیر دوره دیپ کاتالیست قسمت language modeling میشه بفرمایید از چه مدلی استفاده کردید چه تسکی رو انجام دادید و چه دیتاستی رو استفاده کردید
-> فک کنم تسکش text prediction بود ولی دو مورد بعدیش رو اگه بفرماید ممنون میشم
-> سلام شبکه LSTM تولید متن دیتاست Wikitext2 بهبود عملکرد شبکه مبتنی بر ایدههای مقاله AWDLSTM
-> ممنون استاد
-> استاد این شبکه برای دیتاست فارسی هم جواب میده یعنی میتونیم ازش برای تولید متن فارسی هم استفاده کنیم
-> فکر کنم بچهها سمت فارسی هم رفته بودن من هم دوست داشتم سراغ متن فارسی هم بریم منتها حجم آموزش زیاد شده بود و زمان نداشتیم به نظرم جواب میده فکر کنم فقط باید بخش کاستوم دیتاست و توکنایز کردن رو تغییر بدید اصل شبکه تکنیکهای آموزش و غیره ثابت میمونه
-> استاد تولید متن با gpt2 بهتر نبود به نظرتون دلیل خاصی داشت تو این پروژه رفتید سراغ LM ها
-> چرا GPT2 بهتر بود
-> چون کار مدل کردن زبان رو انجام میده و به خاطر مکانیزم توجه جواب بهتری نسبت به lstm به ما میده
-> خب مدل کردن زبان رو قبل ترنسفورمر با شبکه بازگشتی انجام میدادن حتی ما مدل کردن زبان رو با کانولوشنی هم میتونیم انجام بدیم پس فقط gpt2 نیست که مدل کردن زبان رو انجام میده در مورد جواب بهتر هم توی یک دوره آموزشی فاکتورهای مختلفی مطرح هست صرفا این کافی نیست که چون با gpt2 میشه به جواب بهتری رسید پس این رو آموزش بدیم ما چند هدف مهم داشتیم مبحث مدلسازی زبان رو که در دوره یادگیری عمیق گفتیم اینجا به صورت عملی پیاده سازی کنیم مبحث LSTM رو در یک پروژه سطح متوسط به بالا دوستان ببینن فرآیند بهبود یک کار پایهای رو طبق یک مقاله و گام به گام به مخاطب یاد بدیم وقتی از بیرون به قضیه نگاه کنید ممکن هست بگید gpt2 بهتر بود وقتی به محتوا نگاه کنید احتمالا میگید که خیلی چیزا یاد گرفتم و خودم میتونم به سمت gpt2 هم گام بردارم یکی از توصیههای همیشگی من در دوره دیپ کاتالیست این هست که وقتی وارد یک موضوع جدیدی میشید از یک کار پایهای شروع کنید و کم کم به سمت کارهای بزرگتر و جدیدتر حرکت کنید
-> خیلی ممنون از جواب کاملتون استاد
-> درود بر جناب اشرفی و سایر دوستان شبکه ال اس تی ام و در کل شبکههای بازگشتی یکی از مشکلاتشون در مدل کردن زبان Long distance dependency هست یعنی چی مثلا اگر قرار باشه متنی رو بدیم که ماشبن ما قرار باشه شخص و شمار فعل رو با توجه به فاعل جمله بتونه تشخیص بده شبکههای بازگشتی نمیتوننن خیلی خوب روی این موضوع کار کنن وقتی که متن طولانی میشه به این دلیل هست که این نوع شبکهها به صورت sequntial متن رو پردازش میکنن اما چرا ترنسفرمر نتیجه بهتری در مدل کردن زبان داره چون شبکه ترنسفرمر به صورت موازی متون رو پردازش میکنه و یک مکانسیم هم به اسم مکانسیم اتنشن اضافه شده در واقع کل شبکه ترنسفرمر مبتنی بر مکانیزم اتنشن هست در واقع به صورت ساده اگه بخوایم بگیم اتنشن کارش اینه که بافت کلمات رو کاملا از متون استخراج میکنه و میفهمه که کلمات در جملات چه معنایی دارن ترنسفرمر هم متن رو موازی پردازش میکنه هم معنا را در نظر میگیره
-> استاد امکان داره دیتاست فارسیی که بشه روی مدل تست کرد رو معرفی کنید یا اینکه از دوستان اگه کسی از دیتاست فارسی استفاده کردهاند آیدشون رو بهم بدید
-> سلام من دیتاست همشهری رو از xml خارج کردم و متن ها رو جدا کردماگر دوست دارید متن هاش رو مرتب کنید و توکنایز کنید و استفاده کنید ممکنه بکارتون بیاد
-> سلام وقتتون بخیر روی همون مدل استاد امتحان کردید
-> نه من صرفا یک دیتاست فارسی رو مرتب کردم که خدمتتون share کردم
-> ممنون از راهنماییتون
-> به نظرتون استفاده از این دیتاست فارسی برای text generation تو مدل LM مناسبه استاد
-> این دیتا دیتای پرسش و پاسخه من یه مدل باهاش درست کردم که سوال میپرسی جواب میده لینکش رو براتون میفرستم دیتای oscar میتونه دیتای خوبی باشه دیتایی هست بچههای ترگمان ریلیز کردن اونم دیتای بدی نیست حجمشم خوبه
-> خیلی لطف کردید"
"-> سلام وقتتون بخیر من یک سوال داشتم توی موضوع PCA اگر مقادیر explaned_variance و explained_variance_ratio زیاد باشند به چه معنی است و اگر کم باشند به چه معنی است اگر مقادیر اینها زیاد باشه یعنی بعد از انجام عملیات PCA توزیع داده ها خوب نشان داده شده یا بد نشان داده شده
-> مقدار زیاد یک کامپوننت نشون دهنده این هست که اون نقش پررنگی در کاهش بعد ایفا میکنه مقدار کم هم برعکس حالت ratio هم صرفا گزارش به صورت نسبت هست
-> خیلی خیلی ممنون"
"-> سلام پروژه ای انجام میدادم که تو بخشیش دچار مشکل شدم تو قسمت اول باید یک مدل autoencoder CNN طراحی میکردم و تو قسمت دوم دیکودر رو حذف کنم و از انکودر به عنوان pretrained مدل استفاده کنم توی قسمت اول توی انکودر خط زیر رو دارم x indices2 selfmax2x که یک ماکس پولینگ هست توی بخش دیکودر خط زیر رو دارم x selfunpool1x indices2 توی قسمت دوم وقتی که می خوام دیکودر رو حذف کنم از همچین دستوری استفاده میکنم modelunpool1 nnIdentity ولی بعدش این ارور رو میده که قبلا unpoolo دوتا ورودی میگرفته ولی identity یه ورودی میگیره در واقع این روش فقط بخش init تابع رو عوض میکنه و تو بخش forward به مشکل بر میخوره راهکاری که به ذهنم میرسه اما نمی پسندمش اینه که یه کلاس جدا برای انکودر تعریف کنم و وزن های مدل قبلی رو بهش انتقال بدم می خواستم بپرسم راه دیگه ای هست
-> شاید بهتر باشه که یک کلاس برای انکدر و یک کلاس برای دیکدر بنویسید و اینها رو توی یک کلاس دیگه بنام اتوانکودر به هم وصل کنید اینجوری راحتتر میتونید وزنها رو لود کنید
-> ممنون از modelchildren هم استفاده کردم ارور داشت کلاس انکدر و دیکدر رو جدا کردم درست شد"
"-> سلام روزتون بخیر کسی اطلاع داره که زمانی که بیش از حد از GPU کولب استفاده کردیم و دیگ بهمون GPU نمیده چکار میشه کرد
-> بچهها معمولا از چند تا ایمیل استفاده میکنن از سرویسهای دیگه هم میشه استفاده کرد مثل کگل چند مورد هم قبلا بچهها معرفی کرده بودن سرویس پولی کولب هم خوبه ولی ممکنه زود تموم بشه ترجیحا تا مرحله آموزش مدل از جی پی یو استفاده نکنید برای آموزش مدل هم احتمالا مجبور بشید یک بودجهای بذارید کنار
-> ممنونم ی سوال دیگه اگه از ی ایمیل استفاده کنیم برا کولب ولی برای درایو از ایمیل دیگه ایی استفاده کرد یا همزمان ی ایمیل باید باشه
-> میتونید کولب رو به هر گوگل درایوی که میخوایید وصل کنید
-> سلام بعد استاد اینجوری کافیه مدل مون رو save کنیم با اکانت دوم مون دوباره load کنیم به اون یکی سلول هایی که توی اکانت اولی مثل پیش پردازش و لوپ train کردن و optimizer دسترسی نداریم دیگه دوباره باید کپی کنیم یا راه بهتری هم هست غیر کپی کردن
-> سلام نوتبوک رو باید توی ایمیل جدید کپی کنید یکم کار دستی داره البته با یک ترفندی میشه کدها رو توی گیتهاب گذاشت و سریعتر توی ایمیل جدید کد رو راه انداخت ولی فکر کنم برای خیلیها مشکل باشه
-> خیلی ممنون"
"-> دوستان سلام و عرض ادب من طبق قولی که داده بودم خیلی دوست داشتم فردا در حد سه ساعت با هم بتونیم یه جلسه داشته باشیم اما متاسفانه فردا سفری برام پیش اومده انشالله اوایل هفته آینده میتونیم جلسمون رو برگزار کنیم
-> سلام و احترام امکانش هست پیامی که توش فرمدید چه مباحثی رو ارائه میکنید رو یه ریپلای بفرمایید
-> سلام شبتون بخیر هفته آینده چه روزی برگزار میشه
-> سلام تلاشم اینه یک شنبه برگزار کنم تا فردا عصر خدمتتون میگم دقیقش رو
-> سلام وقت بخیر این جلسه برگزار نمی شود
-> متاسفانه لپتاپم مشکل پیدا کرد"
"-> به نظرم اینکه لایسنس نمیخواد صرفا باید توی سایتش ثبت نام کنید دیگه شاید ادیتور دیتاست رو نمیشناسه و میخواد محکم کاری کنه سایت سیتی اسکیپ رو بخونید و بعد یک توضیحی براش بفرستید البته به نظرم با افراد دیگه ای هم مشورت کنید اینجوری توی گروه های عمومی مطرح نکنید شاید بهتر باشه ممکنه پیشنهادهای اشتباه دریافت کنید Privacy This dataset is for noncommercial use only However if you find yourself or your personal belongings in the data please contact us and we will immediately remove the respective images from our servers
-> ممنون"
"-> سلام دوستان نظرتون راجع به این پستی که تو لینکدین گذاشتن چیه
-> اینا که باهم همکار بودن
-> دقیقا منم از همین تعجب کردم حتی یه کلاسم با هم برگزار کردن
-> احتمالا جنبه سیاسی داره
-> جدا از درست و غلط بودن این گزاره چقد بی چشم و رو هست این دوستمون فرضا حتی اگه دکتر زارچی مشکلی هم داشته باشن اینجوری و به این لحن نباید مطرح کنه ایشون اونم تو لینکداین و به شکل عمومی جدا از این دکتر شریفی زارچی مدال طلا در المپیاد جهانی کامپیوتر انفورماتیک دارن کسایی که با پرستیژ این آزمون در کل المپیادها آشنا هستن براشون محرزه که کسی که تو این المپیاد حتی فقط به مدال کشوری هم برسه قطعا از ضریب هوشیای برخوردار هست که نیاز به اینکه کسی مثل ایشون براش بخواد چیزی رو توضیح بده برای ارائه نداره دکتر زارچی زحمات زیادی برای علوم کامپیوتر تو ایران برداشتن واقعا چنین چیزی جواب زحمات ایشون نیست دکتر زارچی به غیر از نظر تئوری و علمی از نظر اخلاقی هم بسیار انسان ارزشمندی هستن به نظر میاد این دوستون تو پیک بالا اثر دانینگ کروگر هستن"
"-> سلام دوستان به مناسبت 8 سالگی آکادمی هوسم تخفیف 45 درصدی گذاشتیم این تخفیف جمعه فردا تموم میشه کد تخفیف HBD8TH هست و از طریق سایت هوسم میتونید در دورهها با تخفیف ثبتنام کنید
-> سلام ببخشید من مهاجرت کردم و لپتابم را کلا عوض کردم و اپلیکیشن قدیمی را ندارم که دسترسی داشته باشم به کانتت درس میتونید راهنمایی کنید که چطوری میتونم اپلیکیشن را نصب و فعال کنم
-> لطفا به پشتیبانی بگید
-> از کجا میتونم بهشون دسترسی داشته باشم
-> "
"-> سلام استاد این بار شما راهنماییم کردید واقعا واسم جالب بود هایپرپارامتر رو تنظیم کردم خیلی تاثیر داشت بعد دوتا مدل همدیگر رو جمع کردم و تقسیم بر دو میکردم و تونستم نتیجه خوب بیارم به نسبت قبلی
-> برای آنسامبل بهتر هست حداقل سه تا باشن 3 5 10"
"-> سلام دوستان یه نرم افزار برای رسم نمودار درختی معرفی می کنید من دارم پایان نامم رو تبدیل به کتاب میکنم ولی توی ترسیم نمودارها خیلی گیر کردم
-> Visio edraw max
-> سپاسگزارم"
"-> بچهها جلسهای که بهتون قول داده بودم دوشنبه هفته آینده خوبه از ساعت ۶ تا ۹ شب
-> سلام خیلی ممنون از اینکه وقت میزارید جلسه تو چه پلتفرمی برگزار میشه
-> انشالله گوگل میت"
"-> سلاموقت به خیر استاد اشرفی و بقیه بچه هایی که سر کار میرن لطفا اگه امکانش هست جواب بدن با دورههای پایتون و دیپ لرنینگ و بینایی و دیپ کانالیست و پردازش تصویر میشه کار پیدا کرداگه نه چه چیزهای دیگه ای لازمه اگه تجربه ای دارین لطفا کمک کنین
-> بنظرم بله دوره کامپیوتر ویژن رو من دیدم امااا منظور از کار کردن کجاست قطعا اول باید از شرکت های کوچک شروع کنید یا به عنوان کارآموز در شرکت های بزرگ استخدام شوید اینکه فکر کنید این دوره هایی که گفتید رو پشت سر بذارم و تبدیل به سنیور شوم اشتباه است
-> به نظرم شركت كوچك پيدا نميشه
-> استارتاپ ها و دانش بنیان های نوپا هستند
-> شما اين چيزي كه ميگيد از چه طريقى و همچنين وبسايتى ميشه پيدا كرد
-> وضعیت شرکت ها رو داخل دانش بنیان و محصولاتی که تولید کردند
-> شما جایی کار میکنید
-> بله
-> دوستان من یک استارتاپ دارم و یک کارفرما محسوب میشم طبق تجربه میگم خدمتتون جناب دکتر اشرفی هم نظرشون رو بگن دوره دیدن خیلی خوب و عالیه باعث میشه که علم آدم زیاد بشه اما صرف اینکه دوره گذرونده بشه نمیشه کار پیدا کرد نیاز به تجربه هم داریم علاوه بر تجربه مهارتهای نرم مثل مهارت کار گروهی ارتباط برقرار کردن با افراد تیم و مهارتهای این چنینی هم برای کار کردن نیازه تجربه هم با کار کردن با انواع دیتاها و کارکردن با مدلهای مختلف به دست میاد
-> یکی از دوستام تو دانشگاه علم و صنعت رفته بود یه جایی کارآموزی به جای کار یادگرفتن میفرستادنش قبض آب و برق رو واریز کنه اکثر شرکتایی هم که نیرو میخوان سابقه کار میخوان
-> اصلا کار مثبتی نمیکنه یعنی کلا در حد کار خدماتی هست
-> بیشر بیگاری میکشیدن بعد چند ماه از شرکت اومد بیرون رفت مرکز رشد دانشگاه اونجام چیز خاصی یاد نگرفت اینا چیزاییه که خودش میگه
-> سلام فقط درس و دوره کافی نیست دوره ارشد رو درنظر بگیرید یک دوره دو ساله آموزشی پژوهشی هست توی فاز آموزش معمولا 8 درس معادل 24 واحد میگذرونید توی پژوهش هم پروپوزال سمینار و پایاننامه دارید که معادل 8 واحد هست درسته که فاز آموزش بیشتر هست اما نکته اینجاست که در خدمت فاز پژوهش هست یعنی باید درسهایی بگذرونید که شما رو برای انجام پایاننامه آماده کنه معمولا دوره آموزش حدود 1 الی 15 سال و دوره پژوهش حدود 6 ماه تا 1 سال طول میکشه حالا کسی که دانشگاه نرفته و خودش میخواد وارد این حوزه بشه و کار پیدا کنه چیکار باید بکنه به نظرم باید اون دوره دو ساله ارشد رو شبیهسازی کنه الان این دورههایی که نوشتید احتمالا حدود 1 الی 15 سال طول میکشه که پشت سر بذارید بعدش باید به هر شکلی که شده فرایند انجام پایاننامه رو شبیهسازی کنید مثلا چند تا پروژه دیپ کاتالیست برای شروع خوبه شروع بعدش فراتر برید برید سراغ کگل یا سایر جاهایی که دیتاهای خوب و چالشی میدن کارهاتون رو به شکل استانداردی در گیتهاب بذارید پروژههایی انجام بدید که هرجایی نباشه دنبال rag و LLM و اینجور کارها ترجیحا نرید اینها خوبن ولی به نظرم آوردهش زیاد نیست مثلا الان یک مسابقه توی کگل هست که مربوط به ملانوما هست این نوع پروژهها خوبن براش وقت بذارید و حتی ایدههای توی مقالهها رو روی اون تست کنید حتی اگه به جای خوبی رسید مقالش کنید مثلا همین کنفرانسهای معتبر داخلی مثل انجمن کامپیوتر و معمولا مسابقههای کگل ددلاین 1 الی 3 ماهه دارن پس در نظر داشته باشید که با زمانی حدود 2 ماه میشه یک پروژه خوب و باکیفیت انجام داد حالا اگه سه تا پروژه جوندار توی رزومهتون داشته باشید چی میشه بعدش برید سراغ کار برای کار انتظار در سطح گوگل نداشته باشید اگه سطحتون خوب هست برید سراغ کار اگه نه که کارآموزی کار هم پیدا میشه محیط کار کارهای بیخود هم داره پس برید جایی که در عین داشتن کارهای بی ارزش کار با ارزش هم داشته باشه مثلا دیگه 50 درصد از زمان شما صرف کار باارزش بشه اونی که قبض پرداخت میکنه آیا کار باارزش هم داره اگه داره بمونه پیشرفت میکنه ابتدای کار خیلی به درآمد فکر نکنید اولویتتون ورود به کار باشه بعضی وقتها شما باید برید به شرکت پیشنهاد بدید که بذار من بیام کارآموزت بشم پول هم نمیخوام مثلا آقای رحمانی توی این گروه حضور دارن و صاحب کسب و کار هستن ممکن هست بازهم افرادی مشابه ایشون بتونید پیدا کنید باید خودتون بگید که من بیام شرکت شما کار کنم
-> ببینید من هم اینها رو درک میکنم ولی به نظرم افراد جویای کار هم توقعاتشون بالا رفته این گروههای تلگرامی هم خیلی تاثیر منفی میذاره مدام دید منفی ناامیدی و توقع بالا رو پمپاژ میکنه
-> کجا دنبال گشتی مهدی جان چقدر براش وقت گذاشتی چند تا نمونه پیدا کردی
-> من در وبسايت جابينجا گشتم و همينطور لينكدين پست گذاشتم نزديك يك ماه وقت گذاشتم چندين شركت رزومه رو براشون فرستادم و همچنان موفقى نداشتم به دليل راه دورى رزومه ام رو رد كردن
-> خب بحث دوری هم مهمه اغلب کارها توی تهرانه نظر من این هست که برای شروع کار هم کار حضوری بهتر از دورکاری هست
-> استاد نظرتون درباره دورکاری برای کشور دیگه چیه
-> 
-> کلا که دورکاری بد نیست ولی صحبتم اینه که فرد تازهکار بهتر هست وارد جامعه بشه و یکسری چیزها رو تجربه کنه مثلا تعامل و کار کردن با همکاران رو یاد بگیره کانکشن بسازه محیطهای کاری بهتر رو پیدا کنه و خودش رو برای اونجا آماده کنه وقتی دورکار باشید اینها رو ندارید و فقط تو خونه نشستید یکی دو موردی دیدم که با خارج کشور کار میکردن ناراضی نبودن
-> ممنونم استاد
-> تریدآف رو همیشه درنظر داشته باشید دورکاری میرید یکسری مزایا مثل راحتی در هنگام کار هزینه و غیره رو دارید ولی درعین حال هزینهای که میدید که میشه اون مواردی که بالا گفتم حالا باید ببینید شرایط زندگی و آیندهتون چطوری هست
-> استاد از تجربه ای کاریتونم میگین
-> بد نیست توی این ایونتها هم باشید مثلا الکامپ پر شرکتهای کوچیک و بزرگ هست شاید بتونید اونجا موردهایی رو پیدا کنید و بهشون درخواست کار بدید
-> من تجربه کار حضوری داشتم ریموت کار نکردم جز دوره کرونا اولین باری که دنبال کار بودم دو مورد داشتم یکی میگفت ما روی پلاک کار میکنیم ما ماهی 1 تومن اینا بهت میدیم قرارداد هم نمیبندیم چون میدونی که قرارداد برای خودت هم کلی مسئولیت داره ما اینجا همش دنبال خلاقیتیم مثلا به این فکر میکنیم که چطوری این آویز لباس رو بهتر کنیم خب من دوست نداشتم این صحبتها رو ولی کار میخواستم به دوستم گفتم که اینطوریه گفتش که بیا به مدیر من هم بگو ببین چی میگه رفتم بهش گفتم و اکی شد اینجا حقوقش بهتر بود و خب من هم سرمو انداختم پایین کار کردم
-> رزومه من به خاطر دانشگاه محل تحصیلم وزن زیادی داشت از طرفی قبل کار هم پروژه فریلنس زیاد انجام داده بودم و مهارتم در کدنویسی و مباحث علمی بالا بود
-> حالا حرفم به دوستان جویای کار این هست که وزن رزومهتون رو بالا ببرید رزومههاتون رو واقعا بررسی میکنن در یک نگاه به رزومه میشه فهمید که فرد کاربلده یا نه
-> سلام استاد میشه توضیح بدین چه چیزایی باعث میشه وزن رزومه رو بالا ببره مثلا ما که تجربه کاری نداریم ولی پروژه تمرینی زیاد انجام دادیم و بیشتر شرکتا تجربه کار براشون مهمه یا شایدم من اشتباه میکنم ممنون میشم در این مورد بیشتر راهنمایی کنید
-> توی اون پیام طولانی گفتم پروژه تمرینی چی انجام دادید الان چی برای عرضه دارید"
"-> سلام اقای دکتر وقت تون بخیر تو اموزش faster rcnn جلسه چهارم یه جایی فرمودین که خودتون قبلا کدی نوشتین که به شکل ان سوپر وایز گونه ای ناحیه دست انسان رو دیتکت می کنه می خواستم خواهش کنم در مورد روش تون توضیح بدین اگر مقاله شده لطفا لینک بدین تا مطالعه کنم چجوری ان سوپر وایز میشه این کار رو انجام داد من به دنبال این هستم که بدون نیاز به دیتاست مناسب ابجکت دیتکشن فقط با دیتاست های کلاسیفیکیشن دیتکشن رو انجام بدم ممنون میشم راهنمایی بفرمایید
-> سلام توی این مقاله گفتم به نظرم روشهای جدید رو هم مدنظر داشته باشید مثلا شاید شبکه فلورنس۲ به کارتون بیاد
-> ممنونم اقای دکتر لطف کردین action recognition نشنیده بودم قبلا برام جالب بود florence2 هم به نظر خیلی فوق العاده میاد ولی نمی دونم چرا ترس برم داشت جرات نمی کنم برم سمتش
-> نترسید"
"-> با مدل xgboost classifier ترین کردم درصد دقت 87 بود پیشنهادی دیگه دارید از این بهتر باشه
-> اول اینکه خیلی خوشحال شدم که این کار رو انجام بدید امیدوارم تداوم داشته باشید به نظرم حتما روی تنظیم هایپرپارامترها کار کنید کگلیها از optuna استفاده میکنن بعدش نوتبوک سایر افراد این مسابقه رو نگاه کنید نکات خوبش رو به کار خودتون اضافه کنید دیگه وقتی به مرحلهای رسیدید که ایده جدیدی نداشتید پروژه رو ببندید و ادامه ندید بعضیا توی کگل میان چند آنسامبل مدل رو برمیدارن آنسامبل میکنن که دیگه این کارها بار آموزشی نداره و اتلاف وقته مهم نیست که به رتبههای برسید حتما یک چک لیست داشته باشید و توی اون کارهایی که دیگران کردن یا ایدههایی که خودتون دارید رو بنویسید و از ساده به سخت اولویت بندی کنید و یکی یکی انجامشون بدید همینا دیگه تجربیاتتم اینجا بگی خوبه یک نوتبوک خوب و تر و تمیز و عمومی هم از خودت بذار که بقیه upvote کنن
-> استاد دستت درد نكنه راهنماييم كرديد چشم حتما اين بار هايپرپارامتر رو تست ميكنم و كار ميكنم باهاش سعى ميكنم تجربيات من رو از همينجا بگم و همچنين نوتبوك بزارم با تشكر"
"-> الان این مدل رو تست کردیم موقع تست چندین بار باید بچرخونیم تا درست دیتکت شه گفتیم بیام قطعات رو خودم قبلا با رنگ سفید همش رو چاپ ۳ بعدی کردم و دونه دونه عکس گرفتم با روبوفلو لیبل زدم و با یولو ترین کردم رنگی این بار چاپ کنم تا رنگ ها کمک کنه به تشخیص بهتر خواستم به جا اینکه دوباره عکس بگیرم و لیبل کنم رنگ دلخواه بدم بهشون و دوباره ترین کنم رنگی بدم که قراره بعدا با اون رنگ ها باز چاپ سه بعدی کنم انقدر شبیه همه موقع اسنبل کردن نمیتونه بفهمه استپ ۴ یا ۵ مثلا شایپ قطعات رنگی کمک کنه به تشخیص
-> 
-> میخوایید هر نوع قطعه یک رنگ خاص بزنید مثلا قطعه 4 همیشه آبی و اگه اینطوری هست که احتمالا خوبه
-> بله"
"-> تیکه های پازل که هرچی جلوتر میره روی هم سوار میشه باید مدل نشون بده فرق step با اون تکی ها اینه که استپ اسنبل شدس
-> حالا میخوایید این سفیدها با پردازش تصویر رنگآمیزی کنید رنگآمیزی کنید که دیتکشن بهتر بشه الان مشکل دیتکشن با رنگ سفید چیه شاید من بد متوجه شدم ولی با چیزایی که برداشت کردم به نظرم رنگآمیزی کار بیهودهای هست"
"-> سلام دوستان ی راهنمایی لازم داشتم من یک دیتا ست دارم همش سفیده تیکه های پازل میخوام رنگ بهش اپلای کنم که دیتکشنش اسون شه چطوری میتونم این کارو کنم ممنون میشم راهنمایی کنید
-> شاید بهتر باشه چند تا تصویر بفرستید
-> 
-> خیلی ممنون از پاسختون استاد عکس ها این مدلیه"
"-> سلام دوستان امیدوارم خوب باشید به مناسبت هشت سالگی هوسم من میتونم در عرض حدود سه ساعت کلیت ان ال پی رو بهتون بگم و حتی بهتون بگم که یک مدل زبانی برت از اول چجوری ساخته میشه و واقعا بتونیم یک مدل زبانی کوچیک رو با هم بسازیم
-> ممنون خیلی خوب میشه اگه این کلاس رو برگزار کنید و من حتما شرکت میکنم
-> سلام عالی میشه و لطف می کنید
-> سلام اگه اوکی هستید باعث افتخار بنده اس طی جلسهای این مباحث او از شما یاد بگیرم
-> سلام شبتون بخیر میشه بدونم به صورت دقیق تر به چه مواردی قراره اشاره بشه
-> خبر خوبیه چیزی که رسانه فارسی بهش نیاز داره همیناس چند وقت پیش دیدم یه استادی شروع کرده به تدریس به زبان انگلیسی برای انگلیسی زبانا کارش قابل احترامهاما تا وقتی که این همه منابع خوب و غنی انگلیسی هست آنچنان توفیقی نمیکنه براشون وقتی جامعه فارسی خودش کمبود مطالب بروز داره بهتره که برا خودمون دل بسوزنیم
-> سلام خیلی ممنون لطف میکنید
-> بیشتر فوکس رو روی ترنسفرمر خواهیم گذاشت به صورت ساده میگیم چه میکنه مکانیزم اتنشن رو بررسی میکنیم در مورد ورد امبدینگها صحبت خواهیم کرد در نهایت مدلهای برت و جی پی تی رو بررسی میکنیم در آخر هم یه مدل برت رو پری ترین میکنیم یا میتونیم یه مدل برت رو فاین تیون کنیم یا مثلا یه مدل تکست تو تکست رو فاین تیون کنیم هر چی شما بپسندید
-> چه خوب میتونیم خود مقاله attention is all you need هم پیاده سازی کنیم به نظرم به درک عمیق تر از خود ترنسفورمر کمک میکنه
-> کجا قراره برگذار بشه اطلاع رسانیش به چه صورته ثبت نامش
-> پیاده سازی شده کاملا نیازی نیست این کار رو از ابتدا انجام بدیم
-> میگمخدمتتون
-> درسته ممنون
-> سلام خیلی عالیه ببخشید این کی برگزار میشه لینک ثبت نام هست بفرستید ممنون
-> محمدجان پیامتون برام جالب بود مثلا خلا چه آموزشهایی رو حس میکنید
-> بنده هم باهاشون موافق هستم و جسارتا نظر خودم رو هم به اشتراک می ذارم به نظرم آموزش خوب و درست و حسابی به زبان فارسی در حوزه nlp کمه و به شدت لازمه خصوصا با توجه به ترند شدنشون و کاربردهایی که دارن مثلا اخیرا بنده نیاز به انجام یک پروژه چت بات پیدا کردم که به نظرم منابع آموزشی فارسی و کافی نبود خصوصا برای استفاده و بهرهگیری از Llmها و قابلیتهای دیگه که مثلا در RAG باهاش رو به رو هستیم به نظرم یکم آموزش کتابخانه هایی مثل langchain و هاگینگ فیس و کار با llmها از این طریق و ترکیب روش ها و مدل برای کاربردهای nlp خیلی جذاب و مورد نیازه
-> خب نیازا که متفاوت هست اما خودم نبود دوره خوب برای nlp و پردازش گفتار و سنتز گفتار البته که تو این دوتا مورد جهانیه کمبودش
-> بچهها سلام من اولین دوره رو دارم برگزار میکنم تا الان هم خروجی خوبی گرفتیم کمکم بچهها دارن با انواع مدلها آشنا میشن و اینکه دارن مدل ها رو هم ترین میکنن و روی هاگینگ فیس هم میذارن
-> 
-> سلام وقت بخیر این جلسه مشخص نیست کی ممکنه برگزاربشه مواردی در خصوص امبدینگ کلمات و همینطور مدل برت هست که با وجود مطالعاتی که داشتم هنوز نتونستم کامل درکشون کنم اگر رفرنس خاصی مدنظر دارید که بتونه در درک بهتر مفاهیم کمک کنه ممنون میشم راهنمایی بفرمایید سایر دوستان هم اگر پیشنهادی داشته باشن ممنون میشم
-> من تازه کلاسهام تمام شده و یکم کارای شرکت سبک شده حتما بهتون اطلاع میدم رفرنس خیلی زیاده اولیش یوتیوبه
-> خیلی ممنون فرمایشتون درسته انچه که یاد گرفتم هم از اموزشای یوتیوب و توضیحات گیت هاب و هاگینگ فیس و اینا بوده اما سوالاتی برام پیش اومده ک نتونستم جوابشونو پیداکنم و فک میکنم شاید دلیلش اینه که مواردی رو که شاید اونقدرام سخت نیستن خوب درک نکردم
-> شما بفزمایید سوالاتتون رو
-> در متن ما به ازای هرجمله ورودی یک بردار embeding داریم متناسب با اندازه ای ک مشخص کردیم مثلا ۵۰ حالا اگر متنی داشتی باشیم با ۴۰۰۰۰تا جمله و max_len رو ۱۰۰ تعریف کرده باشیم اینکه بگیم ماتریس embedding چه ابعادی داره اصطلاح درستیه اگر اره چجوریه و اگر نه مشکل چیه
-> اینو خدمتتون توضیح میدم
-> ممنون میشم"
"-> با سلام و خسته نباشید من یه دیتاست شامل ۵۰۰۰ تصویر هست که ۵ تا کلاس دارد ۳۵۰۰ تا برای train و ۱۰۰۰ برای valid و ۵۰۰ برای test گذاشتم مدل که با داده های train و valid آموزشش پایان یافت بعد بهترین مدل را سیو کردم و سپس لود کردم بعد در این مورد زیر مدل لود شده را می خواهم با داده های تست ارزیابی کنم که مقادیر دقت و بدست آید من مشکل دارم ممنون میشم راهنماییم کنید
-> مشگل کجاست
-> نحوه دادن داده تست به مدل که پارامتر های مورد نظر بدست بیاد
-> پارامتر منظورتون چیه شاخص های ارزیابی مدل"
"-> به علاوه خیلی از کارهای تجاری هستن که لزوما نیاز به مدل اختصاصی ندارن و کارشون با همون مدلهای آماده راه میوفته فکر کنید مثلا یه شرکت خصوصی به زور با منابع محدود بخواد مدل زبانی خودش رو بالا بیاره اونم آیا به مدل خارجیش برسه و یا نرسه در صورتی که میتونه از مدل های آماده و حتی api های خارجی استفاده کنه و محصولی تولید کنه که مورد نیاز بازاره و بتونه توی بازار سودی رو بدست بیاره هر چند به دلایلی داشتن مدل های اختصاصی داخلی و با داده های اختصاصی خودمون خیلی می تونه مفید باشه و حتی در برخی کاربرد ها ضروریه
-> عزیزم ما هم اینا رو میدونیم کار کنن ما که رقیب یا حسود نیستیم ولی دیگه رونمایی و وزیر نمیخواد اینجوری که میشه فکر ما سمت اینترنت اشیا گوگل کولب بومی موتور جستوجوی ذرهبین و خیلی چیزهای دیگه میره ذرهبین تجربه متفاوت وبگردی
-> 
-> خیلی ممنون از نکاتی که فرمودید اسفند ماه پارسال یه همایش توی پژوهشگاه ارتباطات و فناوری اطلاعات برگزار شد و به برخی از نکاتی که فرمودید اشاره شد اساتید مطرح مثل استاد فیلی و مینایی و چند مورد از اساتید و افراد فعال در این زمینه هم ارائه مطلب دادن مدل های زبانی که در اون جلسه بررسی شد مدلهای فاین تیون شده مدلهای خارجی با دیتاهای داخلی بودن یک نکتهای که توی اون جلسه هم مطرح شد این بود که برتری ما می تونه داشتن داده های اختصاصی به زبان فارسی باشه که میتونیم باهاش مدلها رو فاین تیون کنیم که لزوما شرکت های خارجی اونا رو ندارن و یا اصلا براشون موضوعیت نداره و در همون جلسه دادگان ترگمان رونمایی شد در هر صورت ببخشید محضر اساتید پر حرفی کردم با تشکر از شما و استاد اشرفی عزیز
-> درود پرقدرت ادامه بدید"
"-> رونمایی از مدلهای زبانی فارسی همراه اول با حضور وزیر ارتباطات گروه MCINext همراه اول در نخستین روز برگزاری نمایشگاه بینالمللی الکامپ ۱۴۰۳ تعدادی از مدلهای زبانی توسعه یافته خود شامل سیلک با ۱۳ میلیارد پارامتر آهوران با ۸ میلیارد پارامتر و آوا با ۱۳ میلیارد پارامتر را با حضور عیسی زارعپور وزیر ارتباطات و فناوری اطلاعات در سالن اختصاصی همراه اول سالن ۳۵ رونمایی کرد مدلهای زبانی بزرگ LLM سیستمهای هوش مصنوعی هستند که با تحلیل و یادگیری از حجم زیادی از دادههای متنی قادر به تولید متن ترجمه پاسخ به سوالات و انجام وظایف مختلف زبانی هستند و در حال حاضر نیز با حمایت مرکز تحقیق و توسعه همراه اول توسعه یافتهاند ضمن آموزش این مدلها تلاش شده است تا به کمک روشهای همترازسازی RLHF و DPO از تولید محتوای آسیبزا جلوگیری شود گروه MCINext همراه اول تلاش دارد تا در آینده نزدیک امکان استفاده عمومی از این مدلها را نیز فراهم کند
-> برای ترین کردن چهار تا مدل آماده رونمایی و وزیر و وزرا نیازه محتوای آسیبزا
-> حداقل موقع رونمایی یه تستش میکردن ببین اصن کار میکنه یا نه همین فقط رونمایی کنی
-> وقتشه هوسم رو ارتقاع بدیم و مدل زبانی بدیم بیرون
-> مطمئنید مدل آماده بود معماریش جدید نبود البته من آشنایی با این مدل ها که رونمایی کردن ندارم صرفا برام سوال شد
-> موارد مشابه در سال های گذشته یک ZYNQ ZedBoar میخرید 90 میلیون ت میارید میزارید توی شیشه و رونمایی میکنید رونمایی از پردازنده کوانتومی جون
-> عنوان الگوریتم پردازش کوانتومی نوشته شده البته فکر کنم برد اصلی رو جا گذاشتن اینو وسط راه خریدن عنوانی که ذکر شده من که الکترونیک و طراحی برد کار کردم تصورم یک سیستم ماژولار هستش که شامل ورودی و خروجی متناسب هست این فقط یک برد آموزشیتحقیقاتی برای پروژه های در حال تست هستش برد شامل تراشه SoC سری zynq شرکت xilinx یکی مشابه اش رو تو خونه دارم وضع مون توپه ها کوانتومی شدیم
-> جالب اینکه این برای نیروی دریایی بود نکته ی جالبتر اینه ک برای همین حرکت از یک نفر تجلیل شد اما توی یک هفته هم بچه های الکترونیک هم هوش مصنوعی از خجالت اینا در اومدن هفته ی بعد اومدن و معذرت خواهی کردن
-> فکر کنممی خواستناز یکی تجلیل کنن دنبال برد میگشتن
-> سلام حقیقتش task این پیام شما توی ذهنم باز مونده بود ی جا بحثی شد یادش افتادم و این دفعه ک دقت کردم نوشته اولین الگوریتم پردازش کوانتومی و ی سرچ کردم دیدم مقالاتی در زمینه پیاده سازی الگوریتم های پردازنده های کوانتومی روی FPGA موجوده که چندتایی هم مربوط به کشور خودمون بود البته من در زمینه کوانتوم تخصصی ندارم
-> زیر متن فارسی نوشته QPSOLSTM شبکه LSTM که واضحه PSO هم Particle Swarm Optimization هست که مربوط به بهینهسازی براساس حرکت دستهجمعی پرندگان هست حالا Q داره که اون کوانتومه مقالات و کدش توی اینترنت هست اینا اومدن یه برد خریدن الگوریتم رو روی این برد اجرا کردن این ماجرا از ابتدا تا انتهاش زرد بود اصلا شکل ارائه طوری هست که نیازی نیست به مباحث مسلط باشید شکل و فضای ارائه زرده بهش میخوره پروژه کسر خدمت باشه"
"-> سلام دوستان منبعی وجود داره که درباره سورس کد های پایتورچ توضیح داده باشه سورس کد توابعی مث conv
-> هدفتون چی هست از سورس کدها میخواید به چی میخواید برسید
-> منابع زیادی برای چیزی که شما میخواین وجود نداره اما به نظرم به گیتهاب پایتورچ یه سر بزنین کمک کنندست
-> سوالشون جالب رفتم گیت هاب رو سرچ کردم چیزی که میخاستم رو پیدا نکردم اصلا بعد اومدم از جمینای سوال کردم پس کد های شبکه ها کو مگه نه اپن سورسه For example on GPUs conv1d likely leverages cuDNN a highly optimized library for deep learning primitives cuDNNs source code is closedsource and not part of PyTorch The CPU implementation might be part of PyTorchs internal C codebase not directly accessible for various reasons like maintainability or separation of concerns
-> آره دقیقا همینه تازه برای همون cpu هم احتمالا داره از کتابخونههای C اینتل مثل BLAS MKL و غیره استفاده میکنه دوستمون چند روز پیش که سوال کرده بود میخواستم بهش بگم که توی سورس کدهای لایهها و کامپیوننتها خیلی نمیشه پیش رفت چون به سرعت از پایتون میره به C و دیگه ادامه دادن بسیار مشکل میشه اگه واقعا نیاز به فهمیدن کدها باشه باید دانش و مهارت فرد در سطحی باشه که نیازی به آموزش نداشته باشه به همین خاطر پرسیدم که هدفشون چی هست
-> من فکر میکنم اگر هدف کسی از اینکار فهمیدن مکانیزم پشت همچین فریم ورکی هست بهتره که خودش سعیشو بکنه تا حدی از ماژول ها رو پیاده سازی بکنه به نظرم برای فهم عمیق تر از کد ها این منطقی تر میتونه باشه
-> آره جالبیش اینه بعد از میره به c
-> اصلا منظور از open source library built for python همین هستش
-> آره باید یک مینیمال کد نوشته بشه یا بررسی بشه ببین باور کن یک جایی به اسمبلی هم میرسه
-> بیشتر میخاستم بصورت بیسیک و بدون استفاده از کتابخونه ها با پیاده سازی انواع شبکه اشنا شم بیشتر برای دید پیدا کردن
-> به صورت کلی هدف خوبی هست ولی روشی که انتخاب کردید مناسب نیست بعد از اینکه آموزش تئوری و کدنویسی پایتورچ رو دیدید خودتون کد بزنید و درکنارش از چت باتها و بلاگها مثل مدیوم استفاده کنید چون من در گذشته دیدم که چنین بلاگهایی وجود داره ولی بهتر هست خودتون پیاده سازی کنید کدهای فریمورک اوپن سورسی مثل پایتورچ برای اهداف آموزشی نوشته نشده اونها باید طوری کدها رو آماده کنن که تمامی حالات ممکن رو پوشش بده و بدون خطا و باگ باشه به همین خاطر خیلی پیچیده میشه و خوندن کدها بسیار سخته
-> درسته ممنون
-> استاد کتاب dive into deep learning رو چند وقته دارم میخونم پیاده سازی های فرام اسکرچش خوبن و توضیحاتی که داره هم تقریبا کامله
-> آره اون کتاب هم پیاده سازی کوچیک داره و عالیه ممنون"
"-> یادگیری cv و nlpتا چه حد باید dlیاد گرفت ممنو م استاد
-> سلام تسلط بر مباحث یادگیری عمیق یک گام ضروری برای این حوزههاست بیسیک شبکه عصبی شبکه کانولوشنی شبکه ترنسفورمر حتی شبکه بازگشتی و پایتورچ حداقل چیزهایی هست که باید مسلط باشید
-> همین میخواستم ممنون و تشکر استاد"
"-> بصری سازی جالبی داره این مقاله درباره معماری transformer و LLM هاست
-> "
"-> چندتا مرحله داره شما باید دیتاهایی که دارید رو تبدیل کنید به وکتور بعد در یک وکتور دیتابیس ذخیره کنید و در نهایت جواب رو از مدل بگیرید از این وکتور دیتابیس هم بگیرید روش های مختلف رگ رو سرچ کنید اصلا کار سختی نیست اگر ایجنت هم بخواید بنویسید میتونید
-> متشکرم اجازه دارم در پی وی بهتون پیام بدم
-> سلام جناب وقتتون بخیر اگر امکانش هست و وقتشو دارید درباره این مراحلی که گفتید به صورت پرکتیکال تر توضیح بدید
-> بفرمایید
-> بله من هم کمی به راهنمایی بیشتر و معرفی منابع مفید در این زمینه نیاز دارم ممنون میشم اگر براتون مقدوره راهنمایی کنید
-> چشم تلاش میکنم
-> برای ساختن RAG آموزش های آقای mehdi allahyari در یوتیوب به زبان انگلیسی واقعا خوبن البته در آموزش ها از Langchain و ابزارهای آماده استفاده نمی کنن"
"-> سلام دوستان کسی تجربه ساخت چت بات با RAG و استفاده از Llama 3 فاین تیون شده به فارسی رو داره ممنون میشم اگر کسی از دوستان تجربه ای در پیاده سازی پروژه مشابه با این موضوع دارن باهام به اشتراک بگذارن
-> با مدل جدید پارت بسازید Dorna مدل بهتریه
-> ممنونم امکانش هست راهنمایی کنید که چطور ازین مدل برای ساخت چت بات مبتنی بر دیتاست سوال و جوابم بهره بگیرم
-> مشکلی که الان با این مدل دارم اینه که چون خیلی سنگینه و ۸ میلیارد پارامتر داره به صورت لوکال که نمی تونم استفاده اش کنم از طرفی api key رایگان هم نداره و باید از طریق inference endpoints خود هاگینگ فیس استفاده شه که اونم هزینه داره راه حل شما برای این مورد چیه
-> باید مدل رو کوانتایز کنید یکم سرچ بزنید راحته
-> از Lora and qlora برای کوانتایز استفاده کنید مستنداتش داخل هاگینگ فیس وجود داره
-> یه نفر این مدل رو کوانتایز کرده لینک پستش تو لینکدین"
"-> سلام دوستان امکان دسترسی به سورس کدهای مختلف pytorch هست من دنبال سورس کد تابع شبکه cnn هستم که ببینم پشت زمینه کد چیه
-> سلام بله پایتورچ اوپن سورس هستش
-> درود هر بخشی که خواستی انتخاب کن در نهایت کلاس مورد نظر رو انتخاب کن بعدش کنار هر بخشی نوشته source میره توی گیت
-> متشکر
-> ممنون"
"-> سلام دوستان احتمال زیاد برای انجام دادن پروژه هاتون یا موارد دیگه ای نیاز به سخت افزار قوی داشتین و نیاز داشتین که به صورت مجازی بتونین یه سیستم اجاره کنین توی سرویس های ایرانی محدودیت های متعددی هست مثل اینترنت ضعیف فیلترینگ و که کار رو سخت میکنه اگر خواستید از سرویس های خارجی استفاده کنین به نظرم یک سرویس خوب میتونه runpod باشه که همچین امکاناتی رو بهتون میده از جمله نکات خوبش استفاده کردنش به صورت لوکال هست که که مزیت خیلی خوبیه چون میتونین توی یه کد ادیتور مثل vs code بهش متصل بشین و ازش استفاده کنین بدون نیاز به سرور نوت بوکیالبته با سرور جوپیتر هم میتونین ازش استفاده کنین از طرفی تنوع GPU بیشتری نسبت به سرویسی مثل lightning ai داره و حتی H100 هم میتونین اجاره ساعتی کنین و موجودی کیف پولتون هم به صورت لحظه ای میتونین مشاهده کنین یه نکته جذابشم اینه که سرعت اینترنت خیلی بالایی داره به طور میانگین من سرعتش رو توی طول استفاده خودم بین ۳۰۰ تا ۴۰۰ مگابایت برثانیه دیدم و یه نکته دیگه هم اینه که میتونین موقع اجاره یه ماشین مجازی یه سری کتابخونه رو به صورت پیشفرض روشون نصب کنین یعنی به این شکل که مثلا نسخه های متفاوتی از پایتورچ رو اماده نصب دارن که میتونین انتخاب کنین کدومش روی ماشینی که دارین اجاره میکنین نصب باشه منتها برای نگه داشتن فایل ها باید یه storage داخل سایتشون بسازین که فایلاتون اونجا ذخیره بشن که خب اونا هم هزینه خیلی خیلی کمی دارن به صورت ساعتی و همچنین میتونین تو حساب مثلا google cloud ای که دارین دیتا رو اکسپورت کنین برای هاست کردن مدل ها هم عالیه و خودش مثل space های هاگینگ فیس یه تعداد template داره که بهتون کمک میکنه راحت تر مدل ها رو هاست کنین مثلا توی template هاشون اکثر نسخه های stable diffusion رو دارن که میتونین استفاده کنین امیدوارم این اطلاعات کمک کننده باشه
-> اگر runpod رو در گروه سرچ کنین اطلاعات بیشتری هم گیرتون میاد
-> خیلی ممنون خرید اکانت از داخل ایران به چه شکل امکان پذیر هس با سایت های واسط یا مثلا ارز دیجیتال امکان پرداخت هست
-> هم سایت های واسط هم ارز دیجیتال هردوش امکان پذیره
-> تشکر فراوان"
"-> سلام راستش منم دقیقا به همین مشکلات خوردم وقتی سعی داشتم ازش استفاده کنمتوی فروم های خودشونم گشتم دیدم جوابی ندادن در رابطه با مشکلی که برای تایید شماره دارن و بنظر میرسه که برای یه تعداد محدودی کاربر ساختن و تایید اکانت به درستی انجام شده و تونستن به lightning studio دسترسی پیدا کنن اگه میخواین از همچین سرویسی استفاده کنین به نظرم سرویس مشابه بهش runpod باشه که همچین امکاناتی رو بهتون میده و تو این پیام بهش اشاره کرده بودم از جمله استفاده کردنش به صورت لوکال که مزیت خیلی خوبیه از طرفی تنوع GPU بیشتری نسبت به lightning داره و حتی H100 هم میتونین اجاره ساعتی کنین و موجودی کیف پولتون هم به صورت لحظه ای میتونین مشاهده کنین یه نکته جذابشم اینه که سرعت اینترنت خیلی بالایی داره به طور میانگین من سرعتش رو توی طول استفاده خودم بین ۳۰۰ تا ۴۰۰ مگابایت برثانیه دیدم و یه نکته دیگه هم اینه که میتونین موقع اجاره یه ماشین مجازی یه سری کتابخونه رو به صورت پیشفرض روشون نصب کنین یعنی به این شکل که مثلا نسخه های متفاوتی از پایتورچ رو اماده نصب دارن که میتونین انتخاب کنین کدومش روی ماشینی که دارین اجاره میکنین نصب باشه منتها برای نگه داشتن فایل ها باید یه storage داخل سایتشون بسازین که فایلاتون اونجا ذخیره بشن که خب اونا هم هزینه خیلی خیلی کمی دارن به صورت ساعتی و همچنین میتونین تو حساب مثلا google cloud ای که دارین دیتا رو اکسپورت کنین برای هاست کردن مدل ها هم عالیه و خودش مثل space های هاگینگ فیس یه تعداد template داره که بهتون کمک میکنه راحت تر مدل ها رو هاست کنین مثلا توی template هاشون اکثر نسخه های stable diffusion رو دارن که میتونین استفاده کنین امیدوارم این اطلاعات کمک کننده باشه
-> عالی و مرسی دوست داشتی بذار توی کانالت و ما فوروارد کنیم توی پایتورچ
-> ممنونم توی کانالم گذاشتم خوشحال میشم توی پایتورچ هم بزارین
-> سلام من با تعداد زیادی اکانت برای خودم درست کردم و مشکلی نداشت تنها نکته برای وریفای این هست که شماره مجازی با ای پی شما باید یکسان باشد مثلا اگر شماره مجازی آلمان گرفتید ای پی هم حتما باید برای آلمان باشد تا smsش برای شما بیاد
-> بله منم این نکته رو تست کردم اما بازم مشکل داشت اتفاقا چندین تا شماره مجازی مختلف هم خریدم با چندین تا ایپی مختلف هم اکانت ساختم ولی خب یا اون سرور VPN من DNS Leak داشته یا هرچیز دیگه به هرحال موفق به تایید شماره نشدم حتی با اکانت یکی از اقوام که خارج از کشور ان و اونجا اکانت ساختن هم تست کردم با ایپی اون کشور بازم دسترسیم رو میبست چه جوری باید بهشون بفهمونیم قرار نیست با استودیو هاشون کسیو ترور کنیم
-> اینجا
-> دوستان احتمالا شما ام خیلی به فضای کولب عادت کردین و دوست دارین به جای ژوپیتر از اون استفاده کنین من متوجه شدم که با یه سری تنظیمات ساده میشه سروری که از این سرویس میگیرین هم به کولب وصل کنین چند تا مرحله ساده داره که شامل اینا میشن 1 وقتی سرور رو ساختین وارد تنظیماتش بشین 2 این کد رو تو باکس Docker command بنویسین bash c pip install upgrade jupyter_http_over_ws007jupyter serverextension enable py jupyter_http_over_ws cd startsh و از بخش environment variables توکن جوپیتر رو کپی کنینحواستون باشه موقع ساختن پاد باید تیک راه اندازی سرور جوپیتر رو زده باشین و پاد رو سیو کنین 3 موقعی که خواستین به پاد وصل بشین این کامند هم جلوی کامند اصلی بزارین L 8888localhost8888 توی این مورد کامند کامل میشه این ssh root13719827 p 10102 i sshid_ed25519 L 8888localhost8888
-> ادامه حواستون باشه که موقعی که دارین این کامند رو میزنین و وصل میشین به سرور نباید سرور جوپیتری راه اندازی کرده باشین 4 برین توی کولب و وارد بخش Connect to a local runtime بشین حالا اونجا باید توکن جوپیتری که کپی کردین رو توی کامندی که خودش گذاشته بزارین تو این مورد میشه همچین چیزی امیدوارم کمک کننده باشه
-> راستش نه ما توی نوت پد کد میزنیم برای اون جایگزین مشابهی سراغ داری
-> بله Note pad
-> نه نوت پد خیلی پر زرق و برقه idle پایتون گزینه مشابه تری نیست حتی متغیر هارو هم نمیشناسه
-> اونم خوبه کتیبه نویسی هم گزینه جالبیه به نطرم رو سنگ کد میزنیم
-> ادم پير ميكنه برادر"
"-> سلام وقتتون بخیر من نمیتونم اناکودا رو نصب کنم کسی میتونه راهنمایی کنه لینکش برام ایمیل میشه ولی روش که میزنم اررور میده
-> 
-> چرا از خود وب سایت آناکوندا مستقیما دانلود نمی کنید
-> به نظرم VPN بزنید
-> از این لینک آموزش نصب آناکوندا ببینید
-> "
"-> استاد سلام و خسته نباشید ایا در ادامه دوره اوپن سیوی به مبحث هایی مثل هندسه اپیپولار ریاضیات تشکیل تصویر و اینا هم پرداخته میشه
-> سلام توی فصل دوربین و نور از پرسپکتیو و پارامترهای ذاتی و کالیبراسیون میگیم این مباحثها میتونه خیلی سنگین باشه نمیدونم تا چه حد مدنظر شماست
-> تا حدی ک بشه مباحث مربوط به استریو ویژن و مباحث بینایی ماشین ۳ بعدی مقدماتشو فهمید خیلی کم به این مباحث تو قالب ویدیو در اینترنت اشاره شده
-> باید کتاب بخونید توی کتاب شلیسکی هست کورس دانشگاهی هم شاید خوب باشه توی وبپیج کتاب شلیسکی هم یکسری کورس دانشگاهی رو معرفی کرده که برمبنای این کتاب تدریس داشتن دوره ما کاربردمحور هست از مطالب اون قدری میگیم که خیلی توی کارهای عملی استفاده میشه نمیدونم شاید مطالب مدنظر شما گفته بشه شایدم نه چون ریاضیات این بخش میتونه خیلی پیچیده بشه"
"-> امروز زادروز پدر علم کامپیوتر و هوش مصنوعی و دانشمند برجسته علوم کامپیوتر یعنی الن تورینگ هست الن تورینگ ریاضی دان زیست شناس فیلسوف منطق دان دانشمند رایانه و رمزنگار انگلیسی بود او در ۲۳ ژوئن سال ۱۹۱۲ در لندن به دنیا اومد و در ۷ ژوئن سال ۱۹۵۴ در ۴۱ سالگی در اثر مسمومیت با سیانور درگذشت نبوغ او درهمان کودکی اشکار بود و او در ۱۴ سالگی به کالج شربرون در دورست در جنوب غربی انگلیس رفت کسی که ماشین تورینگش پایهی رایانههای امروزی و آزمایش تورینگ اش به یکی از بهترین روش ها برای ارزیابی هوشمندی یه کامپیوتر تبدیل شد الن تورینگ به کمک ماشین تورینگ نقش موثری در الگوریتم ها و محاسبات ایفا کرد آیا میتوان روزی گفت ماشین هوشیار است و میتواند فکر کند او سپس در آزمایشگاه ملی فیزیک در انگلستان شروع به کار کرد و یکی از نخستین برنامههای ذخیرهشونده در کامپیوتر را ساخت اما طرح او اجرا نشد او در سال ۱۹۴۸ به دانشگاه منچستر رفت تا روی منچستر مارک ۱ کار کند که به طور رسمی به عنوان اولین کامپیوتر واقعی جهان شناخته میشود در ۱۹۵۰ در مقالهای معیاری برای هوشمندی یک رایانه پیش نهاد که به آزمایش تورینگ معروف شد بهترین معیار برای هوشمند شمردن یک ماشین این است که بتواند انسانی را از راه یک پایانه تله تایپ طوری فریب دهد که او باور کند که با یک انسان روبروست به پاس خدمت های زیاد تورینگ به علوم کامپیوتر جایزه ای به نام جایزه تورینگ ساخته شد که به عنوان نوبل علوم کامپیوتر شناخته میشه از اونجایی که جایزه نوبلی برای علوم کامپیوتر وجود نداره از جایزه تورینگ به عنوان نوبل علوم کامپیوتر یاد میشه این جایزه به ادم هایی اهدا میشه که خدمات زیادی به علوم کامپیوتر کرده باشن مبلغ این جایزه ۱ میلیون دلار هست همچنین او در جنگ جهانی دوم با شکوندن رمز ماشین انیگما که وسیله ارتباطی رمزنگاری شده بین نیروهای نازی بود کمک زیادی به پایان جنگ کرد دستگاه سلطنتی بریتانیا احترام بسیار زیادی برای تورینگ قائل بود و تورینگ دارای نشان ویژه سلطنتی بود همچنین از سال ۲۰۱۹ تا اخر ۲۰۲۱ عکس الن تورینگ روی اسکناس های پنجاه پوندی چاپ میشد درمورد جایزه تورینگ حرفایی هست که باید بزنم و قراره تو یه پست جدا بهش بپردازم
-> آرتین خودت نوشتی خیلی خوب بود
-> بله ممنونم"
"-> تو سه سوت فایل های ارائت رو بساز سه سووت وارد سایت gammaapp میشی و اکانت میسازی 1فرمی که میاد رو پر میکنی اطلاعاتی مثل اینکه از کجا با ما آشنا شدی برا چی به ما احتیاج داری و اینا 2 روی گزینه Create new AII میزنی سه تا گزینه میاد که برای ساخت پاورپوینت اولیش یعنی Generate رو انتخاب میکنی 3 منویی باز میشه که باید موضوع ارائه رو براش بنویسی حتی به فارسی 4 بعد از ارسال از لیست مقابل Draft outlie زبان فارسی رو انتخاب کن همینطور تیتر های فایل رو هم بهت میده که میتونی بسته به نیازت کم و زیادشون کنی 5در نهایت میتونی فایل رو از سه نقطه بالا و گزینه Export با پسوندهای pdf و pptx ذخیره کنی
-> بدرد نمیخوره این"
"-> سلام دوستان من میخوام توی تصویر زیر قسمت داخل اون لبه های سفید رو کلا سفید کنم تا جایی که میدونم اول باید لبه ها رو به یک روشی پیوسته کنم و بعد داخلش رو با یک روش دیگه ای سفید کنم ولی اسم این روش ها رو نمیدونم ممنون میشم راهنمایی بفرمایید
-> اول dilation بزنید تا تصویر پیوسته بشه و حفره ها پر بشن بعد یک threshold بزنید تا استانه سفید و سیاه کاملا مشخص بشه
-> متشکر عکس باینری رو ترشهلد میشه زد
-> عکس سطح خاکستری و میشه آستانه روش زد و کاملا سیاه و سفید کرد
-> برای dilation حتما فیلتر و صلیب شکل و فرد بزارید مثلا ۳۳ یا ۷۷ و و لنگر و وسط فیلتر تنظیم کنید
-> آخه این عکس خودش باینری هست
-> خب باشه برای dilation چه بهتر که عکس باینری باشه کلا برای تکنیک های مورفولوژی عکس سیاه و سفید بهتر هم جواب میده
-> تا جایی که من میدونم dilation و erosion ابعاد آبجکت رو تغییر میدن و برای ایننجور کارها بهتر هست از open close استفاده بشه عکس هم خودش وقتی باینری هست آستانه گذاری کار اضافهای هست"
"-> سلام وقت بخیر یه سوال ساده دارم یکم بین علما اختلاف هست موقع ترین مدل و پیدا کردن هایپر پارامترای بهینه اینکه اینکارو روی کل داده ترین انجام بشه درسته یا اینکه روی داده ترین و یکمی از ترین هم بعنوان ولید برداریم هم درسته روش دوم غلطه ممنون
-> باید ولیدیشن داشته باشید دیگه توی دوره دیپ لرنینگ که گفتیم اما توی فصل انتخاب مدل دوره یادگیری ماشین مفصل درموردش صحبت کردیم این علمای روش اول کی هستن
-> هم لبی هندی من با اداعای بسیار در زمینه AI گیر داده روشت اشتباهه میخواستم با مستند بگم بهش
-> جزئیات روش شما رو نمیدونم و نظری نمیتونم بدم ولی بدون ولیدیشن چطوری متد رو ارزیابی اولیه کنیم
-> منم دقیقا همینو گفتم گفت روی کل دیتا جوابت بهتر میشه و نباید ولیدیشن داشته باشیم موقع ترین نتیجت میاد پایین تر من روشمو نشون استادم دادم گفت اوکی اما این همچنان همونو میگه البته اینا ترین میکنن بعد یه کی فولد هم جدا میرن که نشون بدن نتیجشون درسته من از همون اول یه فولد یه ولیدیشن گرفتم وترین بعدم باز kfold زدم که مطمئن بشن نتایج فرق چندانی نداره و بعد هم تست ایندیپندنت انجام دادم ممنون از راهنماییتون"
"-> شماره دوم مجله ی هوش مصنوعی مثله سری قبل خیلی خوب بود از مطالعش لذت بردم ممنون
-> ممنون امیر جان
-> سلام آقای دکتر مجلتون رو دیدم کار جالبه موفق باشید میتونید روی کمک منم حساب کنید
-> ممنون میلاد جان استقبال دوستان برامون جالب بود ایشالا بتونیم بهترش کنیم ممنونم بابت پیشنهاد همکاری برام ارزشمنده انشالله بتونیم از دانش شما بهره ببریم"
"-> یه تصویر جذاب دیگه از حمید نادری یگانه لینک توییت
-> ما که حال و سواد ساختن چنین فرمولی رو نداریم با ساختن و ترین یک شبکه عصبی به چنین خروجیای میرسیم
-> دستش راه افتاده دیگه
-> عددای فرمول رو عوض میکنه یهو یه تصویر جدید ساخته میشه
-> با گرادیان کاهشی اپدیتشون میکنه
-> به نظرتون تا حالا کسی صحت و درستی کارشو تست کرده ممکنه همین شکل چنتا فرمول نوشته باشه و یه عکسم بذاره بالاش
-> فرمول به حدی بزرگه که میصرفه اعتماد کنیم و رد شیم"
"-> حتما خدمتتون خواهم گفت اگر یادمرفت در خصوصی یک پیام بدید خدمتتون خواهم فرستاد
-> ممنونم چشم حتما"
"-> با سلام خدمت همگی یک سوالی داشتم من یکسری داده با برچسب 1 و 0 دارم تعداد یک ها در مجموعه داده اولیه بیشتر از صفرهاست من دیتای آموزشی که از روی این دادهها دارم میسازم به این صورته که دو داده از مجموعه آموزش انتخاب میشه اگر هر دو دارای Label یکسان بودن Label یک میگرن و اگر متفاوت بودن Label صفر میگیرن حالا با فرض اینکه بعد ساختن این pairs ها نزدیک 10هزار pairs منفی و 40 هزار pairs مثبت داشته باشیم اگر تو فرآیند آموزش دیتا رو به این صورت به مدل بدم ده هزار داده منفی به همراه ده هزار داده مثبت که بصورت رندوم انتخاب شده و چهار بار این کار رو انجام بدم یعنی چهار بار و هر بار ده هزار داده منفی به همراه ده هزار داده مثبت به مدل بدم با حالتی که به صورت کامل همه مجموعه داده رو یک جا بدم تفاوتی در آموزش مدل ایجاد خواهد شد
-> توی حالت دوم احتمالا مدل بایاس پیدا میکنه به نظرم حالت اول افزایش داده فیک بهتر هست حالا شاید بتونید از طریق آگمنتیشن کپی محض رو با یکسری دادههای ترنسفورم شده جایگزین کنید
-> ممنونم از پاسخ یعنی روش اول تفاوتی ایجاد نخواهد کرد اصلا اسم خاصی داره که من بتونم سرج بزنم و بخونم در موردش در مورد تکنیک افزایش داده داده جوریه که نمیشه ازشون استفاده کرد چون برچسب های مثبت و منفی به صورت experimental بدست اومده کمی تکنیک افزایش داده داستان داره البته down sampling انجام دادم ولی نتیجه جالب نشد"
"-> سلام دوستان وقت تون بخیر به نظرتون ترانسفورمرها و اتنشن میتونن در کلاسیفیکیشن داده های جدولی که خاصیت sequence ندارن کاربرد داشته باشند
-> سلام بله سال قبل موضول نسبتا داغی بود توی وبلاگ سباستین راشکا لیستی از این شبکهها رو آورده بود شاید توی کانال پایتورچ معرفی کرده باشیم
-> بنظرم بله چون تصاویر هم اول به ماتریس تبدیل میشن بعد به کمک این مدل ها پردازش میشوند
-> سلام خیلی ممنونم آقای دکتر کانال تلگرام منظورتون هست
-> ممنونم
-> tmepytorch_howsam
-> میشه لطف کنین آقای دکتر یک کلید واژه برای سرچ معرفی کنین
-> وبلاگ سباستین راشکا
-> یه مدلی مایکروسافت داره چند ماه پیش باهاش کار کردم خیلی خوب جدول رو اوکی میکنه
-> سلام میشه لطف کنین اسم مدل رو بفرمایید"
"-> با سلام و وقت بخیر مقالات اکسپت شده CVPR2024
-> دوستان ممنونم که مطالب خوب رو توی گروه میذارید"
"-> سلام آقای دکتر وقت بخیر ببخشید در مورد کلاس بندی تصاویر مثلا در cnn یا faster rcnn به نظر شما کلاس بندی به صورت فازی چه تاثیری خواهد داشت و اینکه آیا اگر این کار صورت بگیره دیتاها به چه شکل باید آماده بشه مثلا در لیبل گذاری تصاویر باید درجه عضویت نسبت به هر کلاس مشخص بشه و آیا این کار باید دستی توسط شخص اکسپرت صورت بگیره یا نه روش های دیگه ای هم هس
-> سلام بستگی به مساله داره برای بعضی مسائل میتونه مفید باشه باید لیبل کلاس سمپلها رو احتمالی از کلاسهای مختلف بدید مثلا 03 این 05 اون و اینها رو هم باید خودتون تعیین کنید اگه کار تخصصی باشه باید متخصص اون مساله این کار رو انجام بده
-> خیلی ممنون"
"-> Play with me become cryptoexchange CEO and get a token airdrop 2k Coins as a firsttime gift 25k Coins if you have Telegram Premium
-> استاد طبق قوانین گروه هرپیامی زیاد ری اکشن بخوره باید پین باشه
-> نه به مجمع تشخیص مصلحت گروهه
-> سلام آقای دکتر نظرتون در این مورد چیه
-> امروز تعطیله مجمع تشکیل نمیشه
-> استاد یه توکن برای هوسم ایجاد کنید که با انجام تمرینات و بیشتر نکردن سرعت ویدیو های دورها ماین بشه
-> خودم سرعت ویدئوها رو زیاد میکنم"
"-> گوگل کولب سه ویژگی هیجانانگیز جدید اضافه کرده که برنامهنویسی رو خیلی شیرینتر میکنن ویژگی Generate Code که با نگاه به کدهایی که در نوتبوکتون زدهاید براتون کدی رو که میخواید میزنه ویژگی Explain Error که ارورتون و دلیلش رو توضیح میده و اصلاحیهی پیشنهادی رو هم بهتون میگه ویژگی Gemini Chat که دستیار برنامهنویسی یا منتور فولتایمتونه
-> عالی کدنویسی داره جذابتر از گذشته میشه ویژگی چهارم یک ساعت دیگه از مدت زمان روزانه gpuی رایگان کم کردیم"
"-> نمیدونم توکنایزر ساب ورد برای فارسی وجود داره یا نه اما به نظرم اگه با ساب ورد ترین بشه میشه با مدل کوچیک تر هم جواب گرفت
-> طبق تجربه برای خیلی از این کارها باید توکنایزر رو خودمون جداگونه ترین کنیم مثلا برای برت که از Word piece tokenizer استفاده میکنه باید توکنایزر رو آموزش داد مثلا توی حوزههای تخصصی مثل پزشکی و حقوقی نیاز به مدل زبانی داریم"
"-> کلانپیکره متنباز زبان فارسی ترگمان ۴۰ میلیارد توکنی شد پیکرههای متنی به عنوان یکی از نیازهای اصلی توسعه مدلهای زبانی بزرگ LLMها شناخته میشوند شرکت ترگمان به عنوان یکی از شرکتهای دانشبنیان فعال در حوزه هوش مصنوعی و توسعه مدلهای زبانی بزرگ از مهرماه ۱۴۰۲ پروژهای را برای ایجاد کلانپیکره زبان فارسی آغاز نمود که نسخه اول آن با حجم حدود ۳۱ میلیارد توکن در اسفندماه ۱۴۰۲ منتشر شد اکنون پس از گذشت کمتر از ۳ ماه با افزایش چشمگیر تعداد سایتهای خزش شده به ۶۵۰ سایت و بهروزرسانی سایتهای خزششده قبلی حجم کلان پیکره فارسی ترگمان به ۴۰ میلیارد توکن افزایش یافته است شایان ذکر است کلان پیکره فارسی ترگمان به صورت آزاد و با حق بهرهبرداری CCBYNCSA در اختیار عموم قرار گرفته است همچنین با تفاهم میان شرکت ترگمان و ستاد توسعه فناوریهای هوش مصنوعی و رباتیک معاونت علمی فناوری و اقتصاد دانشبنیان ریاست جمهوری حق بهرهبرداری تجاری از این پیکره در اختیار کلیه شرکتهای دانشبنیان ایرانی قرار دارد بر این مبنا تاکنون نزدیک به ۲۰۰ فرد و شرکت ایرانی و خارجی مجوز بهرهبرداری از این پیکره را دریافت داشتهاند اطلاعات بیشتر در خصوص این پیکره در آدرس زیر در دسترس علاقمندان است
-> عالی کلان پیکره یعنی چی
-> حتما منظورشون huge corpus بوده که از این ترجمه زیبا استفاده کردن استاد به قول خودتون خیلی وقتا بهتره همون اصطلاح انگلیسی رو استفاده کنیم اینجا دیگه خیلی ایرانیزه شده
-> من حس میکنم منظورشون LLM بوده
-> اینو تو صفحش نوشته و فک کنم رو مدلی ترین نشده و صرفا متون خامه پس همون Corpuse بهتره این پیکره با نام کلانپیکره فارسی ترگمان Targoman Large Persian Corpus در اختیار عموم محققین و توسعهدهندگان ایزارهای پردازش زبان فارسی قرار گرفته است
-> که اینطور امیدوارم به زودی موجود بزرگ زبانی هوشمندشون هم ببینیم
-> آره دیتاست به نظرم برای LLM میشه گفت ابرمخلوق زبان گستر ایرانیان پژوهش
-> ۴۰ میلیارد توکن خیلیه امیدوارم یه روز برسه که همه تو خونه هامون بتونیم مدل زبانی بزرگ ترین بذاریم یه بار با ۴۰۰ میلیون توکن ترین کردم حدود ۳۶ ساعت زمان برد در نهایتم بخاطر کوچیک بودن سایز مدل نتونست خیلی خوب یادبگیره
-> با چه سخت افزاری
-> 3090
-> ما هم یه بار قرار بود با یکی از اساتید همکاری کنیم رو ۶ میلیارد توکن برای زبان فارسی برت رو ترین کنیم که مثلا نتایج پارس برت رو بهبود ببخشیم یه سری کارا انجام دادیم مثه پیش پردازش و و کد رو هم آماده کردیم که ببریم برا ترین اصلی موقع ترین شد استاد از دادن پول سخت افزار طفره رفت ما هم بیخیال شدیم
-> خیلی بد شد که انجام میدادین احتمالا اینکه موفق میشدین زیاد بود ولی با چه توکنایزری حجم دیکشنری چقدر بود
-> چون ماله یکی دو سال پیش هست دقیق خاطرم نی راستش
-> همینطوری که متن شما رو میخوندم حدسم این بود که الان میگه سختافزار جور نشد دانشگاههای ایران عالین"
"-> سلام دوستان کسی میتونه منو راهنمایی کنه چجوری یه فایل با پسوند arff رو به csv تبدیل کنم با سرچ موفق به انجامش نشدم ممنون میشم کمکم کنید
-> درود فایل arff وقتی لود کردی مجدد با pandas به یک دیتافریم تبدیل کن و بعدش دیتافریم رو بصورت csv یا هر فرمتی دیگه ای ذخیره کن
-> ممنونم ولی اینکاروکردم نشد احتمالا بخاطر نوع فایلمه نمیدونم مشکل چیه
-> اگر فایل محرمانه نیست یا اگر هم محرمانه هست یک رکورد از سمپلشو رو بفرست برام پی وی"
"-> سلام من سرویسای مختلفی رو برای اجاره GPU تست کردم و توی سرویس های ایرانی به مشکلات زیادی برخوردم که قبلا بهش اشاره شده ولی توی سرویس های خارجی به نظرم اگه از کولب پرو استفاده میکنین میتونید به جاش برید سراغ runpod این سرویس هم قیمت کولب هست و سرویساش کلا pay as you go هست هم میتونید مدلاتونو باهاش هاست کنین هم میتونین ازش GPU رو اجاره کنین و هم از طریق SSH هم یه سرور جوپیتر بهش دسترسی داشته باشین جای فایلاتونم همیشه محفوظه به نظرم ارزششو داره
-> ممنون آرتین جان پیامت رو بذاریم توی پایتورچ فوروارد نمیکنیم چون پیام خصوصی ممکنه دریافت کنی و اذیت بشی
-> خواهش میکنم بله مشکلی نداره"
"-> امروز ۶ امین سالروز انتشار مقاله GPT1 است مقاله ای با ۴ نویسنده و شروع پروژه ای که دنیای هوش مصنوعی را متحول کرد و آن را وارد زندگی ما کرد
-> 
-> لینک مقاله
-> "
"-> سلام دوستان این روند یادگیری غیر عادی نیست
-> تعداد داده های ولتون چندتاس
-> حدود 400 تا
-> یکم عجیب که اکیورسی ول از ترین بیشتره فکر کردم تعداد دادهدهای ولتون خیلی کمه
-> بله برای خودم هم عجیب هست دیتا train حدود 1600تا هست
-> اما ی نکته ای هم هست درصد بهبود لاس ترین نسبت به لاس ول بیشتر
-> درسته
-> چرا غیرعادی هست راستی این عجیب نیست که متریک ولیدیشن از ترین بیشتر باشه بالاخره این هم یک حالتی هست که ممکنه پیش بیاد
-> درسته فکر می کردم باید validation پایین تر باشه
-> اگر میخوای مدل های مختلف مقایسه میکنی میتونی از روش کراس ولیدیشن استفاده کنی برای دیتا های کم بهتره
-> مقایسه نبود اما داده ها حدود 2400 در کل بود
-> 
-> سلام وقت شما بخیر درسته باید توضیحات بیشتری فرستاده میشد غیر عادی بودن همین روند یادگیری بود که فکر می کردم از اول باید train بهتر از validation باشد و البته اینکه یکبار 64 هست یکبار 91 هم برام جای سوال هست ولی این قضیه رو قبلا هم در کدهای دیگه دیدم ولی علتش نمیدونم تصویر از اپک اول تا تقریبا اپک 10 هست و تا اپک 20 که رفت تقریبا با هم برابر شدند 94 ممنون از پاسخگویی تون و نکات فنی که فرمودید
-> پس همون نیاز به ترین طولانی داشته درمورد دراپ یکباره دقت رفتارش کمی به انفجار گرادیان میخوره شبکههای بازگشتی و ترنسفورمر معمولا چنین چالشهایی دارن و نیاز هست گرادیان رو کلیپ کنید البته شاید مشکل شما این نباشه
-> بله حق با شماست با train بیشتر مشکل حل شد بله شبکه gru هست ممنون از راهنمایی تون
-> پس توی شبکه های بازگشتی یا از اینور بوم میوفته vanishing gradient میشه یا از اونطرف میوفته exploding gradient میشه
-> محو گرادیان بیشتر در شبکه vanilla rnn هست مثلاهمین gru چنین بحرانی رو نداره ولی انفجار رو دارن ترنسفورمر هم داره یک دستور یک خطی توی پایتورچ باید اضافه کنید اون موقعی که بکوارد شد بعدش کلیپ میکنید و بعد بهینهساز رو یک استپ اجرا میکنید تو دورهها گفتم فکر کنم توی دیپ کاتالیست اصلا چنین حالتی پیش اومد
-> عاره یادمه ی جایی خونده بودم برای اینکه محوشدگی به وجود نیاد lstm و gru به وجود اومد ولی exploding رو ندیده بودم جالب بود"
"-> سلام یه سوال داشتم من لیسانس مهندسی پزشکی گرفتم و الان میخواهم ارشدمو هوش مصنوعی ادامه بدهم یکسال وقت دارم تا کنکور ارشد میخواستم بدونم باید چه منابع و چه چیزایی بخونم که توی فوق لیسانس برام مشکلی پیش نیاد
-> برداشت من این بوده که کنکور منظور نیست و هدف موفقیت در مقطع ارشد هست پایتون و یادگیری ماشین رو عمیقا یاد بگیرید شانس موفقیت در مقطع ارشد بسیار زیاد میشه البته اصلا کار آسونی نیست
-> ممنونم دوره دیپ لرنینگ هم توی ارشد مهمه
-> بله قطعا مهم هست منتها به نظر من یک فرد با مهارت عالی در کدنویسی پایتون و دانش عمیق در یادگیری ماشین بهتر از یک فرد با دانش و مهارت پایتون یادگیری ماشین و عمیق معمولی هست با اونها شروع کنید اگر به سطح خوبی رسیدید بعدا وارد یادگیری عمیق بشید
-> ممنونم از توضیحات کامل"
"-> 
-> ببخشید من این رو ندیدم ظاهرا گذاشته بودین"
"-> بله درسته ولی مقاله ای خوندم2023 که از دیفیوژن مدل برای دسته بندی استفاده کرده
-> بفرستید مقاله رو حداقل برای من جدیده همون احتمالا باید رفرنستون باشه
-> لینک مقاله
-> 
-> سلام به این مقاله یک نگاه بیندازید Your Diffusion Model is Secretly a ZeroShot Classifier Alexander C Li et al"
"-> فک کنم اشتباه برداشت کردید اصلا ذاتا و به هیچ وجه نمیشه همینجوری از دیفیوژن ها برای طبقه بندی استفاده کرد
-> تو این مقاله که اتفاقا بچه های ایرانم نوشتن دسته بندی جالبی ارائه شده از جمله توی دسته بندی و تقطیع
-> ممنونم از توجهتون"
"-> سلام وقت بخیر عذر میخوام کسی از generative models ها مثل diffusion برای کلاسیفیکیشن استفاده کرده
-> چی یعنی چی"
"-> سلام کسی بلده فایل ipynb رو به فایل pdf تبدیل کنه
-> توی خود جوپیتر نوت بوک گزینه اش هست موقع دانلود فایل"
"-> سلام دوستان برای نصب پایتورچ برای gpu قبلش باید چیز خاصی نصب کرد مثلا cudnn و اینکه اگه کارت گرافیک Nvidia نباشه و مال تسلا باشه نصب پایتورچ برای gpu فرق داره و آیا کار خاصی باید انجام داد
-> تسلا مگه کارت گرافیک داره
-> ممنون آقای دکتر این کارت گرافیک های v100 مگه مال تسلا نیست
-> اگه منظورتون Tesla T4 و امثالش هست اینا هم مال خود nvidia هستن
-> چه جالب ممنون من همیشه فکر میکردم این ها مال تسلا هست
-> سلام استاد در یکی از جلسات گفته بودند اما من عکسش رو فرستادم مثل عکس انتخاب کنید اگر لینوکس بودید لینوکس و اگر ویندوز ویندوز و بعد کد رو کپی کنید و اجرا کنید البته قبلش فک کنم گفتند بهتره یک env جدید بسازید"
"-> سلام دوستان شبتون بخیر برای مرور مفاهیم ریاضی و جبرخطی ماشین لرنینگ به جز کتاب mathematics for machine learning چه سورس و کتاب دیگه ای پیشنهاد میکنید اگه منو راهنمایی کنید ممنون میشم دوستان
-> "
"-> سلام خدمت اساتید و دوستان گرامی دسترسی به دیتاست برخی از مسابقات kaggle محدود شده هست و نوشته در صورتی میشه داده رو دانلود کرد که دعوت شده باشید آیا راهحلی برای دسترسی به اون داده ها هست تشکر
-> فکر کنم اینطوری هستن که مثلا یک استاد دانشگاهی برای دانشجوهای خودش برگزار میکنه و جز اونها دانشجوهاش کسی نمیتونه در مسابقه شرکت کنه کگل فقط میزبان مسابقه هست
-> خیلی ممنون از پاسختون"
"-> سلام استاد خواستم تشکر ویژه کنم بخاطر دوره دیپ کاتالیست من چند تا ویس انگلیسی داشتم که تقریبا روی هم ۱۴ ساعت بود میخواستم تبدیل به متنشون کنم اپلیکیشنهایی که امتحان کردم تا ۳ دقیقه از فایل رو رایگان برام تبدیل میکردند یادم به پروژه تبدیل متن به گفتار دوره افتاد و مطالبی که عنوان کردید با استفاده از مدل تبدیل گفتار به متن ویسپر تونستم ویسها رو به متنهای با کیفیت بالا تبدیل کنم تجربهی خیلی خوبی بود
-> سلام ممنونم"
"-> سلام دوستان وقت بخیر من یک دیتاست از داده ای شتاب دارم که شامل ۲۸ هزار تا سمپل میشه و طول هر سمپل هم ۲۵۶ ثانیه هستش بعد می خوام این رو نرمالایز بکنم چون حجم دیتام زیاده نمی تونم مستقیم ازش میانگین و انحراف معیار بگیرم می خواستم بپرسم که می تونم از maximum likelihood استفاده کنم و میانگین و انحراف معیار رو تخمین بزنم یا این روش درست نیست
-> بله میتونید
-> خیلی ممنون استاد
-> کسب تجربه از کار یعنی این پیام کسی که سراغ پروژه نرفته باشه کی به ذهنش میرسه که یک محاسبه میانگین و انحراف معیار ساده میتونه انقدر چالشی بشه اگه نمیدونید پروژه از کجا پیدا کنید با مسابقات کگل شروع کنید
-> به نظر من برای این تصمیم گیری بهتره معیار داشته باشید و مناسبترین معیار توزیع داده هاست توزیع آماری مسیر بهتری رو جبوتون قرار میده
-> مشکل همینه که به خاطر حجم بالای داده ها نمی تونم همه داده ها رو لود کنم و توزیع اش رو رسم کنم
-> خوب وقتی حجم بالاست روش مناسب نمونه برداریه با توجه به گستره دیتاتون یکحجم مناسب از نمونه رو جدا میکنید بهتره که چندبار این کار رو بکنید و توزیع آماری نمونه ها با توزیع کل زیاد تفاوتی نخواهد داشت البته شرایطی هم داره اینکه دیتاتون نویزی نباشه و بایاس نشده باشن که البته اگر این موارد رو داشته باشن نرمالیزه اشون کنید باز اگر این مشکل رو داشتید احتمالا دیتای مناسبی برای پروژه ندارید یا دیتاتون خیلی پیچیده است
-> اها درسته خیلی ممنون"
"-> سلام وقتی لاس منفی میشه علتش چی میتونه باشه
-> سلام باید جزئیات بگید مثلا توی گن لاس میتونه منفی شه و در عین حال خروجی هم داشته باشید سوالهای کلی و بدون جزئیات خیلی خطرناکن چون معمولا جوابهای دریافتی این نوع سوالها باعث اتلاف وقت و گمراهی بشه
-> لاس انطباق دامنه هست لاس از دو قسمت ایجاد شده لاس کلاسیفیکیشن و لاس فاصله ی توزیع دو دامنه منبع و هدف دقت ولی رفته رفته زیاد میشه
-> این هم نمودارش هست آیا مشکل لاس اینه که نمیتونه دامنه ها رو با هم تطبیق بده
-> یادگیری ماشین ویدئو ۱۵۵ ۱۹۳۷ تا ۲۰۰۳ رو ببینید"
"-> توصیه Yann LeCun به دانشجویان و محققان AI Dont work on LLM This is in the hands of large companies theres nothing you can bring to the table You should work on nextgen AI systems that lift the limitations of LLMs روی مدلهای زبانی LLM کار نکنید این حوزه در دست شرکتهای بزرگ است و شما چیز زیادی برای ارائه ندارید بهتر است روی نسل بعدی سیستمهای هوش مصنوعی که محدودیتهای LLMها را برطرف میکنند کار کنید
-> کاملا واضحه کی میتونه با این کامپیوتر ها امکانات و حتی lab های ضعیف دانشگاهی روی این مدلهای وحشتناک میلیاردی پارامتر برای ایجاد مدلرقویتر کار کنه وقتی میبینی ایجاد یه مدل pretrain شده زبانی حتی کوچک برای کارهای متوسط روی فارسی اونم تخصصی مثلا حوزه پزشکی مهندسی علوم دیتا هایی میخواد که یا نیست یااستاندارد نیست یا در اختیارتون نمیگذارن و عیر اون بر ای هر کار مدل زبانی وافعا چندمتخصص linguistic باید دردسترس باشن انقدرم پیشرفت وحشتناک شده که ادم حس تحقی میکنهاین به این معنی نیست که ایرانیها ضعیفن اتفاقانفرات ما اونور غوغا میکنن به این معنی واقعاپیشرفت هوش مصنوعیدر یککشور امکانات فرا شخصی و سازمانی حتی به نظرم دولتی میخواد که همه میدونیم اینجا چه خبره تنها ارزوی من اینه تو این حوزم یه روز مسخره کشورهای همسایه با این سرمایه گذاریشون نشیم
-> محدودیت LLM مثلا چیزی که من دیدم اون اوایل از نظر زبانشناسی به جد semantic گه بودو تا جدودی pragmatic هم اولیه بود Pragmatic یعنی شما داری از چیزی صحبت میکنیکه انتظار داری معنی نهفتش رو طرف مقابل بگیره مثلا به کسیدر جایی بست میگی هوا گرمه چقدر اون میگه میخوای کولر رو روشن گنم اینو الان دیدم توی GPT4o بهبود پیدا کرده ما توی زبانشاسی از morphology استارت میزنیم اخرین رده ها میشن semantic pragmatic discourse بحث discourse که نهایتشه اینه که بتونه گفتگو ژانر خپدشو بگیره مثلا میبینی یه ادم سیاسیاصطلاحات خودش رو تو اون فضا داره یک ادم کارگردانسینما توی زمینه خودش تیپ گفتگوی خودش رو داره و قرار میست ادمهای عادی بفهمن حالا از gpt انتظار میره بتونه ژانرپذیر باشهو با یک کارگردان عین کارگردان با یک سیاستمدار توی فضای سیاسیو پ مثلا بایه وکیل توی فضای حقوقی بجثا و گفتگپی نسانواره نه سوال و جواب اکادمیک که میتونه هنوز این جای کار داره ولی چطوری میشه اینجا این کارها رو کرد رویاون مدلهای پردازشی وحشتناک شاید یه گوشه کوچک چرا
-> جایی هست که بشه بیشتر درباره محدودیت LLM ها مطالعه کرد
-> سلام من حقیقت تازه سعی کردم یه مطالعه جدی روشون داشته باشم ببینم تا کجا میشه رفت جلو بنظرم دو دسته محدودیت هست بطور کلی ۱ تو حوزه زبانشناسی رایانشی از نظر مقایسه با زبان انسان و تعاملات انسانی محدودیتها رو میتونین شناسایی کنین کمی باید وارد زبانشناسی بشین و بعد ببینین یه مدل پیشرفته مثل gpt کجاست میتونین کتاب ساده جرج یول رو بخونین به اسم زبانشناسی برای اشنایی با مفاهیمش ۲ محدودیتهای شبکه های عصبیشم عین سایر مواردی دیگه که توی همین کروه دوستان دارن با مطالعه مدلها و سرچ میتونین حوزه هاشو پیدا کنین
-> من میخواستم تازه برم سمتش
-> برای چی پایاننامه
-> بله
-> استاد نظر شما چیه تو پایان نامه ازش استفاده کنیم یا ن
-> برای پایاننامه باید related works رو جدی بگیرید در واقع کارهای دیگران و ضعف و قوتهاشون میتونه نشون بده که چه مسیری و چه روشی رو انتخاب کنید"
"-> دوستان سلام وقت بخیر یه سوالی داشتم یه کتابی می خواستم که رفرنس انگلیسی باشه و مبانی هوش مصنوعی و یادگیری عمیق و ماشینی رو خوب توضیح داده باشه خوشحال میشم اگه رفرنس خوبی رو خوندید به من هم اطلاع بدید
-> سلام بنده یه سری کتاب داشتم اینجا فروارد کردم شاید مفید باشن
-> دست شما درد نکنه خیلی لطف کردین
-> خواهش میکنم دوست عزیز"
"-> سلام وقت بخیر امروز تونستم مجله ی خبرنامه جدید رو بخونم خواستم تشکر کنم بابت مطالب خوبتون و اینکه توی مدیای فارسی واقعا جای همچین فعالیت های زیبایی خالی بود فعالیت این مجله به چه صورتیه هفته نامست ماهنامه است یا گاهنامه
-> سلام ممنونم بابت فیدبک خوشحالم که دوست داشتید گاهنامه هست توی کانال اصلی هوسم با انتشار هر شماره اطلاعرسانی میکنیم"
"-> سلام دوستان نتایج ارشد اخیرا منتشر شد و بنده برای انتخاب رشته کمی دچار مشکل شده ام به نظرتون بین گرایش سخت افزار دانشگاه پردیس شهید بهشتی و هوش مصنوعی دانشگاه سراری قم یا علوم تحقیقات کدوم رو انتخاب کنم و مسیر اصلی قرار بدم بنده در ایران سهامدار شرکت هستم و مشغول به کار علت خواندن ارشد هم گرفتن امریه و کسب تخصص برای تاسیس شرکت تخصصی خودم هست ممنون میشم بنده رو راهنمایی بفرمایید
-> سلام رفیق ی پیام لطفا ب من بده اون چیزایی ک میدونم رو میگم"
"-> سلام اقای دکتر اشرفی تبریک برای ۱۵
-> سلام ممنونم ایشالا یه روزی همه باهم برنابئو"
"-> سلام وقت همگی بخیر با اجازه استاد از دوستان کسی ردیابی چندموضوعه کار کرده چند تا سوال داشتم سپاس
-> سلام شاید بهتر باشه سوالها رو مطرح کنید من کار کردم"
"-> قهرماننننننننن
-> for ever
-> برا ی اصفهانی زشته سپاهانی نباشه
-> سنگین بود"
"-> شرکت OpenAI از هوش مصنوعی ChatGPT Edu برای دانشگاهها و مدارس رونمایی کرد شرکت OpenAI از نسخه جدید ChatGPT با نام ChatGPT Edu رونمایی کرد که بهطور خاص برای دانشجویان دانشگاهیان و اساتید طراحی شده است این مدل برای مراکز آموزشی طراحی شده است که میخواهند هوش مصنوعی را بهطور گستردهتری در دسترس دانشآموزان و جوامع دانشگاهی خود قرار دهند مدل ChatGPT Edu به جدیدترین مدل زبانی بزرگ این شرکت یعنی GPT4o دسترسی دارد OpenAI ادعا میکند که این مدل در تفسیر متن کدنویسی و ریاضیات تجزیهوتحلیل مجموعه داده و امکان دسترسی به وب بسیار بهتر از نسخههای قبلی خود عمل میکند جزئیات بیشتر
-> برای دسترسی داشتن بهش باید درخواست بدین که خیلیم تو قبول کردنش سخت گیرن درخواست منو رد کردن
-> بگید چه اطلاعاتی وارد کردید با حالات مختلف بهش درخواست بدیم تا نهایتا پیدا کنیم چه حالتیو قبول میکنه
-> ببینین تو فرمش که یه سری اطلاعات عادی میخواد مثل اسم و ایمیل کاری و یه سری اطلاعات هم میخواد مثل اسم شرکت و وبسایت شرکت و تعداد کارمندای شرکت که همشونم فیلدای ضروری ان و یه توضیح میخواد ازتون راجب اینکه چطوری قراره این مدل رو استفاده کنین من فکر میکنم بخش اخر از همشون مهم تره سعی کنید طوری جلوه بدید که انگار دارید برای یه هدف بزرگ و عجیب غریب استفادش میکنین چون به من تو متن ایمیلی که بهم دادن گفتن که هدف ما اینه که این مدلو برای کار های بزرگ و مفید قرار بدیم"
"-> کار خودم پردازش ویدیو های اولتراسوند هستش نمیدونم کدوم بتونه جوابگو باشه
-> حتما باید لپ تاپ باشه اگر مقایسه کنی لپ تاپ ها هزینه طراحی و ظرافت میگیرن سیستم pc به صرفه تره بنظرم
-> بله دانشجوی شهر دیگه ام باید همراهم باشه همیشه
-> پس بنظرم وزن رو هم خیلی در نظر بگیرید چون آسیب فیزیکی داره براتون"
"-> لپتاپ خودم با این مشخصات بود ویکتوس دچار مشکل شد الان گارانتی میخواد برام عوض کنه و ۵ تا مدل برام پیشنهاد داده کدوم خوبه
-> با توجه به کانفیگ هایی که نوشته شده اولی از همه بهتر و جدیدتر هست"
"-> سلام برای تابستون تخفیفی چیزی در نظر میگیرین
-> سلام بله تولد هوسم هم نزدیکه احتمالا اواخر خرداد تخفیف میذاریم توی کانال هوسم اطلاع رسانی میشه
-> عجب تولدی
-> ایشالا یه روز یک جمع حضوری داشته باشیم
-> استاد برای دانشجوهایی که بیشتر دوره هاتون رو باهاتون پیش اومدن یه تخفیف خوب واسه یه دوره بهشون نمیدید که روحیه بگیرن
-> اینجور آدما به پشتیبانی پیام بدن
-> سلام استاد وقتتون بخیر دوره جدید بینایی کامپیوتر زمان برگذاریش مشخص هستش
-> سلام بعد دوره اوپن سی وی احتمالا اواسط تابستون"
"-> احتمال زیادی هست اگر تا الان اپنهایمر بعنوان کسی که سرنوشت بشر رو به ساعت نمادین ۱۲ نیمه شب و پایان نزدیک کرد جاش رو در اینده به کسی بده که در اینده بنیانگذار هوش مصنوعی کاملا خوداگاه بشه امیدوارم خیلی زود نباشه اون موقع
-> یکی از فیلم های دهه 80 میلادی راجع به 2020 ضبط شده بود بعد هرکی تو خونش یه ماشین پرنده داشت و تو خیابون هم پر از ربات های مختلف بودجالبیش اینه خبری از تلفن همراه و چیزایی که الان توی 2024 عادی هستن نبود فکر کنم اکثر پیش بینی های الان نسبت به آینده هم همین شکل باشن کلا تفکر اضطراب آور یه چیز خیلی خواستنی از طرف مردم هست همیشه دوست دارن یه چیزی برای ترسیدن داشته باشن"
"-> Sam Altman Says OpenAI Doesnt Fully Understand How GPT Works Despite Rapid Progress
-> رسما سام آلتمن مدیر OpenAI در اجلاس جهانی اتحادیه مخابرات دنیا در ژنو سوییس اعلام کرد که همزمان با پیشرفتهای GPT همانطور که نمیدونیم در سطح نورون به نورون تو مغز چی رخ میده تا ما به چیزی فکر کنیم OpenAI واقعا نتونسته اتفاقات GPT رو کاملا درک کنه با تکیه اینکه ما هم مدل های نوع قابل تعبیر و هم مدلهای نوع تجربی داریم
-> کلا یک چیز ترسناکه اونم این هست که ندونی مدل چطور عمل میکنه و دسترسی به اصلاحات داخلیش نداشته باشی این همان چیزی است که در انسان نیست هر فرد با کنترل مغز خود می تونه جلوی یک فاجعه تفکری رو بگیره مثلا فرض کنید انسانی بخواد خاطرات سخت زندگی که می تونه منجر به عقده های روانی بشه رو کنترل کنه این داستان در مدلهای هوش مصنوعی وجود نداره مدل هرچی یاد گرفت رو نمی تونه خودش حذف کنه و انباشتگی افکار سخت و روانی در ساختار مدلهای حاضر می تونه هوش مصنوعی ترسناکی رو به وجود بیاره یعنی انسان می تونه با ترک یادآوری خاطرات تلخ و سخت اون اتصالات نورونی رو از بین ببره در صورتی که در ساختار مدلهای پیش رو چنین چیزی وجود نداره
-> دهه ۵۰ میلادی که کار بررسی یک نورون برای حل مسائلی خیلی ساده شروع شد کسی تصور نمیکرد دهه دوم قرن بعدی یک حافظه عظیم گلوبالی به اسم اینترنت و ترکیبش با این مدلها بتونن به رابطه اینتراکتیو با انسان برسن پس ماهم نمیتونیم درک کنیم مدلهایی هم میتونن بیان که به making decision برسن شاید اون مدلها به نوعی Biobased باشن و ترکیبی از هوش مصنوعی و ساختارهای ارگانیک مغز باشن کسی چه میدونه
-> اینکه اینده چی میشه نامشخص هستش اما اگر با این ساختارها پیش برن وضعیت خوبی پیش رو نداریم اما در کل اگر بخوان به ساختازی مشابه مغز برسن باید کلی تغییرات داده بشه و در آخر هنوز کلی از عملکرد مغز بعد از چندین دهه نامشخص مانده اینکه بتونن همچین کاری بکنن خودش کمی ابهام داره
-> جالبه امروز اینستاگرام تصادفی بحثی رو دیدم که لینک هم داره و به فابلیت تکثر خودکار تکنولوژی در اثر هوش مصنوعی نقطه singularity گفته میشه که طبق تخمین ۲۰ سال ازش فاصله داریم جایی که نقطه برگشت ناپذیر تکنولوژیهاست و هوش مصنوعی میتونه خودش خودشو توسعه بده"
"-> سلام کسی تجربه استفاده از معماری میکروسرویس در پروداکت پروژه های یادگیری ماشین داره اصلا کاربرد داره در کل میخوام بدونم برای ارائه یک محصول هوش مصنوعی از کدام معماری مهندسی نرم افزار استفاده می کنند ممنون میشم راهنمایی کنید
-> سلام وقت بخیر بستگی به نوع پروژه و تعداد یوزر داره
-> شما تجریه استقرار دارید مهندس یادگیری ماشین چقد باید روی این مفاهیم مهندسی نرم افزار عمیق بشه
-> بله MLOPS را بررسی بفرمایید بستگی به سطح پروژه و نیاز کارفرما داره
-> سلام بسته به اینکه مدل قراره در چه محیطی اجرا بشه تفاوت اجرایی داره مثلا دو محیط پیش رو دارید ۱ فناوری لبه یا edge 2 فناوری کلود در هر کدوم میکرو سرویسی قابل اجرا هستش اما ملزومات متفاوتی داره در کل اجرای میکرو سرویسی تیاز به مدیریت منابع داره به عنوان مثال GPU مثل CPU عمل نمیکنه که شما بیاید و با یک فایل نسبتا ساده مانیفست در کوبر بین پاد هاتون تقسیمش کنید در یادگیری عمیق شما باید خیلی حرفه ای تر GPU رو بین پادها تقسیم کنید که این کار با پلاگین های انویدیا انجام میشه در کل پروسه وقت گیری هست پس باید اول مدنظر داشت که آیا نیاز هست یا نه در ضمن برای استفاده بهینه از GPU در این حالت نیاز به GPU خاص هستش مثلا یک مورد اینکه GPU شما قابلیت MIG رو داشته باشه"
"-> سلام دوستان وقتتون بخیر سیستم پیشنهاد دهندهای که اکثرجاها میبینیم با چه الگوریتمی نوشته میشه
-> این یک شاخه هست و بنابراین روشهای مختلفی براش وجود داره مثلا کمپانیهای بزرگ روشهای خاص خودشون رو دارن چند سال پیش نتفلیکس یک توضیحاتی درمورد سیستمش داده بود سادهترین شکلش رو اندرونگ توی کورس یادگیری ماشینش گفته این موضوع یکی از پروژههای ماشین کاتالیست هست ایشالا"
"-> سلام استاد وقت بخیر سایت بخصوصی برای تهیه همچنین بلوک دیاگرام هایی میشناسید
-> احتمال زیاد با نرمافزارهای آفیس کشیده شده حالا یا پاورپوینت یا ویزیو ورد هم میشه اما به نظرم پاور و ویزیو راحتتر هستن من همه شکلهای آموزشها رو با همینها میکشم و برای پایان نامه و مقاله هم از همینها خصوصا ویزیو استفاده کردم من کارهام رو با drawio پیش نبردم ولی باهاش آشنا هستم و به نظرم در لول ویزیو هست فکر کنم میتونید شکلهاتون رو روی کاغذ بکشید و عکس بگیرید یا اینکه شکل آمادهای رو بدید به چت جی پی تی که کد بلوک دیاگرامشو بزنه و توی drawio کد رو اضافه کنید فکر کنم ویزیو هم میشه
-> سلام استاد ممنون از راهنمایی تون drawio رو تست کردم خیلی فوق العاده بود و همونطور که گفتید چت جی پی تی کد ترسیمش رو میده البته انقدر کار باهاش ساده و لذت بخشه که نیازی به اونم نیست
-> ممنونم بابت توضیح"
"-> خب این که طبیعیه نکتهی منفی اش کجاست
-> چی کی
-> من زود فرستادم پینگ پروکسی بالا بود میگم این که کثیفه خب طبیعیه چرا یجوری دربارش صحبت میکنید که انگار جالب نیست باهاش کار کنیم
-> بنده خدا توی همون پیام گفته متاسفانه آخر سر بیخیال شدم
-> جالب شد برم ببینم چیه امیدوارم خیلی کثیف باشه
-> پس لطفا کثافتترینهاش رو با ما هم به اشتراک بذار
-> دانلود کردم آقای دکتر لجن گرفتتش نتایج و شیر میکنم باتون"
"-> مقاله زیر رو دقیق بخونید آموزنده هست البته صرفا بعد علمی رو منظورم نیست بلکه توضیحاتی درمورد مسیر انجام پروژه و فضای کار میگه که ارزشمند هست مثلا ابتدا سعی کردیم محیط آزمایش رو انقدر کوچیک کنیم که سرعت انجام آزمایش بره بالا و اگر قرار بود این مسیر شکست بخوره سریعتر تکلیفش مشخص بشه مساله تشخیص محصولات باسلام یک تجربه عملی از بهکارگیری LLM ها
-> سلام استاد اين مطالب واقعا جالب بود يه بار اين ديتاست باسلام كار كردم و ديدم انقد حجم زياده مخصوصا ستون ها بعد بسيار كثيف و پيچيده بود متاسفانه اخر سر بيخيال شدم
-> اغلب کارهای واقعی کثیف هستن کارهای آموزشی و دانشگاهی معمولا تروتمیز هستن
-> کثیف بودن منظورتون ساختاری است یا علمی
-> نه اينطورى نيست يعنى كه داده هاى كه اضافه يا پرت داده غيره ديگه بيشتر داده ها كه غير مرتبط هستن بدرد نميخورن و و و داستان ديگه در كل به اين ميگن داده كثيف ربطى به علمى ندارن
-> اها چون معمولا صحت کارهای عملی و دنیای واقعی خیلی بیشتر از مقالات
-> در جواب پیام مهدی گفتم دیگه دیگه چطوری ساختاری یا علمی بشه"
"-> استاد نظرتون در این مورد چیه
-> جالب بود ممنون نظری ندارم
-> منظورش از خط آخر چیه
-> شاید خودش بیشتر دراین مورد توضیح داده باشه احتمالا مصاحبه هست حالا به صورت کلی برداشتم رو میگم خب محدودیتهای LLMها چی هست هزینه دیتا نیاز به فاین تیون و یکسری موارد دیگه مثل تفسیرپذیری بایاس و غیره حالا حرفش این هست همونطور که ظهور ترنسفورمر یک موج دیگهای از پیشرفت در هوش مصنوعی بود و به خاطر قابلیت مقیاس پیدا کردنش LLMها ساخته شدن حالا باید دانشجوهامحققها برای رفع این محدودیتها تلاش کنن ساده بخواییم به قضیه نگاه کنیم الان LLMهای مختلفی که اومدن از لحاظ معماری تفاوت قابل توجهی باهم ندارن دیگه انگار به یک سطح اشباعی رسیده و اتفاقا عملکردهاشون هم به هم نزدیک هست میگن دیگه افراد جدیدی که وارد میشن دوباره روی همینها کار کنن نتیجه خاصی نخواهد داشت طرحی نو دراندازید البته به نظرم این مسائل شامل حال ماهایی که در کشوری زندگی میکنیم که جایگاه خاصی در هوش مصنوعی نداره نمیشه
-> متشکرم
-> "
"-> سلام استاد وقت بخیر من تا انتهای فصل سوم یادگیری عمیق پیش اومدم و هنوز مسیر طولانی در ادامه دارم فقط میخواستم یک تشکر و خسته نباشید خدمتتون داشته باشم کاملا مشخصه چه وسواس انرژی و زمان زیادی برای تهیه آموزش ها گذاشته شده و اونقدر با بیان روان و شیوایی مطالب پله پله گفته می شن که اشتیاق برای ادامه یادگیری دو چندان میشه خواستم به نوبه خودم از شما و تیم خوبتون تشکر کنم برای زحمتی که برای تهیه این آموزش کشیدین
-> ممنونم خوشحالم که جزئیات کار دیده میشن براتون در این مسیر موفقیت آرزومندم"
"-> سلام و وقتتون بخیر یادگیری هوش مصنوعی رو از کجا باید شروع کرد
-> سلام فصل یک دوره اموزشی یادگیری ماشین هوسم"
"-> اما با این ارور مواجه میشم کسی میدونه چکار باید کرد این کد شبکه ی عصبی قسمت ماشین لرنینگ هست که آقای دکتر گفتن
-> سلام میگه با ستاره نمیتونه ضرب رو انجام بده فکر کنم برای ضرب باید از دستورات خود پایتورچ استفاده کنین
-> بله دقیقا ممنون امتحان میکنم
-> نمیخواستم جواب بدم چون قبلا گفتم سوال رو توی گروه خودش بپرسید اصلا مگه توی دوره یادگیری ماشین پایتورچ گفتیم که میگید باشه امتحان میکنم گفته ضرب بین یک متغیر float و None امکان پذیر نیست باید چک کنید که چرا یک متغیر None هست
-> اومدم امتحان کنم دقیقا همین یادم افتاد که پایتورچ رو اونجا نگفتید و اینکه فکر کردم توو این گروه همه سوالا رو میشه پرسید پوزش"
"-> دوستان مرکز تحقیقات هوش مصنوعی پارت اومده یه LLM رو مخصوصا روی زبان فارسی اموزش داده که طبق چیزی که من میدونم روی 90 میلیارد توکن یعنی یه دیتا با حجم 500 گیگ ترین شده ظاهرا به تازگی اوپن سورسش هم کردن اسم مدلش توکا هست و تو خیلی کارا ظاهرا استفاده کردن و تو خیلی سرویسا بردنش هدفشونم اینه که یه مدل با ۱۳ میلیارد پارامتر رو به اسم درنا جایگزین این کنن لینک دسترسی به مدلش توی هاگینگ فیس هم گذاشتنش
-> به احترام نامگذاری جذابشون باید کلاه از سر برداشت
-> در ادامه این مدل 350 میلیون پارامتر داره من این مدلو تست کردم و عملکرد نسبتا خوبی داشت به نظرم پیشرفت خوبیه به نظرم اگه فاین تیون بشه میشه که تو یه سری موارد استفادش کرد به زودی اینکارو انجام میدم و اینجا هم اطلاعاتی که فهمیدم رو به اشتراک میزارم"
"-> مدل YOLOv10 منتشر شد مقاله گیتهاب
-> ایشالا خبر مرگش هی هر روز یه یولو
-> ایشالا میگن این نسخه نانو این مدل ۱۰۰۰ فریم بر ثانیه رو ساپورت میکنه امیدواریم
-> همش همین چیزا رو میگن
-> یولو به این خوبی همه چی هم داره میدونید چند نفر با yolotrain و yolopredict توی بیو پروفایلشون زدن computer vision engineer اگه بمیره تعداد زیادی از انجینر هامون رو از دست میدیم
-> 
-> یاد خودم افتادم روز اول که یولو فاین تیون کردم همچنین حسی داشتم
-> تقطیع تشخیص دسته بندی ردیابی فقط با یه خط کد اصلا لازم به علم دیگه ای ام نیست جواب خوبی هم میده خدایی
-> حالا من معتقدم کسی که بتونه مسله ای را حل کند مهندس محسوب میشه ایرادی هم نداره از ابزار استفاده کنه اتفاقا این خیلی با ارزش که برای حل چالش ها چالش ها رو با ابزار ها مچ کنیم و جواب بگیریم
-> بله هرکس دیدگاهی داره استفاده کردن از ابزار ها خوبه نه اینکه ما فقط ابزار هارو بشناسیم به نظر من هرچیزی که بر پایه تئوری نباشه تقلیدی بی ارزش و هست
-> نمیدونم اما اگه قرار باشه تئوری کل ابزار ها رو الگوریتم ها را بشناسیم فرصت باقی نمیمونه نظر من اینکه دانش اولیه اون ابزار کافیه مثلا یولو در برابر چه مسله ای روباست است یا چگونه برای مسله تشخیص اشیا برچسب گذاری درست انجام دهیم یا مثلا موقع کمبود تصویر چگونه داده افزایی کنیم حالا ابزارش مهم نیست بنظرم دانش اولیه مهمهه
-> حس میکنم شما کار تحقیقاتی انجام میدید
-> خب این کار هارو میشه با ۲ روز توضیح دادن به یه مهندس نرم افزار نسخه ی خیلی قوی ترش رو اجرا کرد چرا اصلا کسی که میخاد بره توی دنیای ویژن ۲ یا ۳ سال روزانه چند ساعت وقت بذاره
-> مهندس نرم افزار چالش هارو میتونه بشناسه با ابزار ها کار کرده من منظورم فقط گیت کلون کردن نبود اینکه بدونی چیو کجا استفاده کنی خودش خیلی دانش و تجربه میخواد
-> اتفاقا برنامه نویس ها به صورت خیلی قوی میتونن چالش هارو شناسایی کنند و حل کنند اصلا کار یه برنامه نویس حل کردن چالشه من هم گفتم اگه دو روز بهش توضیح بدن خب
-> جمله آخر شما منو یاد یکی از پکیج فروش های برنامه نویسی انداخت مم تاجایی که من دیدم و میدانم مهندس ها چالش هارا شناسایی راهکار پیشنهاد میدن و برنامه نویس ها صرفا پیاده سازی میکنن
-> پس برنامه نویس ها قدرت حل مساله و شناسایی مسائل رو ندارن البته مقایسه کردن من با آقای مم ناعادلانس"
"-> ممنونم آقای دکتر باعث دلگرمی شدید
-> کمی از نوشتههاتون رو خوندم خوب مینویسید نگارشتون هم خوبه
-> خوشحال شدم آقای دکتر ممنونم از توجهتون"
"-> سلام دوستان من دارم یک شبکه کلسیفیکیشن طراحی میکنم هرکاری میکنم از سایز ورودی لایه mlp ایراد میگیره چطوری باید سایز لایه ورودی mlp رو حساب کنم که این ارور برطرف بشه
-> سایز ورودی mlp میشه تعداد فیچرها یا MN M تعداد فیچیرها N تعداد سمپلها
-> من دقیقا همین مقدار رو گذاشتم ولی بازم ارور میده
-> این عکس خودش داره میگه خطا کجاست
-> ارور که نشون میده مشکل از کانولوشن هست نه mlp انتظار ورودی با 3 کانال داشته بهش ورودی با 16 کانال داده شده
-> سلام وقت بخیر استاد درباره دوره اموزش opencv ک گذاشتید میخاستم بپرسم وارد مباحث دوربینو هندسه تصویرو اینام میشید
-> سلام بله ایشالا"
"-> دوستان سلام وقتتون بخیر یه سوالی داشتم من یه کدی رو حدود 5 ماه پیش در کولب با شبکه ی عصبی نوشتم قشنگ همون موقع کد کامل ران و خروجی داد الان بعد 5 ماه همون کد ارور میده کسی می دونه علتش چیه و چه راهکاری داره
-> سلام بررسی کنید ایا میتونه بخاطر پکیج های لازم باشه
-> نه یه اروری داخل خود شبکه کانولوشنی میده در صورتی که قبلا اصلا یه همچین چیزی نبود و شبکه راحت اجرا می شد"
"-> سلام من از وقتی با شبکههای عمیق آشنا شدم بقیه کارهای قبلی و سنتی تو این حوزه یه جورایی از چشمم افتادن و برام بیارزش شدن مثل پردازش تصویر و یادگیری ماشین و برای همین حس میکنم که ارزش ندارن آدم وقتشون رو رو ی اونها بذاره و عمیق باهاشون آشنا بشه و بیشتر ارزش داره هی روی همون انواع معماریهای شبکه عمیق تمرکز کرد خوشحال میشم طرز فکرم رو نقد کنین و با خیال راحت روی سنتیها هم وقتم رو بذارم
-> من الان مشغول هفته ششم دوره OpenCV از پردازش تصویر تا بینایی کلاسیک هستم سوال شما با کار من 100 مچ هست به نظرت تصویری که پیوست کردم چی هست اگه میدونی چی هست به نظرت به چه دردی میخوره
-> تصویر مربوط به RGB که بیانگر سه رنگ سبز و قرمز و آبیه و برای هر پیکسل از تصویر سه تا عدد ذخیره میشه که معلوم بشه چقدر از هر کدوم رو داره HSI رو شک دارم ولی به نظرم H یه مدار ۳۶۰ درجه است که نشون میده مربوط به کدوم رنگه مثلا قرمز ۰ درجه آبی ۲۴۰ درجه و عددها رو همینجوری گفتم S مقدار اشباع I هم مقدار روشنایی حالا به جای RGB میتونیم با سه تا عدد HSI نشون بدیم کاربردش مثلا اگه بخوایم تصویر رو فیلتر کنیم میتونیم تصویرو به فضای HSI ببریم و فقط مثلا روی I فیلتر کنیم سوالی که برام هست هر کدوم از این کارهارو پیشپردازش یا هرکار دیگهای رو از نظر تئوری فک میکنم خود شبکه بتونه انجام بده
-> فرض رو بر این بذاریم که شبکه همه کارهای کلاسیک رو میتونه انجام بده حالا یک سوال چند وقت پیش گفتن که مردم میتونن جت بخرن حالا اگه جت خریدیم سوپرمارکت رو هم با جت میریم مثلا سر کار رو هم با جت میریم حالا الان شما شاید حاضر باشید که دیتکشن و سگمنتیشن رو یاد بگیرید ولی VLMهای امروزی میتونن حتی دیتکشن و سگمنتیشن هم انجام بدن خب پس یک VLM برداریم و دیگه هیچ نیازی به یاد گرفتن هیچ تسک و هیچ شبکه و متد دیگهای نیست فضای کاری تجاری اینطور نیست چیزی که با رگرسیون لجستیک جواب میده رو با LLM انجام نمیدن فقط همین هم نیست ببینید هنوز کورسهای پردازش تصویر و بینایی کامپیوتر کلاسیک در دانشگاههای خارج از کشور همچنان برگزار میشه نمونش به پیج کتاب ریچارد شلیسکی نگاه بندازید
-> ممنون برای توضیحاتتون"
"-> سلام از دیروز تا الان هر نوت بوکی که داخل گیت هاب باز میکنم با این تصویر روبرو میشم کسی میدونه داستان چیه مشکل از کجاست اول فکر کردم بخاطر اینترنت و یا ip هست ولی متوجه شدم با بقیه دستگاه ها و شبکه ها هم همین اتفاق میوفتد بعد تصورم این بود که شاید فایل های خودم خراب شدند که بعد با چک کردن نوت بوک های افراد دیگه متوجه شدم مثله اینکه همه نوت بوک ها این داستان پیش میاد
-> من هم این مشکل رو امروز تجربه کردم کلا گیتهاب این نوتبوکها رو بد باز میکرد و نیاز بود که یکی چندباری رفرش کنی ولی الان دیگه اصلا کار نمیکنه
-> 
-> خب الان ما باید کار خاصی انجام بدیم یا اینکه خود گیت هاب مشکل رو برطرف میکنه
-> سلام فک میکنم از نت هست با یک نت دیگه چک کنید
-> با تمام اپراتور های شبکه و همینطور فیلترشکن تست کردم حتی با دستگاه های مختلف هم چک کردم تا اخر روز جمعه همه چیز خوب بود
-> لینک
-> تمام نوت بوک ها این اتفاق میوفته مثلا
-> دانلود میشه
-> بله دانلود میشه حتی با nbviewer هم قابل مشاهدست ولی مشکل اینجاست که وقتی کنار رزومه گیت هاب میفرستیم ممکنه HR تا این حد پیش نره از طرفی کد جدید رو توی لینکدین پست کرده بودم اینکه کد باز نشه و با ارور مواجه بشن اتفاق جالبی نیست
-> عاها این ایشیوش باز شده دارن فیکس میکنن اره با nbv میشه دید
-> توی مدیوم هم خروجی کاراتو بزار ولی hr ها برای دعوت مصاحبه منظورت هست خیلی چک نمیکنند اعتمادی ب کد ندارن با دو تا سوال و مصاحبه انلاین هندل میکنن
-> که اینطور خیلی ممنونم از توضیحاتتون"
"-> دوستان سلام جدی عرض میکنم میخام پروژه ی تحقیقاتی درباره رابطه پمپ بنزین و متغیر های اقتصادی در ایران انجام بدم بنظرم خیلی شاخص مهمی هست مثلا همین الان که خبر حادثه بالگرد اتفاق افتاده گفته میشه پمپ بنزین های تبریز صف ایجاد شده اگر کسی تمایل داره خوشحال میشم
-> میخواید به چی برسید
-> خیلی خوش خیالی"
"-> سلام وقت همگی بخیر دوستان هر معادله ای رو میتونیم مجهولاتش رو با شبکه عصبی به دست بیاریم مثلا معادله ی AXAT X Q X مجهول A Q معلوم
-> میتونید از الگوریتم ژنتیک استفاده کنید خیلی زود به نقطه بهینه میرسه و مجهولات رو پیدا میکنه
-> سلام وقتتون بخیر شما با نحوه کارکرد الگوریتم ژنتیک برای انتخاب باند تصاویر hsi یا msi اشنایت دارید
-> سلام نه متاسفانه من اطلاعاتی ندارم تو این زمینه"
"-> سلام یه سوال داشتم اگه بخوایم یه سیستم حضور غیاب با شناسایی چهره بزنم از چه نوع شبکه cnn ای باید استفاده کنم که بشه بعد از اموزش اصلی بازم روی تصاویر جدید بدون نیاز به دیتا های اولیه اموزشش داد مثلا فرض کنید یه مدل کلسیفایر روی تصویر چهره ۱۰۰۰ نفر train کردیم خب حالا میخوام الان کلا این ۱۰۰۰ تا کلاس رو از لایه اخر شبکه حذف کنم روی چهره دوتا کارمند ترینش کنم بعد دوباره روز بعدی روی چهره کارمند سوم این شدنیه یا اصلا نباید از مدل های کلسیفایر استفاده کرد نمیخوام از شبکه های سیامی استفاده کنم چون موقعی که یه نفر بخواد حضورشو ثبت کنه باید چهرش با چهره تمام کاربرای توی شبکه یک بار مقایسه بشه
-> این open set problem هست دیگه مگه توی کلاسیفایر مقایسه انجام نمیشه همون مقایسهای که توی کلاسیفایر دارید توی اوپن ست هم دارید اینها فقط توی train کردن این دو روش باهم فرق دارن
-> ممنون از پاسختون چیزی که ذهنمو درگیر کرده اینه که این کار اصولیه که از یک شبکه سیامی استفاده کنم و وقتی کاربر صورتش رو جلوی دوربین گرفت عکس صورتش رو با صورت تک تک کارمندها مقایسه کنم و فاصله اقلیدوسیشون رو حساب کنم تا تشخیص بدم که این کاربر اصلا وجود داره یا نه و اگه وجود داره حضورش رو بزنم یا روش بهتری هم وجود داره
-> گفتم دیگه به این گفته open set problem اتفاقا اصل راه حلش مقایسه پروب و گالری هست اون راه کلاسیفایر غیراصولی هست نکته بعدی این هست که شبکه سایامیز صرفا توی ترین هست وگرنه توی اینفرنس که شبکه سایامیز نداریم صرفا یک شبکه هست که در خروجی یک بردار ویژگی میده
-> درسته ولی برای مقایسه توی مرحله استفاده از شبکه باید تصاویر رو به شبکه بدیم تا بتونیم بردار ویژگیشون رو بگیریم و فاصله تصاویر رو حساب کنیم دیگه
-> توی کلاسیفایر مگه این کار رو نمیکنید
-> نه دیگه من از کلاسیفایر استفاده نکردم فقط یه شبکه Siamese ترین کردم که بردار ویژگی عکس هارو بهم بده و بعد فاصله اقلیدوسیشون رو حساب کردم تا ببیم برای یه نفر هست یا نه شما میگید که باید دوتا شبکه داشته باشم یکی Siamese و یکی کلاسیفایر اگه منظورتون اینه میشه بفرمایید که چطور باید ازشون استفاده کنم ممنون
-> 
-> استاد من خیلی خیلی ممنونم از شما واقعا دیدم باز شد و تازه متوجه شدم این موضوع رو که تفاوتی ندارند این دوتا روش از لحاظ محاسباتی و فقط تفاوتشون توی اضافه کردن کلاس های جدیده که توی close set این موضوع بسیار سخته و از صحبت های شما متوجه شدم که باید بردار ویژگی هر یوزر را ذخیره کنم و بعدا توی چک کردن ازش استفاده کنم نه اینکه هر دفعه تصویر تمام کاربر هارو از شبکه رد کنم واقعا ممنونم ازتون که وقت گذاشتید
-> سلام آقای دکتر شبتون بخیر مشاوره ساعتی هم انجام میدید برای پروژهها"
"-> سلام با توجه به برگزاری نمایشگاه کتاب تهران میشه لطف کنید و چند کتاب درباره یادگیری عمیق و بینایی کامپیوتر معرفی کنید تا قبل از پایان نمایشگاه خریداری کنم ممنون از شما
-> سلام راستش کتاب زیاد هست ولی فکر کنم خیلیاشون توی ایران موجود نیستن کتابهای نمایشگاه رو ببینید و بعد اسشون رو بگید شاید بهتر بشه تصمیم گرفت درهرصورت موارد زیر برای من مرجع همیشگی بوده کتاب پردازش تصویر گونزالز کتاب Hands on ML اوریلی کتاب ISL کتاب ESL کتاب یادگیری عمیق گودفلو کتاب بینایی ریچارد شلیسکی کتاب Machine Learning Refinrd"
"-> import os osenvironPYTORCH_CUDA_ALLOC_CONF expandable_segmentsTrue توی قسمت مدیریت حافظه پایتورچ استفاده اش کردم و تونستم مدل و بچ سایز رو یه مقدار بزرگتر کنم بدون اینکه به ارور cuda out of memory بخورم در حالی که قبلشم از نهایت توان داشتم استفاده میکردم
-> اون بخش رزرو شده رو استفاده میکنه
-> آره چیزی که خوندم همین بود البته تاثیرشم خیلی زیاده انگار نصف حجم گرافیک رو دوباره بهت میده"
"-> سلام توکنایزر فارسی چی پیشنهاد میدین
-> والا هضم که خیلی خوب و معروفه و من هم راضی بودم ولی فکر کنم این دو سه سال اخیر بازهم موارد خوب اومده که حضور ذهن ندارم
-> منم از هضم استفاده میکردم اما میخام u200c رو جدا کنه قبل و بعدش رو توی توکن های مختلف اما اینجا نمیکنه باید دستی بزنم
-> هزار و هضم
-> ما در بحث پیش پردازش متن بحثی تحت عنوان lemmatization داریم کلا ما فقط توکنایز نمیکنیم خیلی اوقات لازم هست که یکسری کارهای پیش پردازشی علاوه بر توکنایز انجام بدید این نیم فاصله جداکردن هم به همین شکل هست نیاز به یکسری مراحل دیگه دارید البته این دیدگاه من هست شاید واقعا یک توکنایزر بتونه جدا کنه ولی دیدگاه من این هست که توکنایزرها معمولا معنایی توکنایز نمیکنن بلکه صرفا به خاطر آرایش و شکل کلمات کار میکنن برای کارهای مبتنی بر معنایی شما باید به خاطر پردازش بیشتر هزینه بیشتری بدید که معمولا بحث lemmatization شامل پردازش معنایی میشه
-> سلام استاد خسته نباشید وقتتون بخیر ببخشید مزاحمتون شدم استاد ببخشید ی سؤال من الان دارم روی ی مقاله ای کار میکنم و نتایجش رو بهبود دادم الان میخوام ی گزارش علمی ای بنویسم میخوام نمودار های لرنینگ کرو رو بیارم مقادیر accuracy خوبه و قابل انتظاره و مشکلی هم نداره ولی مقادیر لاس اینطوری نیست ی جاهایی اصلا nan هست لاس علی رغم اینکه خیلی سعی کردم اینو بهبود بدم نتونستم و توی مقاله ای هم که روش کار میکنم هیچ حرفی از لاس زده نشده فقط همون دقت رو عنوان کردند میخواستم نظرتون رو بدونم که آیا نمودار های لاس رو بیارم تو گزارش خودم یا بنظرتون نیازی نیست
-> توی مقاله که لاس رو گزارش نمیدن توی گزارش علمی رو نمیدونم نمیدونم کی قراره گزارش شما رو ببینه اما اینکه نن دارید به نظرم مشکل بزرگی محسوب میشه باید دلیلش رو پیدا کنید
-> اهان چشم خیلی ممنون
-> من معمولا توی کارها میام این عبارت رو با اسپیس جایگزین میکنم مثلا Textreplaceu200c البته نمی دونم چقدر اینکارم اصولیه
-> 
-> وقتی از پایتون برای این کارها استفاده کنید بهتره چون هم بهینه تر از سیستم استفاده کردید و هم از بیلت این های پایتون استفاده کردید که سرعتتون رو زیاد میکنه توی پردازش هم کنترل دقیق تر و بیشتری دارید برای این کارهای ساده حتما گزینه اولتون pure python باشه البته این به تعداد داده ها و حجم داده هاتون هم مربوط میشه که اگر از لایبرری خاصی استفاده نکنید مثل هضم بهتره صرفا برای لول کارهای ساده و نسبتا متوسط یا جایی که سرعت براتون ملاکه
-> اینو البته من در حال گذراندن یه دوره NLP هستم جدید در گروه اپ شده و قابلیتهاش رو نمیدونم اگر کسی حرفه ای NLP الان کار میکنه میتونه تستش کنه
-> ببخشید اما من متوجه نشدم چی گفتین
-> سلام مجدد ببینید اصولا کار روی متن حتی در سمت پیشپردازشش کار محاسباتی سنگینی حساب میشه وقتی ما از کتابخانه های دیگه استفاده میکنیم درواقع یه سطح از زبان پایتون اومدیم بالامثال سادهای هستش ولی دقیق نه هر چی از زبان پایتون فاصله بگیریم محاسبات کندتر خواهد شد البته این برای همهی کتاب خانه ها صدق نمیکنه برای مثال نامپای سرعت حدود ۱۰ برابری بهتون میده اون هم به خاطر نوع خاص داده ساختاری هستش که ایجاد کردن ولی بالاخره در اینجور کتابخانه ها مثل هضم که آپتیمایز بودنش صد در صد تایید نشده ابزارهایی رو ازش استفاده کنید که میدونید کار سختی رو قرار انجام بده مثلا اون جدا کردن قبل و بعد u200 که گفتید یه کار سادهای هستش بهتره با استفاده از توپل ها مثلا انجام بشه که سرعت بیشتری هم داشته باشه همون دستی که گفتید منظورم اینه ببینید اگر کتابخانه کار سختی انجام میده و آپتیمایز هستش مثل پایتورچ نهایت استفاده رو ازش ببرید با توجه به کارتون اگر کار کاره ساده ای هستش از هضم استفاده نکنید تا سرعتتون بیاد پایین فرض من این بود که دادههای حجیم و زیادی دارید موفق باشید"
"-> دوستان سلام من یه مدل دارم که وقتی توی کولب اجراش میکنم موقع پردیکت کردن بخاطر تعداد زیاد نمونه ها RAM پر میشه و سشن ری استارت میکنه راهش چیه به نظرتون
-> همه نمونه هارو یه جا نده بهش و دسته دسته لود کن توی رم و خروجی بگیر"
"-> درود امیدوارم حالتون خوب باشه من یه سئوال در مورد فصل هفتم شبکه ترنسفومر داشتم ممنون میشم محبت کنید و پاسخ بدهید سئوالم در مورد استفاده و استفاده نکردن از positional encoding هستش که شما توی این فصل در موردش توضیح دادید فک کنم اگه با مثال توضیح بدم بهتر منظورم رو میرسونم و شما واضح تر متوجه سئوال من میشید من یه ماتریس snapshot دارم که بعدش 10006 هستش یعنی 6 تا متغییر مستقل دارم که توی 1000 گام زمانی ذخیره شون کردم حالا اومدم و از یک encoderonly Transformer network استفاده کردم و 80 درصد دیتا رو به عنوان train گرفتم وقتی از positional encoding استفاده نمیکنم جواب ها خیلی خوب هستش ولی وقتی میام و positional encoding استفاده میکنم جوابم هام قابل قبول نیستند خواستم بدونم وقتی ما درگیر مسائل تبدیل هستیم مثل ترجمه ماشینی صوت به تکست متن به آهنگ و استفاده از positional encoding اجباری هستش درسته ولی باری مسائلی که درگیر مسائل تبدیل نیستم برای مثال time series prediction نیازی به positional encodingنیست اگه برداشتی که من دارم درست نیست میشه محبت کنید و احتمالا اگه دلیلش رو میدونید محبت کنید وبگید دلیل اینکه بعد از اعمال positional encoding جواب هام خراب می شن چی میتونه باشه
-> من این تست رو یه کار دیگه ای قبلا داشتم به این نتیچه رسیدم پوریشنال انکودینگ بسته به نوع دیتا و جایی که واقعا ترتیب دیتاها دستخوش جابجایی نمیشه و real time هست فقط complexity کد رو بالاتر میبره و بهبود انچنانی حداقل در سری زمانی به من ندادحالا برای کسایی که NLPکار میکنن ممکنه نتیجه بهبودی بهتر باشه و توصیه شده بود ولی من روی NLP ش کار نکردم
-> ممنون از پاسخ گویی تون"
"-> سلام روزتون بخیر من در مورد نصب opencv با مشکل مواجه شدم ممنون میشم راهنمایی بفرمایید با آناکوندا کتابخانههای مربوطه رو نصب کردم ولی در import کردن با ارور DLL load failed while importing cv2 The specified module could not be found مواجه میشم با دستور conda list در anaconda prompt چک کردم نصب شده بودند conda رو هم آپدیت کردم و ورژن پایتون رو هم چک کردم که 31014 بود ویندوزم 11 هست و آپدیته numpy و matplotlib هم نصبند
-> اگه سوال مربوط به دوره اوپن سی وی میشه اونجا مطرح کنید
-> من همچین مشکلی رو داشتم اگر سیستم عالمتون ویندوز هست نوع ویندوزتون رو چک کنید اگر نسخه ویندوز سری N دارید به احتمال زیاد مشکل از همون هست برای رفع مشکل پکیج Windows Media Package رو از قسمت add optional features در apps features تنظیمات ویندوز دانلود و نصب کنید مشکل حل میشه
-> ممنونم که وقت گذاشتید نگاه کردم ویندوز من از سری H هست از طریق pip install نصبش کردم و برای خودم جالب بود که به راحتی نصب شد"
"-> سلام ببخشید دوستان راهی هست که در پایچارم نمودار اتلاف آموزش شبکه و بصورت پویا و همزمان با پیشرفت آموزش مشاهده بشه
-> سلام حالا خود پایچارم که مطمئن نیستم ولی تنسوربورد گزینه خوبیه به نظرم
-> سلام بله ابزار mlflow"
"-> سلام ببخشید دوره دیپ کاتالیست ب پایان رسیده
-> سلام اگر اشتباه نکنم یه پروژه دیگه ازش مونده
-> بله تموم شده 5 1 پروژه بود که کامل منتشر کردیم آخر دوره یک پروژه هدیه معرفی کردیم که بچهها انجامش دادن ولی هنوز ویدئوش رو منتشر نکردیم
-> متشکرم"
"-> استاد این لینکی که برای kan gpt گذاشتید و اومده که لایه های mlp رو با kan جابه جا کرده توی نمودار هاش زیاد تفاوتی نداااره و حتی انگار یکم ضعیف تر هست و یه چیز دیگه ما توی مدل زبانی دیپ کاتالیست اگه اشتباه نکنم میومدیم اکسپوننشیال لاس رو پرپلکسیتی حساب میکردیم ولی اینجا اومده 2 رو به توان لاس کرده به عنوان پرپلکسیتی
-> بنظرم نکته مهمش توی activation function هاست که متفاوته و مثل mlp یکی نیست
-> با اون فرمول هم میشه حساب کرد دقت بهتر تنها معیار ما نیست این شبکه حجم پارامترهاش نسبت به شبکه مبتنی بر mlp کمتر هست پس پارامترها سرعت و خیلی فاکتور دیگه در کنار دقت مطرح هست
-> اصل ماجرا همینه دیگه دیگه وزن نداریم بجاش ورودی میره داخل تابع فعال سازی ک مدام در حال یادگیری هست و خروجی متفاوتی را بیرون میده کدهاش که خیلی افتضاحه ولی مقاله بسیار جالبی هست هرچند که خیلی مشکلات داره ایده نوپای هست
-> اگه 2 به جای e حساب کنیم چیزی که توی دیپ کاتالیست به دست اوردیم اطراف 20 میشه
-> نمیدانم"
"-> دوستان لطف میکنید اگر این کتاب رو دارید یا منبعش دانلودش رو میشناسید برای من بفرستید من سر دانلود کتاب خیلی مشکل دارم چجوری یسری دوستان راحت دانلود میکنن Mastering NLP from Foundations to LLMs
-> توی سایت libgenis هستش ولی نسخه epub اسم شو سرچ بکنین سریع میاد بالا سایت ایبوک تو بوک هم جدیدا نسخه چاپی شو می فروشه ولی ازشون نپرسیدم بر اساس pdf هست یا epub
-> "
"-> یعنی من الان توی وضعیم که پایچارم سه گیگ رم میگیره برای ران شدن بعد مدل کد پایچارم یه گیگم بگیره فقط ۲۴ مگابایت برای مدلم باقی میمونه بعد آقای دکتر میگه ۱۰۰ میلیون که کمه
-> حساب و کتاب در حد مگابایت یاد یکسری خاطرات توی محیط کار افتادم دوستان تا حالا با LLMها چه کارهایی انجام دادید
-> این محدودیت ها و حساب کتاب ها باعث میشه بری سراغ ترکیب تکنیک های نالج دیستیلیشن و کوانتیزشن و به جای این که از لیست ها و دیکشنری ها استفاده کنم از arrayarray ها بهره ببریم در نهایت میتونیم llm ها رو روی ۴۰۰ مگابایت رم اجرا کنیم کتاب High performance python هم میتونه کمکتون کنه خیلی زیادد
-> من بیشتر تمرکزم روی fine tuning بوده و سری Llama2 Llama3 رو هرکدوم نسخه های 8 میلیاردیشون رو روی دیتاست های فارسی سوال و جواب مثل alpacapersin فاین تیون کردم که خیلی خیلی به پرفورمنس مدل به عنوان یه assistant کمک کرد با اینکه سری Llama3 به خودی خود خیلی نسبت به نسل قبل پرفورمنس تو زبان فارسی داشت یه چالش جذابیم که مطرح بود تو این کارا این بود که من سعی میکردم همه این پروسه ها رو لوکال انجام بدم از اونجا که اینجور مدل های زبانی بزرگ مثل Llama Mistaral gamma کاملا روی لوکال قابل اجرا هستن و طبق تجربه ای که به دست اوردم فهمیدم این قابلیت این مدل ها خیلی خیلی به پیشرفت مدل های اوپن سورس برای اجرا شدن روی لوکال میده یه بحثیم که کمتر توش کار کردم اما به نظرم خیلی جالبه قرار دادن همچین LLM هایی توی شبکه های RAG بود معمولا LLM ها دوتا مشکل اصلی تو جواب دادن دارن که یکیشون مشخص نبودن منبع و جواب هاشونم ممکنه خیلی اوقات دقیق نباشه این دوتا مشکل رو میشه با RAG تا حد زیادی بهبود داد و مثلا من تونستم توی چند تا مورد شخصی سازی شده استفادشون کنم و طبق تجربه میتونم بگم که خیلی عالی عمل میکنن مثلا یه مثالی که انجام دادم این بود که دیتای مربوط به ۲۰ تا کشور رو از ویکی پدیا تبدیل به پی دی اف کردم و حالا هر موقع که مدل میخواست جوابی در رابطه با یه کشور جنریت کنه با توجه به دیتای جدید جمع اوری شده که بهش موقع جنریت کردن جواب میدادم خیلی خیلی جواب بهتری نسبت به زمانی که خام ازش استفاده میکردم میداد درکل به نظرم LLM ها خیلی جای پیشرفت و استفاده تو زمینه های متفاوتی رو دارن
-> سلام وقتتون بخیر ایا جواب هایی که Llama3 به فارسی میده به اندازه جواب های gpt4 با کیفیت هست منظورم اینه که میشه در پروژه تجاری ازش استفاده کرد سوال بعد اینه که برای RAG از چه فریمورکی استفاده کردید متشکرم
-> به خاطر همین گفتم که یادآور خاطراتی برام هست ممنون
-> سلام شبتون بخیر به نظر من کیفیت جوابایی که میده خیلی خوبه در مقایسه با GPT4 فکر میکنم که بله اگه پرفورمنس خیلی خیلی قوی ای نمیخواید اره میشه روش حساب کرد به نظر من شاید بهتر باشه واسه استفاده سازمانی نسخه های بزرگتر Llama رو استفاده کنین و برای RAG هم از پایتورچ استفاده کردم
-> ممنون پس برای RAG از ابزار هایی مثل lang chain استفاده نکردید ببخشید یه سوال دیگه هم داشتم برای fine tune کردن Llama از چه gpu ای استفاده کردید و اموزش های یوتیوب برای انجام دادنش کافی هست
-> برای RAG بستگی به پیچیدگی پروژه میتونید از langchain یا llamaindex استفاده کنید آموزش های یوتیوب کافی هستند و با کولب هم میتونید فاین تیون کنید برای دیتابیس برداری هم از chroma یا pinecone
-> اگر از مدل های اپن سورس میخواهید استفاده کنید Llama3 Mixtral البته برای چت توجه داشته باشید که در langchain بخش memory را برای چت هم استفاده کنید
-> چرا خب بستگی به مسئله داره میتونین از llama indexollama llamacpplangchain هم استفاده کنین من از جی پی یو V100 استفاده کردم و بله اموزش های یوتوب کافی ان اما به نظرم دانش پیش زمینه نیازه تا حدودی"
"-> شماها چه سیستم های قوی ای دارید
-> اره نفری یه دونه a100 داریم ما"
"-> سلام آقای دکتر تست گرفتید روی سیستم خودتون
-> نه من تست نکردم
-> ممنونم آقای دکتر"
"-> یک خبر جالب اینکه Jetbrains سازنده پایچارم IDEهاش رو به مدل لوکال 100 میلیون پارامتری با Context سایز 1500 توکن مجهز کرده و این یعنی کد نوشتن با پایچارم لذتبخشتر از قبل میشه مدل هم از سختافزار شما برای اجرا استفاده میکنه و نیازی به اینترنت نیست حالا چطوری کار میکنه اگه با پایچارم کار کرده باشید میدونید که auto complete قوی داره و همین که یکی دو تا حرف از دستور رو بنویسیم سریع دستورات مرتبط رو پیشنهاد میده حالا اینطوریه که دیگه کلا دستور رو همراه با ورودی و خروجی به شما پیشنهاد میده به خاطر همین بهش میگن full line code complete وبلاگ Jetbrains برای اطلاعات بیشتر
-> من چند مدته ازش استفاده میکنم بیشتر زمان هایی که قفل میکنم نمیدونم چی بنویسم یهو میاد و یه پیشنهاد میده و منم میرم ادامه کارم رو میدم کلا پایتورچ و شبکه های مختلف رو میشناسه
-> بعد مدلش کوانتایز یا هرس شده یعنی میشه روی یه سیستم کم قدرت هم پردازش بشه و ران بشه
-> نمیدونم لپتاپ و سیستم من جفتشون جی پی یو و سی پیو خوبی دارن باید تست کنید ولی رو لپتاپم صدا فن خیلی اوقات در میادالبته نمیدونم که علتش اینه یانه
-> ممنونم لطف کردین
-> مدلش اونقدر بزرگ نیست البته پایچارم بدون این هم رم زیادی میگیره و روی سیستمهای ضعیف کمی اذیتکننده هست
-> حالا کوانتایز شده بودنش رو نمیدونم ولی به نظرم مناسب سازی شده برای ران کردن روی لوکال چون هر سیستمی که GPU نداره از طرفیم الان شما میتونین مدلایی مثل Llama3 نسخه هفت میلیاردی اش رو هم روی سیستم خودتون بدون نیاز به جی پی یو و سی پی یو قوی اجرا کنین چون مستقیما رو رم سیستم تاثیر میزاره من فکر میکنم اینم همچین حالتی باشه و اگه رم سیستمتون برای اجرای خود پایچارم و مدلش همزمان کافی باشه فکر نمیکنم اذیت کننده باشه چون مدلشم خیلی بزرگ نیست
-> آخه توی پست نوشتید ۱۰۰ میلیون پارامتر داره
-> خب کمه دیگه مدلهای زبانی الان بیلیونی هستن
-> ممنونم از توجهتون متوجه شدم
-> برای سیستم من نه
-> با ۱۶ گیگ رم میشه مدل ۸ میلیاردی هم اجرا کرد این که کلا ۱۰۰ میلیونه"
"-> فکر کنم دیگه تا ابد با اظهار نظرم از شرکت در دوره های هوسم محروم بشم
-> نه هوادار افراطی نیستم"
"-> داستان چیه
-> رفتیم فینال با امام
-> دون کارلو آنجلوتی
-> نه امام فقط خوسلو
-> گفتم شاید تبریک دومادیه و شام افتادیم استاد"
"-> جناب اقای دکتر اشرفی تبریک میگم خودتون میدونید
-> آره میدونم ولی شما از کجا میدونستی
-> توی آلیانز آرنا شل گرفتیم اینجا هم دقایق آخر شل کردیم مفت باختیم حیف گل به اون قشنگی هایپرسونیک بود دیگه این دنیا بدرد نمیخوره از وقتی یوپ هاینکس رفت دیگه این تیم گود فیت نشده مبارکتون باشه استاد"
"-> لینک فعلی هم عوض شده و هفته آینده لینک جدید رو در اختیارتون میذارم
-> لطف میکنین ممنون"
"-> دوستان سلام وقت تون به خیر با اجازه از استاد و دوست عزیزم جناب دکتر اشرفی می خواستم بگم که امروز از ساعت ۵ و نیم جلسات یادگیری تقویتی رو شروع می کنیم و در صورتی که مایل بودید در جلسات شرکت کنید ار طریق گوگل میت می تونید وارد جلسه بشید
-> سلام قرار نبود ساعت پنج و نیم باشد
-> چرا پنج و نیم هست اشتباه تایپی بود
-> سلام من چند بار از ساعت ۱۷۳۰ تلاش کردم وارد جلسه شوم ولی به من اجازه ورود داده نشد برنامه یا لینک ورود تغییر کرده
-> نه لینک همونه به هاست جلسه گفتم اکسپت کنن اگه درست نشد توی پیوی بهم پیام بدید
-> ممنونم
-> سلام ببخشید جلسه ضبط میشه
-> سلام و ارادت بله داره ضبط می شه و در اختیار همه قرار می گیره"
"-> دیتاست سایت باسلام شامل اطلاعات فروش و مشخصات 24 میلیون محصول موجود در سایت باسلام به همراه دیتاست 33 میلیونی کامنت محصولات لینک دیتاست در هاگینگ فیس لینک دیتاست در کگل دیتاست مجموعه_داده
-> خبر بسیار خوبی بود ممنون"
"-> یه مدت هم هست دسته جدیدی از مدلای Physics inspired به اسم Poisson Flow ها وارد Generative AI شدن که خیلی عملکرد خوبی کنار مدلای دیفیوژن دارن البته خیلی هم سخت هستن از نظر درک ریاضی بنظر من منابع کاربرد در CT denoising
-> این اولی دقیقا همینه حل معادلات دیفرانسیل معمولی رو با این شبکه ها انجام دادن مرسی مقالات جالبی بودند"
"-> استاد از 8 نفری که توی این مقاله جدید kan اسمشون اومدن 6 نفرشون فیزیک خوندن فیزیک اونا با فیزیک ما فرق داره
-> جالب بود راستش من درمورد فیزیک دید ندارم اما چیزی که از دبیرستان یادم هست زیرشاخههای زیادی داشت کلا فیزیک با ماشین لرنینگ ترکیب بشه جالب میشه مثلا physics informed machine learning رو سرچ کنید توی یوتوب هم یک نفر هست ویدئوهای جالبی در این زمینه میذاره خیلی مسائل با ترکیب فیزیک میتونه منجر به نتیجه بهتری بشه برداشتم این هست که در بحث تولید ویدئو هم فیزیک مهم هست
-> وقتی بحث ساختار مغز وسط بیاد کلا تفاوت بین ساختار جامع مغز و شبکه های فعلی نمایان میشه دو ماهی دارم روش تحقیق میکنم اتفاقات جالبی در یادگیری مغز وجود داره
-> این موضوع برای من هم خیلی جالبه خوشحال میشم اگه به چیز جالبی برخوردین تو گروه به اشتراک بزارین
-> حتما البته باید درستی اش بررسی بشه کتاب و ویدئوهای مختلفی رو بررسی کردم البته فقط این قضیه برام اثبات شده که مغز ارتباطات بین حافظه ای هست که توسط مرور و ترکیب انها یادگیری رخ میده
-> به ما بگو لطفا جلسه آنلاین
-> بتونم بصورت PDF در میارم
-> شاید با یه درک نسبی خوبی از نحوه کارکرد مغز به سوالای خیلی خیلی زیادی پاسخ داد شما بهتر میدونین اما به نظر من بحثایی هست که درک نحوه کارکرد مغز انسان توی اون موضوع میتونه خیلی کمک کننده باشه توی فهمیدن اون بحث به خصوص وقتی اون بحث مربوط بشه به شبکه های عصبی
-> بله پروفسور Steven Brunton از University of Washington در این زمینه خیلی فعال هستن و مرتب ویدئو هم تولید می کنن در یوتیوب
-> امیررضا آره خیلی معلم هست
-> این شبکه های physic informed هم برای حل معادلات دیفرانسیل در فیلد ریاضی کاربردی خیلی الان به کار میرن چون دیگه لزومی به گسسته سازی ندارن اتفاقا یه استاد ایرانی مازیار رئیسی این شبکه ها رو معرفی کردند مقالاتش رو اگه علاقمند بودید بخونید خیلی خوبه
-> سلام آقای دکتر این کانال یوتیوب که میفرمائید لینک ش رو میشه لطف کنید و همچنین اگر منبع دیگه ای در این زمینه میشناسین لطف کنین بفرمایید ممنون میشم
-> 
-> سپاسگزارم"
"-> سلام دوستان کسی تا حالا با سرویس های gpu دار ایرانی مثل ابرگرافیک کار کرده چجوریه ارزش داره استفاده کنیم
-> این متن بالا رو قبلا ما نوشته بودیم فردوسی بود فکر کنم محسن هم توی این گروه از تجربیاتش گفته سرچ کنید
-> سلام وقتتون بخیر بله ببینین سرویسی که میگیرین حالتای مختلفی میتونه داشته باشه حالا یا با ویندوز که وصل میشین به سرور ویندوزی مثلا از طریق remote desktop یا لینوکسی باشه که با ssh میتونین به کرنل متصل بشین یا سرور های ژوپیتر هم هست که وصل میشین به یه سرور ژوپیتر که توی وب هست به نظرم ارزش استفاده دارن اما به نظرم با کولب پرو هم مقایسه کنین
-> ممنون داخلی باکیفیت استفاده کردید چیزی که مشکلات بالا رو نداشته باشه
-> بله اون قبیل از مشکلات رو نداشت
-> سلام البته امیرکبیر هم سیستم داره حتی GPU A100 هم داره البته تست نکردم اسمش هم ابررایانه سیمرغ هستش
-> البته ابر فردوسی هم چند هفته پیش من تست کردم خیلی از این مشکلات رو حل کرده بودن"
"-> سلام و وقت بخیر مجدد شما داخل ویدیوی آخر دوره ی faster rcnn گفته بودید قرار است یک پست یا دوره ایی در رابطه با دسته بندی ژانر های موسیقی داشته باشید میخواستم بدونم این پست و یا ویدیو انجام شده چون تمام پست های وبلاگ همینطور کانال یوتیوبتون رو چک کردم ولی پیدا نکردم
-> سلام عجب حرفی زدم چرا توی اون دوره چنین چیزی گفتم ژانر موسیقی که خیلی راحته کد هم براش زیاده کدهای آماده رو دیدید اون چیزی که توی فصل صوت دوره یادگیری عمیق گفتیم و کدی که زدیم برای این پروژه کاربرد داره فقط کافی هست دیتاست رو عوض کنید"
"-> سلام وقت بخیر وقتی میخواهم که از یک مدل آماده ی پایتورچ استفاده کنم اگر معماری مدل پیچیده و زیاد باشد گاها پیدا کردن مجموعه ها و زیر مجموعه ها از روی نوشته ها خیلی سخت است و حتی گاها با زدن tab هم مرحله بعدی نمایش داده نمیشود به همین خاطر برای درک بهتر مدل ها رفتم دنبال رسم و ویژوالایز کردن مدل بعد از بررسی و سر و کله زدن با کلی کتابخانه بهترین کتابخانه ایی که توانستم ازش خروجی بگیرم torchviz و استفاده از make_dot بود ولی متاسفانه این روش علاوه بر لایه ها کلی اطلاعات اضافی دیگر مثله backpropagation نیز نمایش داده میشود که باز هم باعث شلوغ شدن میشوند به طوریکه برای رسم مدلی با سه لایه کانولوشن معمولی ده ها بلوک ترسیم میکند میخواستم ببینم کسی روشی میشناسه که بشود لایه ها و زیر لایه ها را به صورت مرتب و مشخص مشاهده کرد
-> چند سال پیش توی یک کاری همکارم از netron استفاده کرد و راضی بود خودم هم یکی دوباری از تنسوربرد استفاده کردم و خوب بود کلا من همون خروجی متنی رو آنالیز میکنم
-> ممنون تشکر"
"-> سلام دوستان با پایتون چطوری این معادله رو بنویسم که در اخر اون دستگاه تولید بشه فقط در همین حد حل نمیخوام ولی ضرب رو انجام بده بین دو ماتریس غیر هم جنس
-> ضرب ماتریسی هست دیگه دو تا ماتریس رو توی نامپای تعریف کنید و بعد با ضرب ماتریسی انجام بدید A V
-> یکیش غیر عددیه
-> پیشنهاد قبلیم خوب نبود فکر میکنم باید از sympy استفاده کنید من به ذهنم رسید که باید سیمپای باشه و رفتم از چت جی پی یک نمونه کد گرفتم و اجرا کردم و به نظر متناسب با اهداف شماست راستی لطفا به پشتیبانی پیام ندید چیزی به ذهنم برسه و وقتش رو داشته باشم حتما میگم پشتیبانی بابت دریافت پیامهای غیرضروری همیشه گلایه داره import sympy as sp Define symbolic variables x1 x2 x3 spsymbolsx1 x2 x3 Define the variable matrix X and the numeric matrix A X spMatrixx1 x2 x3 Variable matrix A spMatrix2 3 1 1 0 2 3 1 0 Numeric matrix Perform the matrix multiplication result A X Print the resulting system of equations for i in rangelenresult printfEquation i1 equation fresulti printequation
-> خیلی ممنونم بله من عذر خواهی میکنم کمی عجله کردم"
"-> سلام وقت بخیر من برای بهینه کردن پارامترها فقط optuna رو میشناسم دوستان اگه مورد مشابه بهتر سراغ دارن ممنون میشم راهنمایی کنن
-> سلام منظورتون هایپرپارامترها توی دیپ لرنینگ هست
-> بله"
"-> سلام خدمت همگی و استاد اشرفی عزیز من هدفم اینه که در وقت ازاد NLP رو شروع کنم گفتم اگر دوستانی که تو این زمینه تجربه دارن لطفا یک نقشه راه بهمراه منابع لازم مطالعاتی پیشنهاد بدن
-> سلام با اجازه از استاد اشرفی تو گروه دربارهی این موضوع یادمه چندین بار بحث و گفتگو شده و فک کنم سرچ بزنید نظرات استاد رو هم بتونید پیدا کنید"
"-> سلام دوستان کسی کدنوشته شده ردیاب kcf با ویژگی های HOG و VGG بصورت اماده دارد یا مقاله ای در این زمینه معرفی کنه
-> سلام من سال ۹۵ با متلب نوشتمش برای کاربرد خاصی لازم دارید
-> اگر لطف کنید بفرستید ممنون میشم بله برای پیاده سازی بخشی از یک مقاله
-> اواسط هفته آینده میتونم واستون بفرستم
-> مشکلی نیست خیلی لطف میکنید اگر اجازه بدید داخل پیوی جهت به یاد موندن پیام بزارم"
"-> سلام استاد من از pretained wav2vec2 برای دسته بندی صدا استفاده کردم با اینکه وزن هارو فریز میکنم و چک میکنم میگه ترینینگ false ولی وقتی شبکه شروع میکنه به ترین با فانکشن train one epoch داخل دوره این بخش رو دوباره چک میکنم میگه True و trainable میشه چکارش میتونم بکنم که ترین نشه
-> خب اهمیتی نداره بالاخره گرادیانی که براش محاسبه نمیشه"
"-> سلام من یک سوالی داشتم برای مدل هایی که اپوک ندارن میتونیم بر اساس training size بریم و learning curve رو با sklearnmodel_selectionlearning_curve رسم کنیم همونطوری که خطا رو بر حسب اپوک تحلیل میکردیم اینم میشه تحلیل کرد مثلا برای random forest regressor
-> اسپم نکنید"
"-> سلام آقای دکتر وقت تون بخیر توی جایی که بخوایم کلاسیفیکیشن دو کلاسه انجام بدیم ما میتونیم لایه آخر رو یا یک نورون بزاریم با سیگموید و یا دو نورون بزاریم با سافت مکس میخواستم ببینم این دو حالت واقعا مثل هم هستن و تو نتیجه اثر نمیزارن چون تو حالت دو نورونه بالاخره یکسری وزن های بیشتری داره و همین نمیتونه باعث افزایش قدرت یادگیری مدل بشه
-> سلام به نظرم تاثیری نداره"
"-> سلام وقت بخیر استاد روزتون مبارک ممنون از آموزش هاتون یه سوال داشتم مفهوم interpretebality به فارسی چه کلمه ای براش استفاده میکنیم توی هوش
-> سلام فکر کنم تو فارسی توصیف پذیری یا تفسیرپذیری میگن تو بحث xai
-> ممنونم ازتون"
"-> پاک کردم ولی دایمنش آخر رو حذف کرد شبکه باید مثلا 61 بشه که بتونم ترین کنم به نظرتون چی میتونم بهش کم یا اضافه کنم این مشکل حل بشه
-> نه اسکوییز فقط دیمنشن 1 رو پاک میکنه چون حاوی معنای خاصی نیست اون بعدی که 49 هست رو اگه حذف کنه کل فیچرهاتون از بین میره احتمالا باید قبل اینکه وارد فولی کانکتد کلاسیفایر بشه یک کاری کنید که به اون سایز مطلوبتون در بیاد"
"-> سلام استاد به نظرتون چرا با اینکه در لایه آخر موقع ریترن کردن دایمنشن دوم رو حذف میکنم باز هم خروجی شبکه سه تا دایمنشن داره و نمیتونم شبکه رو ترین کنم روزتون هم مبارک
-> سلام ممنون اون عدد 1 داخل اسکوییز رو پاک کنید ببینید درست میشه یا نه"
"-> سلام وقت بخیر من باب تنوع روز معلم رو به استاد اشرفی عزیز اینطور تبریک میگم که چند ماه پیش یه شرکت رزومه من رو بخاطر فرا تر از حد انتظار بودن رد کرد ممنون که دانشتون رو اینقدر عالی با ما به اشتراک میذارین که فرا تر از حد نیاز شرکتها میشیم
-> سلام ممنون نمیدونم بگم خوشحال شدم یا ناراحت شدم موفق که هستید آرزو میکنم که به اهدافی که دارید برسید"
"-> سلام آقای دکتر و مجموعه هوسم انشاالله همیشه در زمینه کاری تان پیشرو باشید ببخشید روز آموزگار رو دیر تبریک گفتم درگیر مراسم خاکسپاری عمویم بودیم بازم بهتون تبریک میگم و انشاالله حالاحالاها دوره بدید بیرون البته این روز رو هم به تمامی دوستانی که در رفع مشکل دیگران در گروه وقت میگذارن و خودشون آموزگاری هستن هم تبریک میگم
-> سلام محسن جان ممنونم از شما خیلی چیزها یاد گرفتیم خوشحالم که در گروه حضور و فعالیت دارید"
"-> سلام من هم واقعا تشکر فراوان میکنم از شما واقعا استاد فوق العاده ای هستید امیدوارم باز هم دوره های جدید تولید کنید من دوره های اموزشی زیاد دیدم ولی دوره ماشین لرنینگ و دیپ لرنینگ شما با کیفیت ترین دوره ای که تا الان دیدم
-> سلام ممنون حقیقت این حس متقابل هست ما هم از شماها فقط حمایت تشویق و احترام دیدیم تلاش کردیم کمکتون کنیم که پیشرفت کنید و البته خودمون هم ذره ذره پیشرفت کردیم و شماها خیلی در این پیشرفت نقش داشتید و دارید
-> "
"-> سلام استاد روز معلم اقاى دكتر اشرفى مبارك و هميشه موفقيت داشته باشيد يكى از بهترين استاد مايى
-> سلام ممنونم مهدی جان باعث افتخاره"
"-> سلام و عرض ادب استاد اشرفی روزتون رو تبریک میگم امیدوارم همواره سلامت و برقرار باشیدو ما بتونیم از محضرتون بهره مند شیم
-> سلام ممنون ایشالا سلامت باشید"
"-> سلام خدمت همه عزیزان جا داره امروز رو به استاد بزرگوار تبریک بگم که واقعا به معنای واقعی بهترین استادی بودید که توی این زمینه داشتم استاد دلسوز و با سواد پایدار باشید و تنتون سلامت باشه و بتونیم سالیان سال از دانش شما استقاده کنیم
-> سلام ممنونم شما همیشه نسبت به بنده لطف دارید امیدوارم همه دوستان بدقولیها و کموکاستیها رو بر ما ببخشن عمیقا ازتون بابت حمایتها تشویقها و ممنونم"
"-> سلام وقت بخیر جایی شنیدم که میگفتند برای استفاده از یادگیری جمعی باید از مدل های ساده استفاده کرد چند تا مقاله ی مرتبط با یادگیری جمعی هم که خوندم از معماری های ساده ایی در حد دو لایه کانوولوشن استفاده کرده بودند میخواستم بدونم این حرف بخاطر پایین اوردن هزینه هست و یا اینکه برای عملکرد هم بهتر است یعنی استفاده از مدل های پیچیده که قابلیت فیت و یا اورفیت بر روی داده دارند برای استفاده در یادگیری جمعی مناسب نیستند و اثر منفی دارند با توجه به اینکه مبحث یادگیری جمعی موضوع یکی از ویدیوهای دوره ی دیپ لرنینگ بود گفتم تو این گروه مطرح کنم
-> 
-> بله دقیقا Ensemble Learning متوجه شدم ممنون از توضیحتون"
"-> اگه خود اصولش رو میخواین ما توی ابزاردقیق سیستمهای اولتراسونیک داریم میتونین از نظر رفتار پالس و رفت و برگشت اکو توی کتابهای ابزاردقیق بخونین
-> ممنون"
"-> 
-> کوئرا عالیه"
"-> بچه ها سایتی میشناسید که پروژه های کوتاه با پایتون رو داشته باشه و به عنوان تمرین بشه استفاده کرد
-> سلام از این استفاده کنید میتونید روی تمرینات ساده فیلترش کنید"
"-> منظورم پیاده سازیه یک چنین مدل پراکتیکالی در صنعت واقعا چه چیزهایی قرار به کمک مدل بیان جالبه واقعا نظری دارین بگین
-> در صنعت نمیدونم اما در حوزه اکادمیک ما از تفسیرپذیری اینطوری استفاده میکنیم که مثلا با یک مدل از آمینواسید های یک پروتئین کاربردش رو پیش بینی میکنیم و از تفسیرپذیری کمک میگیریم ببینیم کدوم آمینواسید ها بیشترین تاثیر رو در دسته های کاربرد داشته
-> اینجا فکر کنم interpretability مطرحه تا explainabilityتوی توضیح پذیری باید هر جواب قابل توضیح باشه مثل اینکه بگی یه مدل عاقلی داره که هر جوابی رو براساس منطقی بهت میده که اون منطق قابل درکهمیگم فلسفیه درکشولی پیاده سازیش ظاهرا یه سری ابزار داره که هم دیتا تو پری پراسس explainable بشه هم مدل هم ولیدیشن"
"-> من چون کارم بیشتر صنعتیه این سه روز نمایشگاه هانوور پیشرفتهای زیمنس توی Industrial AI رو دنبال کردم که کلا چرخش برمبنای دیتای IoT و از edge تا cloud هست و مدام داره با همون معماری CICD بهبود پیدا میکنهیعنی قرار نیست از اول بهترین مدل و کد و ارایه شه و دایم داره بهبود میگیره ولی خیلی رفتن به سمت XAI یا explainable AI که در مورد مفهومش خوندم میدونیم مدل black box ماشین لرن پشتش مشخص نیست که بر چه مبنایی یک خروجی رو بهش میرسه ولی قراره XAI ابزارهایی رو داشته باشه که هم دیتا reasonable باشه هم مدل قابل توجیه باشه و هم نتایج به یه عبارتی نباید به نتایح غیر قابل موجه برسه درکش سخت بود برام که پیاده سازی در واقع قرار چطوری باشه اگر کسی واقعا درکش کرده قبلا ممنون میشم توضیح بدین
-> بحث سر بهترین مدل نیست برخی مواقع نیاز هست که مدل موارد جدیدی را در طول زمان یاد بگیره و اپدیت بشه این باعث ماجرای CICD میشه"
"-> دوستان یه سوال درمورد پیش پردازش تصاویر اولتراسوند داشتم چه روشی رو میتونیم برای افزایش فریم ریتاستفاده کنیم
-> واقعیتش این امر بیشتر از اینکه پردازش تصویری باشه پردازش سیگنالیه مربوط به آرایه های مبدل و البته که به مد تشکیل تصویر و نوع بیمفورمینگ برمیگرده مثلا اینکه پیچیدگی محاسبات بیمفورمر از درجه چند باشه مرتبه2 یا مرتبه3 یا یا اینکه بیمفورمر وفقی یا غیروفقی باشه همینطور عمق نفوذ مد نظر در تصویربرداری که برای تشکیل و رفت و برگشت اکوها چقدر صبر کنیم یا نوع تصویربرداری که plane wave imaging به مراتب سریعتر از synthetic aperture هست و در ultrafast و هرجا که فریم بالا بخوان همین رو انتخاب میکنند یا مد تصویربرداری A B M doppler و البته موارد جانبی دیگه ای هم مثل تعداد المانهای مبدل اثردارند بنظرم منظورتون از پیش پردازش همون بیمفورمینگش باید باشه درسته
-> اینا تو چه درسی هست برای تصویرگره
-> درس اولتراسوند پزشکی البته بیمفورمینگ رو کمتر اونجا بهش پرداخته میشه توی سیستمهای تصویرگری پزشکی هم دستگاه و هم تصویربرداری اولتراسوند معرفی میشه ولی خب کمتر و عمومی تر
-> آها منم مهندسی پزشکیم درس اولتراسوند رو نداریم ولی متاستفانه دوره ای سراغ دارین شما
-> توی زیر مجموعه های رادیولوژی کورس ارا ببینین از اواتراسوند چیزی پیدا میکنین البته این کورسها بیشتر جنبه پیرا پزشکی و پراکتیکالن"
"-> ببخشید دوره جدید قراره چی بیاد
-> پردازش تصویر رو دیدید دیگه بعد این بینایی هست تابستون
-> نه ندیدمپس ایشالا این دوره شروع می کنم تا بینایی ماشین بیاد"
"-> کانسپت آقا کانسپت رو یاد بگیرید یه چیزی هست من کمتر دیدم دوستان برن سمت یادگیری پایتون چرا واقعا این کار انجام نمیشه خوشحال میشم نظر بچه ها رو بدونم
-> برای اینکه خدا رو شکر لفت دادن از دانشنیاموختگان بود
-> چون هدف یادگیری AI براشون نیست هدف تموم کردن پروژه و پایان نامه هست خوب عجله همین میشه
-> فکر کنم دانشجوی رشتههای پزشکی بود و بحث پروژه و پایاننامه نبود راستی دوستان من نگفتم توصیه نکنید حرفم چیز دیگهای بود که مغلطه کردن مطلب جالبی دیدید لطف کنید و در گروه به اشتراک بذارید
-> "
"-> تصمیمم بر این هست که دیگه اینجا زیاد صحبت نکنم ممنون از اظهار نظر دوستانه ی شما و مدیریت هوسم
-> گفتن نظرات بد نیست اینکه بگیم من می تونم همه مدرسها رو آنالیز کنم درکش کمی سخته"
"-> که بتونن صحت گفته مدرسها رو بفهمن و بیراهه نرن
-> من backward پیش میرم هر نکته ای نفهمم بک وارد همون مبحث مطالعه میشه
-> متوجه هستم هرکس اموزشی میبینه لیست امزشهارو نگه میداره منظورم همون لینکها و سایت ها و اموزشهایی هست که یافتید
-> ریاضیات nlp بیس آمار و احتمال میخواد آیا نیازه من همه آمار و احتمال مرور کنم نه کافیه در مورد توزیع شرطی مطالعه کنید برای افراد رشته مهندسی ممکنه واضح باشه اما برای من دوره های فرادرس و مکتبخونه و ویدئوها یوتیوب در باره ی هر مطلب جدید کمک کننده بود سوال شما مشخص میکنه چی یاد بگیرید
-> می خوام مطلبی دوستانه و نقدانه بهتون بگم سعی کنیم ادعا زیاد نداشته باشیم مطالبی که شما میگید ما راهش رو رفتیم تعداد کتب ریاضیاتی که خواندم بالای ۲۰ جلده اما گفتن اینکه من تشخیص میدم کدوم مدرس در زمینه AI ریاضیات دقیقی میگه کمی منو اذیت میکنه ادعا در این زمینه سنگینه اگه من راه و رسم ریاضیات AI رو بلد باشم پس باید چند رفرنس دقیق بدم نه کلیات چون همه مباحث ریاضی در AI که ضروری نیست بازم امیدوارم به عنوان سخن دوستانه در نظر بگیرید
-> سعی کنیم ادعا زیاد نداشته باشیم این جمله آشنای تعدادی از افراد هست اینکه از دید یه مبتدی خودآموز اظهار نظر کنید در مورد طرز تدریس این طرز فکر اصلا جالب نیست
-> کماکان منتظر رفرنس برای دوستان هستم"
"-> درسته البته معمولا طبق آگهی هایی که دیدم برای خارج از کشور معمولا کار با Cloud و آشنایی با نوشتن API با فریمورک هایی مثل جنگو یا فلسک و Fast API و کار با داکر و دیتابیس و این طور چیزا رو می خوان از دیتا ساینتیست ها بعضی جاها هم نمی خوان بستگی به شرکت داره معمولا چقدر تیم بزرگی داشته باشه من خودم برای شروع یه VPS به صورت روزانه اجاره کردم با سیستم عامل Debian و یک API خیلی ساده رو با فلسک نوشتم و فایل ها رو منتقل کردم به سرور و ران گرفتم روی سرور که کم کم مهندسی نرم افزار در دنیای واقعی رو هم یاد بگیرم برای شروع واقعا می تونه کمک کننده باشه
-> بسیار عالی من این کار رو با ورک استیشن خودم و اجاره IP ثابت در خونه انجام دادم
-> عالی
-> روش کار این بود سیستم عامل اوبونتو نصب درایور انویدیا نصب فریم ورک پایتورچ نصب داکر نصب پلاگین کانتینر انویدیا جهت دسترسی به GPU در کانتینر نصب FASTAPI کد نویسی و ساخت ایمیج کد و FASTAPI ران کردن ایمیج دسترسی port forwarding به کانتینر
-> سلام ببخشین من متوجه نشد اینجا برای چه کاری میخواین مایکرو سرویس کار کنین که که نیاز به container داکر و یا حالا بالاترش کوبرنتیز بوده ممنون میشم این config رو توضیح بدین برای سرویس cloud خاصیه
-> سلام نیاز به کانتینر حتما برای کلود نیست برخی موارد افراد مختلف گروه ورژن های کتابخانه مختلف دارن بعد اگر بیان و مدل منو بخوان ران کنن دچار عدم همخوانی ورژن میشن برای رفع این مشکل میان مدل و نیازمندی هاشو کانتینری میکنن ساخت ایمیج تا بقیه افراد جهت کار و ران کردن مدل دچار اختلال نشوند البته یکی از کاربردهای کانتینر کردن هست در ادامه اگر بخوایم مدل رو برای چندین کاربر اجرا کنیم نیاز به کوبرنتیز داریم کوبر توسط قابلیت Scaling که داره می تونه طبق درخواست و بار درخواست کاربران مدل رو تکرار کنه و همچنین صحت عملکرد کانتینرهارو هم بررسی کنه البته نشد کاملتر بگم تایپی سخته
-> سلام این برام خیلی جالبه دلم میخواد مثال عملیش رو ببینمالبته اگه باید container یاد بگیرم یه بحثدیگست
-> سلام بله کار با داکر رو باید یاد بگیرید اگه بخواد تجربه عملی داشته باشید خیلی سخت نیست می تونید خودآموز شروع کنید البته ویدئو هاش در یوتیوب هست
-> سلام بله توی کورس ارا دیدمولی هنوز وقت نگذاشتمولی اگر یک فشرده خوب توی یوتیوب میشناختید برام بدید"
"-> جلسه اول بدون توضیح این فرمول میشه رفت مرحله بعد در صورتی همین فرمول پیش نیاز ریاضی داره Likelihood Maximum likelihood estimation Log likelihood Etc و اگه ندونی چقدر ریاضی و چه مبحثی نیازه گیج میشی اینکه بیس لنگوییج مدلینگ یهو مطرح بشه بدون شفاف سازی اولین جلسه سخته
-> این دوره برای شروع به نظر منم سخت بود"
"-> من یه سوالی برام پیش اومده برای موفقیت به عنوان یه دیتاساینتیست تا چه حد لازمه که مهندس نرم افزار خوبی باشید و به مفاهیمی مثل دیزاین پترنها و سیستم دیزاین مسلط باشید چون خود مهندسی نرم افزار دنیاییه که ته نداره و ترکیبش با یه فیلدی مثل دیتا و ماشینلرنینگ که سرعت آپدیتش سرساماوره یه کم ترسناک به نظر میرسه
-> ببینید نکته این هست که شما به یکسری skill برای دیتاساینس نیاز دارید اگه اینها رو داشته باشید یا حتی بخشی از اینها رو داشته باشید وارد فضای کار میشید توی این لینک کورسرا بهخوبی اینها رو معرفی کرده و تکتکشون مهم هست اون مواردی که شما گفتید توی این لیست نیست پس نیازی نیست دیتاساینتیست اونها رو داشته باشه اگه داشته باشه خوبه ولی جز ضروریها نیست برای ورود به کار نیازی نیست جونیور نیازی به اینها نداره به نظرم قسمت بزرگی از گمراهی و ابهامها مربوط به قبل کار رفتن هست وقتی پاتون به محیط کار باز بشه میشه انتظار داشت که بعدش راحتتر رشد کنید تجربه کسب کنید skillهاتون رو آگاهانه تقویت و بیشتر کنید یعنی من بفهمم کسی وارد کار شده میگم که دیگه میشه باهاش خداحافظی کرد و دیگه رسالت ما تموم شده ولی وقتی صحبت از اون skillهای کورسرا میکنیم نباید ساده ازشون بگذریم وقتی میگه data visualization و مثلا data wrangling صرفا یک کورس گذروندن نیست باید اون مبحث رو بخورید بتونید چند تا کار سبک و جالب انجام بدید و بذارید تو گیتهابتون جاهایی که مبهم هست بگید بیشتر اینجا صحبت میکنم
-> استاد چقدر زمان میخواد این همه مسلط شیم
-> ممنون از توضیحاتتون این رو بیشتر برای این مطرح کردم که اخیرا خیلی دیدم داره مطرح میشه که برای ورود به این فیلد شما باید اول یه مهندس نرمافزار قوی باشید تا مدلهاتون قابلیت دیپلوی شدن در دنیای واقعی رو داشته باشه و البته خیلی هم به اون آگهی استخدام معروف آندره کارپاتی استناد میکنن که شرط اولیه رو همین دانش وسیع مهندسی نرمافزار اعلام کرده بود
-> درسته البته ایشون طبق آگهی خودشون Deep Learning Engineer استخدام می کرد برای اتومبیل خودران و نه دیتا ساینتیست
-> درسته ولی ارجاعاتی که بهش داده میشه مساله رو به کل حوزه دیتا بسط میدن
-> بیشتر منظور دانش DevOps هستش که در حوزه ماشین لرنینگ میشه MLOps
-> بله همین mlops بیشتر منظوره
-> تا جایی که میدونم و تجربه دارم ی دید کلی و جنرال باید وجود داشته باشه کسی که دانشمند داده است بنظرم علاوه بر تحقیق و توسعه در زمینه لرنینگ باید کار دیپلوی هم بدونه بستگی داره کجا کار میکنی اما در هر حالت باید با ابزار های دیپلوی مثل داکرکوبرسی آی سی دی ام ال فلو آشنا باشه حتی اگه انقد اسکیل شرکت بزرگ باشه که وظیفه دیپلوی بر عهده دانشمند داده نباشه برای ارتباط یا تعامل با کسی ک دیپلوی میکنه باید بلد باشه اینارو بنابراین دانش خودمونو به مفاهیم دیتاساینس محدود نکنیم
-> بله دقیقا این داشتن دانش در زمینه دیپلوی موجب میشه افراد گروه بیشتر حرف هم رو بفهمن و باگها و مشکلات را درک کنن به خصوص در زمینه الکترونیک دیجیتال و برنامه نویسی این مشکل رو ما داشتیم بچه های الکترونیک حرف بچه های نرم افزار رو متوجه نمیشدند و برعکس این موجب عدم هماهنگی و درک باگ و مشکل محصول میشد
-> Mlops واقعا برای کار حرفه ای مهمه که توی دانشگاه اصلا بحث نمیشه برای کار حرفه ای واقعا لازمه و کلا معماری پایپ لاین CICD و لایف سایکل مدل ماشین لرنینگ برای کار continous خیلی مهمه منم دوست دارم بتونم یادش بگیرم تو یک فرصت"
"-> یه چیزم بگم در مورد دوره هاnlp nlu استنفورد یوتیوب سراغش نرید بیسیک چون سخته و طولانی هی باید خودتون مطلب اضافه بخونید دوره معروف Andrew ngهم کورسرا فری نیست دیدم اما مکتبخونه فک کنم دورش رو داره
-> دورهای که معرفی کردید بد نیست اما نکته مهم این هست که با 10 ساعت آموزش بعید میدونم روی یک مطلبی بشه عمیق شد ارزش کورسهای دانشگاهی مثل کورس استنفورد این هست که به شکل اصولی و درست nlp رو یاد میگیرید به نظرم خیلی تفاوته بین فردی که 10 ساعت آموزش میبینه با کسی که 50 ساعت آموزش دیده و کلی هم مجبور به یادگیری مطالب جانبی شده هردو طرف میفهمن مدلسازی زبان یعنی چی ولی این کجا و آن کجا یکی دو روز پیش من در یکی از ویسهام گفتم که اگه از مسیر آکادمیک وارد این حوزه نشدیم تا حدی امکان سعی کنیم مسیری طی بشه که نزدیک به اصول و قواعد دانشگاهی باشه
-> طرز بیان مهمه مثلا دوره استنفورد ۲۰۲۳ یهو اومده یه فرمول گفته جلسه اول بدون توصیح اضافه مجبوری خودت سرچ کنی تازه بفهمی از قضیه مارکو اومده و conditional probality اینکه مدرس بدونه تا چه حد از ریاضی بگه تا چه حد کد مهمه یا به دوره دیگه کلا کده و تیوری کم بعد میای مقاله بخونی فرمول تیوری ریاضیه همین فرمول در دوره قدیمی خیلی ساده توصیح داده شده وقتی فهمیدی حالا میشه اومد سراغ این دوره که پیشرفته تره یا بعد بری هر سرفصل رو با جزئیات بیشتر یادبگیری
-> کورس برای دانشجوهای استنفورد هست طبیعتا انتظار مدرسش این هست که لایکلیهود رو دیگه بدونن ضعفهامون رو به حساب خودمون بنویسیم و همچنین به دیگران اینطوری توصیه نکنیم هرطور صلاح میدونید برای خودتون تصمیم بگیرید اما در پیشنهاد به دیگران احتیاط کنید من هم توی توصیههام گفتم این کورس خوبه اما ممکن هست برای شما مشکل باشه
-> بله من دانشجوی استنفورد نیستم likelihood هم میدونم اما قضیه مارکو رو نه به نظرم اینکه از دید یک مبتدی به تدریس نگاه بشه مهمه بیس ریاضیم رو از اول یادگیری ai خودم ساختم و میدونم کدوم مدرس داره بیراه میگه زائد میگه کلی میگه کاربردی میگه همین دید جریت اظهار نظر و توصیه رو از دانشجوها گرفته چرا توصیه نداشته باشیم افراد بسیاری مثل من هستند که از بک گراوند غیر مهندسی و غیر کامیپوتر ساینس وارد این فیلد میشن چرا نگیم فرمول ریاضی اتنشن رو یه کانال خوب گفته که ویو ملیونی گرفته با اینکه بسیاری مدرسها نتونستن مفهوم رو منتقل کنند کافیه کامنتها رو بخونید در کانال statquest یا 3blue1brown اتفافا به اشتراک تجربیات بسیار معتقدم مهم نیست مبتدی هستید یا خیر کماکان که تجربیات خودآموزیم رو در کانالهای زبان به اشتراک گذاشتم از روز صفر یادگیری Lets learn together
-> statquest فوق العادس
-> بسیار عالی رفرنس تون در مورد مباحث ریاضیاتی AI معرفی کنید برای استفاده دوستان
-> در چه زمینه ای"
"-> سلام آقای دکتر امکان داره بفرمایید که دوره NLP تقریبا کی آماده میشه که برنامه ریزی کنم اگه دیر ارائه میشه یه پلن دیگه براش بچینم شما خودتون کدوم دوره رو پیشنهاد میدید برای NLP ترجیحا فارسی اگرم دوره خوب فارسی نبود انگلیسی هممشکلی نیس چون موضوع تحقیقاتیم رو میخوام تو حوزه NLP انتخاب کنم گفتم یه دوره ازش ببینم دیدم بازتر بشه نسبت به موضوعات و روش ها و اگرم مقاله و کتاب به درد بخور و مناسبی بود ممنون میشم معرفی کنید سوالا زیاد شد ببخشید
-> 
-> ممنون ازتون بابت توضیحاتتون استاد منظورتون اینا بودن واسه دیپ nlp واسه دیپ ویژن
-> دیپ nlp یکی از دوستام گفت خیلی سنگینه و لازمش اینه که قبلش یه دوره کلاسیک رو بگذرونی تا بتونی باهاش پیش بری
-> بله
-> میتونید دو سه فصل اول همون کتاب رو بخونید ما یک فصل nlp توی دیپ لرنینگ داریم کسی که اون رو بفهمه برای این دوره استنفورد آمادگی داره یکسری چیزها رو هم خوب بلدید ازجمله نحوه ترین شدن شبکه عصبی بازگشتی و ترنسفورمر به نظرم میتونید ولی باید زیاد وقت بذارید"
"-> سلام استاد وقت بخیر خسته نباشید یه سوالی داشتم میخواستم بدونم دوره بینایی کامپیوتر مدرنی که قصد دارید برگذار کنید بعد از opencv مشخص نیست حدودا کی قرار هست برگذار بشه همین دوره کلاسیک مثلا حدودا مشخص نیست چقدر طول میکشه
-> سلام برنامه ما این هست که تا آخر تیر این دوره ببندیم و بعدش دوره بینایی مدرن رو شروع کنیم البته ممکن هست این زمان دیرتر هم بشه
-> خیلی هم عالی ممنون از شما فقط یه چیزی که بالاتر دوستان هم اشاره کردن درمورد بررسی پیپر ها داخل یوتوب به نظر منم خیلی کار جالب و خفنی میشه مخصوصا اگر پروژه های ترند و جدید تر باشه مثلا کنفرانس های اخیر که برگذار شدن خیلی عالی میشه اگر همچین برنامه ای هم اجرایی بشه
-> آره خودم هم دوست دارم"
"-> مدل های بزرگ زبانی جذابه ها آقای دکتر در حد همون کانسپت البته با شناختی که دارم احتمالا برای اون مایلید کورس طراحی کنید تا توی یوتیوب آپلود کنید
-> کورس فعلا وقت خالی نیست ولی یوتوب براش ویدئو میذاریم الان دارم روی یک ویدئوی معرفی LLM کار میکنم ممکنه ویدئوی بعدی همین باشه"
"-> میشه فقط pythorch یاد گرفت و احتیاجی به تنسور و opencv نداشت
-> با اجازه قبل از اینکه استاد اشرفی پاسخی بدن من یه جواب کوتاه بهتون میدم به نظر من این نمیتونه همیشگی باشه اگه منظورتون از تنسور تنسورفلو هست بستگی داره که روی چه مسئله ای دارین کار میکنین چون تو بعضی موارد ممکنه واقعا نیاز بشه که با تنسورفلو پروژه رو انجام داد در مورد OpenCV هم من به شخصه میتونم اینطوری بگم که خیلی تو بینایی کامپیوتر بکار میاد و یادگرفتنش واقعا مفیده اما بازم درکل جواب سوابی که پرسیدین بستگی به نوع فعالیتتون داره
-> سلام در برخی موارد باید تصویر رو قبل از ورود به شبکه پیش پردازش کرد مانند فیلتر کردن و کم و زیاد کردن نور و تصویر پس بهتره در بینایی ماشین opencv هم کار کرده باشید این به خصوص در کاربرد OCR خودشو نشون میده
-> به نظرم در وهله اول مشکل این هست که خیلیها برنامهنویسی بلد نیستن یا مسلط نیستن من هیچ آموزشی برای تنسورفلو 2 ندیدم و خیلی خیلی کم ازش مطالبی خوندم توی دوره دیپ کاتالیست پروژه آخر یک کد مرجع توی سایت تنسورفلو بود و 80 درصد کار ما شبیه اون بود قبل از پیادهسازی توی ویدئو گفتم که مرجع ما این هست و میخواییم با این کدها بریم جلو مدام کدهای تنسورفلو رو نشون میدادم و سعی میکردم براساس تئوری و کدهای تنسورفلو پیادهسازی به پایتورچ رو انجام بدم بدون اینکه به تنسورفلو مسلط باشم به راحتی کدها قابل فهم و توضیح بود مطمئن هستم دانشجوهای اکتیو دوره هم همین حس رو داشتن که کد تنسورفلو رو میفهمن به نظرم دلیل فهمیدن ساده بود دانش و مهارت برنامهنویسی خوب دانش خوب در مبانی یادگیری عمیق و مهارت خوب در پایتورچ همون ترنسفر لرنینگ دیگه مطالبی بلد هستید که مطالب جدید رو به راحتی یاد میگیرید بنابراین حرفم این هست که اون چیزهایی که به عنوان اصل انتخاب کردید رو خوب مسلط شید به صورت ارگانیک با اندک زمانی به موارد مشابهش مسلط میشید اگه نیاز باشه چون به نظرم تنسورفلو خیلی ضرورتی نداره الان هاگینگفیس ضروریتر از تنسورفلو هست در مورد اوپن سی وی هم خب اگه کسی میخواد توی حوزه ویژن وارد کسب و کار بشه لازم هست پردازش و بینایی کلاسیک بدونه و خب در کنارش اوپن سی وی رو هم باید یاد بگیره
-> البته تنسورفلو دو بود دیگه نه تیاف۱ خیلی نچسب بود
-> نوشتم 2"
"-> سلام اگر ممکنه کارهای مالتی تسک رو بیشتر مرور کنید در یوتیوب
-> کارهای مالتی تسک یعنی چی این یک موضوع هست"
"-> سلام علت این خطا چیه
-> بذارید روی cpu ببینید چی نشون میده خطای کلی داده
-> رو CPU که میذارم یک دفعه خیلی سریع RAM پر میشه و کرش میکنه برای همین گذاشته بودم رو GPU نمیدونم چرا انقد سریع رم پر میشه
-> خطا این بود که طبقه آخر sigmiod نذاشته بودم قبلا گفته بودین نمی خواد و خود به خود خودش محاسبه میکنه نمیدونم شاید اینو برای softmax گفته بودین و برای sigmoid صدق نمی کنه یا شاید تو آپدیت جدیدش تغییر کرده باشه
-> سعي كنيد بچ رو تكي بندازيد و تست كنيد ببينيد مشكل از كجاست"
"-> سلام وقت بخیر من یه مدل ساده ی mlp داشتم استفاده میکردم و طبق معمول بدنه ی کد ثابت و دیتا و مدل رو تعریف کردم حین اموزش در اپوک دوم با یک ارور runtime مواجه شدم که برای کد loss backward بود وقتی سرچ کردم گویا میگفت برای بار دوم که میخواهد محاسبات را انجام دهد به اطلاعات قبلی دسترسی نداره و نمیتونه لاس رو به صورت بکوارد پاس بده راه حلش هم true کردن retain graph بود با اینکه مشکلم حل شد و دیگه اروری نگرفتم و فرایند اموزش مدل به خوبی انجام شد ولی هنوز متوجه نشدم ایراد کار چی بود حتی دیتامو به یک مدل mlp که قبلا در دوره یادگیری عمیق 2022 که زده بودم دادم بدون تغییر مدل و سایر بخش ها باز هم با همچین قضیه ایی روبرو میشدم دلیل اینکه برای اینکار مجبور به هچین فرایندی شدم ولی تو پروژه های دیگه تا حالا با این مورد مشکلی نداشت چیه
-> سلام میتونید یک اسکرین شات خوب از بخش ترین شدن مدل بدید اگه کار سادهای پیاده کردید احتمالا کدتون مشکلی داره که چنین خطایی داده درواقع ممکن هست یک چیزی رو ننوشته باشید
-> بله حتما بخش ترین شدن همان تابعی هست که همیشه استفاده میکردم الان توی راهم به محض رسیدن ارسال میکنم
-> خدمت شما
-> درسته به نظرم نباید اون خطا رو بده چیزی به ذهنم نمیرسه بد نیست چک کنید که بجای lazy linear از linear استفاده کنید چی میشه اون کاربرد ratain graph یچیز دیگه هست و اینجا شما نیاز ندارید
-> بله حتما امتحان میکنم من همین کد و مدل با lazy linear رو با یک دیتای دیگر امتحان کردم همچین مشکلی بوجود نیومد
-> سلام وقت بخیر لایه ی linear ساده هم امتحان کردم ولی نتیجه یکسان بود
-> چیزی به ذهنم نمیرسه باید کد بررسی بشه فقط میتونم بگم اون خطا رو نباید بده یکجایی در کد باگ وجود داره"
"-> سلام من چند وقتی هست که فعالیتم رو توی لینکدین شروع کردم ممنون میشم یه نگاهی بندازین و نظراتتون رو بهم بگید
-> Wow As a 15 yo Geek تبریک میگم واقعا"
"-> یه پیشنهاد دیگه دارم یه پلی لیست بذاريد یه پروژه صنعتی تعریف کنید از آخر بیاید اول یعنی طرف entrylevel level با پروژه بره جلو نه یه سری توضیحات تکراری گسسته که خب خیلی جاها هست یکی هم رو تایتل hookویدئوها کار کنید مثلا ویدئویی میاد میگه بیسیک nlpدر ده ساعت در چنلsimplelearnویوی بالا گرفته این دیگه هنر کپی رایتینگه که میتونم کمک کنم دیگه اینکه رو یه موضوع خاص کار کنید دو تا پلی لیست کامل با آپلود منظم بهتر از چند تا پلی لیست ناتموم با موضوعات پراکنده هست عنوان طولانی نذارید چون هاید میشه در یوتیوب در موقع باز کردن یه پلی لیست مثلا طرف مینویسه قسمت اول پایتون _ متغیر متغیر معلوم نیست در صورتی که اصله کاور ویدئوها هم کلید واژه موضوع رو بولد بذارید مورد آخر ceo یوتیوب امسال روی ویدئوهای کوتاه تاکید کرده رقابت با تیک تاکه شاید پس مقاله تغییرات یوتیوب ۲۰۲۴ بخونید یه آرتیکل از خود شرکته که ۴تا مورده فک کنم ویدئوهم در موردش زیاد هست در یوتیوب اگه ویدئو طولانیه در قسمت توضیحات بنویسید هر دقیقه چه موضوعی شروع میشه ولی در کل بالای بیست دقیقه زمان ویدئوها رریسکیه مدت زمان ویدئوها کم کم ببرید بالا ببینید متوسط واچ تایمتون چقدره
-> ممنون"
"-> منظورم graph neural network بود
-> در حد آشنایی اولیه رو از همون فصل موجود در دوره یادگیری عمیق ببینید بعد که اون رو خوب دیدید دوره استنفورد هم خوب هست یک دوره تخصصی و سطح بالا داره بعدشم که باید دیگه مقاله بخونید و پیادهسازیهای مختلف ببینید
-> بهروابطواینکهاز کجا اومدن اشاره شده تو دوره استنفورد
-> نمیدونم دنبال چی هستید حالا یک نگاهی بهش بندازید ببینید در راستای اهدافتون هست یا نه"
"-> سلام وقت بخیر کسی یه منبع کامل مثل کتاب برایگراف میشناسه که منسجم وخوب تیوری و اساس کارشوتوضیحبده
-> ریاضیات گسسته ابولفضل گیلک گراف رو خوب گفته آشنایی با نظریه گراف علیرضا علیپور"
"-> و اگه میشه یه پلی لیست یوتیوب جدا جهت بررسی مقاله هم بذارید مثل ژورنال کلاب چون درین مورد حتی یوتیوبرهای خارجی هم پلی لیست خوب ندارن و فک کنم بازخورد خوبی داشته باشه
-> مقالههای کلاسیک مثل الکس نت رزنت و غیره بذاریم نگاه میکنید کلا لطف بزرگیه که دوستان پیشنهاد بدن چه چیزهایی دوست دارن تو یوتوب باشه و مشتاق هستن که ببینن
-> عالی میشه استاد یا حتی با صلاح دید خودتون معرفی و مروری روی مقالات جدید و برتر کنفرانسهای معتبر مثل NeurIPS CVPR ICLR ICCV و غیره
-> به نظر من اگه مقالات جدید باشه بهتره مثلا مقالات جدید و پر ارجاعی که خوندین و براتون جالب بوده اگر این مقالات جدیدی که جالب بودن رو حتی در حد مطرح کردن اسمشون توی گروه یا کانال هم انجام بدید به نظرم خیلی خوب میشه
-> اره هر مقاله ای باشه فقط بخش فرمول جدید هر مقاله و ریاضیاتش رو توضیح بیسیک بدید و ساختار مدلها
-> به نظرم کانال تازه هست یه ذره بیسیک تر که مخاطب جذب بشه یا دو تا پلی لیست باشه entrylevel و ابتدایی تر و یکی دیگه state of the art articles و اینکه منظم باشه مثلا آپلود هر پلی لیست در یک روز خاص و یه گروه تلگرام جدا برا اعضای کانال یوتیوب بذارید که متریالهای بیشتر آموزشی رو اونجا بذارید چون عملا چیزی به عنوان ژورنال کلاب هوش مصنوعی با مدرس فارسی نداریم در یوتیوب و جدیده پس میگیره ضمن اینکه لینکدین اعضای همین گروه معرفی کنند کانال یوتیوب رو که چه بهتر
-> سلام به نظر من عالیه اگه پوشش دادن مطالب جدید مثل RAG و GNN هم داشته باشیم هم عالی تر میشه
-> نه آقای دکتر
-> داشتیم برنامه میریختیم با الکس نت شروع کنیم داشتیم ستآپ رو آماده میکردیم
-> خیلی دیگه اون بورینگه آقای دکتر
-> سلام فصل یادگیری دوره یادگیری عمیق خیلی خوب بود همچین مباحث به روزی رو ترجیح میدم
-> متوجه شدم ممنون
-> مثل اینکه از نظر دوستان بورینگ نیست البته یادمه شما اون موقع دوره بینایی رو خیلی عمیق نگاه کردید
-> آره آقای دکتر همون دیشب ۴ تا لایک گرفت فهمیدم دوستان علاقه دارند موفق باشید خوشحال شدم تو ذهنتون مونده بود
-> یه دوره بررسی مقاله بر روی مقالات ۲۰۲۳ عالی میشه برگزار بشه و پیاده سازی کنین مقاله رو
-> با پیادهسازی کار خیلی سختیه ولی شاید خیلی سبکش رو بتونیم یوتوب داشته باشیم دیگه صف دورهها هم پر هست ممنونم که نظر میدید
-> سلام استاد دانشجوهاى شما بيشتر nlp علاقمندن ولي من يادگيرى تقويتى علاقمندم شما كى برگزار ميكنيد
-> این که دیگه خیلی دوره فعلا این چند تا دوره رو برگزار کنیم بیناییها بعدش هم nlp و ماشین کاتالیست
-> استاد اگه دوره های کاتالیست پروژه هاش رو بشه جدا خریداری کرد خوب میشه درباره دیپ گفته بودین نمیشه ولی گفتم زودتر بگم برای ماشین شاید برنامه ریزی جوری باشه که بشه
-> فکر کنم دوستانی که دیپ کاتالیست رو کامل دیدن تایید میکنن که یکی از مهمترین فاکتورهای اون دوره ترتیب و مکمل بودن پروژههاست از بیرون به نظر میرسه چند تا پروژه جدا از هم هست ولی واقعیت اینطور نیست این رویکرد خوبی بود و میخوام همین حفظ بشه جدا از این از لحاظ پشتیبانی و مدیریت سایت هم تک پروژهای زحمت داره امیدوارم شرایط به شکلی باشه که هرکی دوست داره بتونه شرکت کنه
-> استاد شما گفتيد اين فروردين ماه دوره رو شروع كنيد
-> نه اون دوره نیست یک فصل از دوره دیپ هست"
"-> معرفی و آموزش کار با paperswithcode کانال یوتوب هوسم را با این ویدئو افتتاح کردیم لطفا هوسم را در یوتوب دنبال کنید لینک ویدئو در یوتوب howsamorg instagramcomhowsam_org youtubecom
-> سلام ممنون از معرفی این سایت دو تا سوال در مورد خوندن مقاله و قسمتusage over time این سایت دارم سوال اولم این هست که geluالان بیشتر استفاده میشه صرفا مقالات هست یا روتین صنعت یعنی من مبتدی از الان کدوم رو کار کنم چون همیشه حرف از relu بود در آموزش ها سوال دومم این هست آیا افراد سینیور اکثر ریاضیات یک مقاله رو میفهمند چون هرچی درnlpجلوتر میرم میفهمم ریاضی بیشتری برای فهم نیاز دارم من هنوز در فهم اتنشن مشکل دارم فک کنم هرچی ویدئو بود در یوتیوب دیدم هنوز سوال دارم الان میخوام انواع اتنشن رو بررسی کنم یه جا هم خوندم کمپانی ها بزرگ در بعضی مقالات یه جوری ایده ی مقاله مطرح میکنند که مبهمه و صرفا ارایه ی مقاله هست که بگن ما نحوه ی کار رو گفتیم حالا فهمش دیگه با شما
-> سلام با اجازه از آقای اشرفی و ادمین عزیز من یه پاسخ مختصر به سوال دوم شما میدم پیش از اینکه آقای اشرفی جواب کاملتر رو بیان کنند به هر حال بیس هر علمی از جمله یادگیری ماشین ریاضیات هست حالا اینکه افراد سینیور ریاضیات مقاله رو میفهمن یا نه بستگی به خود افراد و موقعیتی که توش کار میکنند داره الان هر کسی با هر بکگراندی خودشو سینیور ماشین لرنینگ اسپشیالیست میدونه ازین افراد بگذریم از افراد سینیور که تو کارهای تحقیقاتی مشارکت میکنند یا هدفشون پیشبرد روشهای قبلی و توسعه روشهای جدیدتر و کارآمدتر هست انتظار میره کاملا تئوری مقالات رو بفهمن افرادی که کارشون این هست که مدلهای توسعه داده شده توسط بقیه رو به صورت یک سرویس در اختیار مصرف کننده قرار بدن نیاز به فهم عمیق از ریاضیات روشی که میخوان استفاده کنن ندارن هرچند اگر که درک داسته باشن مفیده بلکه تسلط به موارد دیگهای براشون ضروری هست از بین موضوعات مختلف یادگیری ماشین NLP ریاضیات عمیق و پیچیدهای نداره نسبت به موضوعاتی مثل یادگیری تقویتی و مقالات هم برای چاپ شدن باید محدودیت صفحه ژورنالها رو رعایت کنند که این باعث میشه تو مواردی مختصر نویسی کنند ولی بازم باید نوآوری و روش کارشون واضح باشه تا تو خود فرایند داوری به مشکل نخورن همچنین اتنشن واقعا مفهوم سادهای داره پس مجموعا توصیه میکنم بکگراند ریاضیتون رو تقویت کنید تا راحتتر مقالاتی که میخونید رو درک کنید
-> 
-> من تجربی خوندم دبیرستان اما از همون اول شروع ai حسابان و ماتریس و ریاضیات مهندسی و احتمال مکتبخونه و فرادرس و یوتیوب هم خوندم به تدریج اما همچنان تا میرسه به فرمول جدید مقالات گیر میکنم که خب بذار تجزیه تحلیل کنیم چی میگه این فرمول میفهمم ها اما نه اونجور که بتونم فرمول ریاضی معادل اونچه در ذهنم هست رو پياده کنم دوست دارم تحقیقاتی کار کنم بیشتر حتی کتابapplied ai هم با توصیه یکی از اساتید تا وسطاش خوندم ببینم فرق صنعت و دانشگاه چیه
-> برای ایدهپردازی و کار تحقیقاتی بهتر هست تحت نظر یک مشاور یا راهنما کار کنید کار تحقیقاتی هم ملاحظات خودش رو داره و حتی برای کارهای اول تحقیقاتی بهتر هست راهنما یا مشاور ایده بزنن و شما صرفا اجراش کنید در واقع شاید رانندگی رو بلد باشید ولی مسافرکشی بلد نیستید
-> خب یه سوال دیگه نظرتون در مورد کورسهای یوتیوب استنفورد ai چیه بهتره از همونجا کانکشن بگیرم جهت تحقیقات چون کورسهاشون رو خیلی خوشم اومد خیلی چالشی بود یعنی هر ویدئو استنفورد خودش نیاز به ۵ تا ویویو clarification داره nlp and nlu یاopen ai چون علاقمند به human_ai interaction هستم
-> بیسیکml رو میفهمم به مقاله که میرسم فرمول جدید برام مبهمه اصول مقاله خوانی در فیلد ai هم اگه یه ویدئو بذارید یوتیوب عالی میشه
-> ممنون از راهنماییتون
-> 
-> ممنون از راهنماییتون معمولا هر روز در موردnlpپست لینکدین میذارم اصولا هم سوالات خودم طی مسیر یادگیری هست مثلا الان رسیدم به human_centric ai درnlp
-> خب لینکدینتون رو بذارید ببینیم
-> خوشحال میشم نظرتون بدونم چون اخیرا content writing به صورت منظم در لینکدین شروع کردم هدفم هم اشتراک تجربیات طی مسیر یادگیری هست"
"-> درود دوستان اگر تجربه ای توی Fine Tune کردن مدل های متن بازی مثل Llama Mixtral دارید ممنون میشم راهنمایی کنید من دارم با ollama مدل Llama 3 رو روی لوکال لود میکنم که بتونم برای یه تسکی که داشتم استفاده اش کنم مشکلی پیش اومد و مجبور شدم بیارمش روی کولب و موقع استفاده از مدل به ارور Cannot assign requested address برمیخورم ایا روش بهتری برای لود کردن مدل هست
-> "
"-> سلام استاد وقت بخیر استاد پروژه های دیپ کاتالیست رو به صورت تکی نمیشه تهیه کرد من فقط language modeling اش میخوام
-> سلام خیر دلایل بسیار زیادی داره که نمیتونیم چنین کاری انجام بدیم
-> استاد الان که بحث کاتالیست شد یادم افتاد به شدت جای ماشین کاتالیست خالیه و واقعا جاش داره احساس میشه این دوره برنامه ای براش در چند ماه آینده ندارید
-> سلام نه فکر نکنم 1403 برگزار بشه قبلا توی گروه خیلی درموردش صحبت کردیم ولی برنامههای خوبی براش داریم
-> استاد هرکاری کردید لطفا به NLP هم فکر کنید و اونو عقب نندازید
-> سلام اگر خیلی عجله دارید میتونید تا وقتی آقای دکتر استارت میزنن با سرچ awesome nlp github مطالعه خودتون رو شروع کنید موفق باشید
-> هم درموردش فکر میکنیم و هم روش کار میکنیم تقریبا هرهفته براش وقت میذاریم
-> دقیقا
-> من مشتاقانه منتظر دورتون هستم
-> ارادت"
"-> آیا امکان نوشتم بات اینستاگرامی مثل بات تلگرامی وجود داره اگر آره چطوری منبع معرفی کنید
-> اکثر این شبکههای اجتماعی بات دارن تلگرام اینستاگرام و توییتر رو مطمئن هستم که دارن یکسری کسب و کار و خدمات مبتنی بر همین باتهای اینستا همین داخلی وجود داره من منبعی نمیشناسم قبلا برای تلگرام از یوتوب استفاده کردم و خوب بود"
"-> سلام دوستان من یسوال غیر مربوط به هوش مصنوعی دارم میتونم اینجا بپرسم
-> البته خیلی هم بی ربط نیست و برمیگرده به بحث دیتا کرالینگ
-> حالا ریز ریز خب بگید دیگه"
"-> حالا باز خودمم میخوام سعی کنم ی دور از صفر همه این کار هارو انجام بدم ببینم به این نتایج میتونم برسم یا نه و استاد واقعا اموزشاتون عالیه تو ارشد فک کنم راحت 60 درصد واحدام با اموزشای شما پاس میشند
-> خواهش میکنم هرجا هستی موفق باشی خیلی تغییر کردی"
"-> البته اگر موردی هم هست که میخواید بگید قطعا استقبال میکنم
-> جواب چی بود
-> کدای گیت هاب پروژه رو توی بخش پردازش داده پیدا کردم شیپ داده های خام مثلا همچین چیزی بود یک داده فقط 3000 17 3 وقتی کدای پیش پردازش گیت هاب رو اجرا کردم دیدم ی X_train تولید شده به این شیپ 450 1740 51 اون 450 یعنی از 550 تا 450 تا سیگنال رو برداشتند واسه آموزش 51 هم که 17 ضربدر 3 هست پس فلتن کردند میمونه 1740 که واسم سوال شد چیه یکمی کدا رو پایین بالا کردم دیدم ی جاهایی padding کردند فهمیدم که سیگنال به طول 3000 رو شکستند به سیگنال های کوچیکتر اگه یادتون باشه تو پیام قبلی که پاک کردم نوشته بودم توی ی سیگنال چند تا عمل ممکنه اتفاق افتاده باشه یعنی فرضا توی این 3000 نقطه زمانی توی 1000 یک عملی رخ داده و توی 1500 یک عمل دیگه احتمالا بعد از اینکه سیگنال ها رو به سیگنال های کوچیکتر تقسیم کردند بیشترین طولش 1740 بوده
-> من روی سوال اول که فلتن بود میخواستم فکر کنم به این فکر میکردم که راه دیگهای مثل استفاده از کانولوشنیهای برای استخراج و داون سمپل هست یا نه یکم پیچیده میشد ممنون
-> این لینک مقالشون هست که اگر علاقه داشتید مطالعه کنید"
"-> چرا پاک کردی داشتم فکر میکردم
-> جوابو گرفتم"
"-> سلام دوستان کسی دورهای برای ویولت نمیشناسه برای پردازش سیگنال eeg میخوام میخوام تا حدودی مفهومش رو یاد بگیرم نه الزاما کد نویسی
-> سلام و وقت بخیر مفهوم خیلی پیچیدهای نداره با سرچ تو اینترنت و خوندن بلاگ پستها هم میتونید یاد بگیرید کلیت مفهومش رو
-> نگاه کردم اونا رو کافی نبودن به نظرم"
"-> سلام دوستان من هر بار که این کد را اجرا میگیرم بخش تست خیلی بی قاعده نوسان میکنه چرا اینطوری میشه کد کلاسیفیکیشن روی دیتاست با حدود ۲۰۰۰ تصویر هست
-> سلام باید برای هر پروژه بررسی دقیق انجام بشه ولی به صورت کلی رایج ترین چیزهایی که من باهاش برخوردم این هاست که بهتره بررسی کنید ۱ تنظیم لرنینگ ریت بالا ۲ داده های کم در ولیدیشن ۳ بالانس نبودن کلاس های تارگت ۴ وجود اوتلایر در دیتاست
-> ممنون از پاسخگویی تون آخه روند train خوبه اما تست مثلا یهو از دقت ۸۰ میره برای ۴۰ دوباره بر میگرده ۷۰ نوسانی عمل میکنه
-> بعضی از مواردی که مهدی گفت میتونه فقط برای ولیدیشن رخ بده درحالیکه ترین نرمال باشه خیلی مسائل باید چک بشه از جمله اینکه مدل نسبت به این دیتاست پیچیده هست یا نه چون ممکن هست وزنهای بزرگی داشته باشه و تغییر اندکی در ورودی منجر به تغییر بزرگی در خروجی بشه روی ترین این اتفاق نمیفته چون دادهها رو دیده ولی روی دادههای جدید این ناپایداری ممکن هست به وجود بیاد کلا بعد از آموزش مدل یک کار مهم آنالیز مدل هست که کمتر بهش توجه میشه باید به شکلهای مختلف آنالیز کنید تا ببینید درست کار میکنه یا نه یا اگه خوب کار نمیکنه مشکل از کجاست
-> ممنون از راهنمایی تون الان یعنی مدل پیچیده است یک رزنت با مکانیزم توجه هست بله حتما سعی می کنم مدل را آنالیز کنم
-> ما که نمیدونیم پیچیدگی مدل رو با توجه به دیتاست گفتم تکه تکه اطلاعات دادن هم خطرناکه دیگه"
"-> سلام کسی میدونه این شکل ها و بلوک دیاگرام ها رو چجوری باید درست کرد
-> 
-> اينجا بخونيد چندتا وبسايت گذاشته
-> احتمالش زیاده که ویزیو باشه
-> سلام Mathmiteduennui alexlenailmeNNSVGindexhtml"
"-> سلام وقتتون بخیر در یادگیری تقطیر دانش شبکه معلم در یک دیتاست با ماهیت دیگر ترین میشه
-> همچنین آقای اشرفی اگر ممکنه در مورد این سوال توضیح بفرمایید ممنون میشم
-> روشهای متفاوتی وجود داره بستگی به اهداف و تسک داره اگه سادهترین حالتش رو میگید که خب بله معلم رو هم روی دیتاست اصلی ترین میکنیم همین رو در دوره بینایی و دیپ پیاده کردیم"
"-> سلام وقت بخیر در دوره ها نحوه اتصال به سرور و چگونگی استفاده ازکدهای گیت هاب در سرور گفته شده اگه گفته نشده ممنون میشم راهنمایی داشته باشید
-> سلام متوجه دقیق منظورتون نشدم اما برای اتصال کدهای گیت هاب به کولب از دستور git clone میتونید استفاده کنید"
"-> سلام برای استفاده از ترنسفر لرنینگ روی داده های پزشکی مثل CT MRI Ultrasound دیتاست پزشکی ای هست که قبلا ترین داده شده باشه ازش استفاده کنم یا همون Imagenet استفاده کنم
-> سلام همون ایمیجنت خوبه"
"-> import numpy as np import matplotlibpyplot as plt System matrices A1 nparray01101 02330 09294 07130 02660 07942 00627 00199 00732 A2 nparray04326 06656 02175 02521 00672 05607 03860 00513 04489 Q1 npeye3 Q2 npeye3 GNN Parameters gamma 10 a1 1 a2 3 p 05 def activation_functionx return a1 npexpnpabsxp npabsx1p npsignx p a2 npsignx Initial conditions X1_0 2 nprandomrand3 3 1 X2_0 2 nprandomrand3 3 1 Simulate GNN max_steps 1000 X1 npzerosmax_steps 3 3 X2 npzerosmax_steps 3 3 X10 X1_0 X20 X2_0 for t in rangemax_steps 1 E1 A1 X1t A1T X2t Q1 E2 A2 X2t A2T X1t1 Q2 dX1 gamma A1T activation_functionE1 A1 activation_functionE2 dX2 gamma A2T activation_functionE2 A2 activation_functionE1 X1t1 X1t dX1 X2t1 X2t dX2 Plot results fig axs pltsubplots2 3 figsize12 8 for i in range3 for j in range3 axsi jplotX1 i j axsi jplotX2 i j axsi jset_titlefxi1j1 plttight_layout pltshow سلام دوستان وقتتون بخیر کسی میدونه چرا این کد توو حلقه اجرا نمیشه ممنون میشم اگر میدونید راهنمایی کنید
-> بنظر میاد مشکل از تابع activation_function هست که هم خطاری سینتکسی داره هم موقع اجرا به مقادیر گیر میده
-> ممنونم از پاسختون نمیدونم بدون for امتحان کردم اون جواب داد"
"-> سلام وقت بخیر ببخشید کسی اطلاع داره با چه چیزی میشه شکل های فلوچارت مانند مدل هارو کشید مثل این عکس
-> از نرم افزار visio ماکروسافت میتونی استفاده کنی
-> یا diagramsnet
-> باتوجه به شکل فونت و فلشها فکر کنم از آفیس استفاده کرده همین کار رو با ورد پاورپوینت و ویزیو که حسین گفت میتونید بکشید ویزیو دیفالت با آفیس نصب نمیشه کار باهاش سخت نیست اما میتونید همه اینها رو توی پاورپوینت هم بکشید و بعد به صورت تصویر ذخیره کنید"
"-> سلام وقتتون بخیر معیارهای bleu برای ارزیابی متن تو دوره ها توضیح داده شده
-> سلام فکر کنم دیپ کاتالیست پروژه Image captioning"
"-> یک گیت هاپ یا آدرس سایت مثل پایتون میشه معرفی کنید برای تمرین پروژه دیپ لرنینگ
-> عزیزم بهتر نیست کل پیامهاتون رو یکجا بنویسید و بعد در قالب یک پیام بفرستید
-> Kaggle خیلی درموردش در گروه یا کانال پایتورچ صحبت شده
-> من عضو نبودم
-> سلام استاد اميدوارم حالتون خوب باشه يك سوالى داشتم ازتون كه خواستم بدونم من مقاله اى كه ميخوام بزنم و يه ايده اى دارم به هرحال موضوعش اينه ميخوام ديتاست اطلاعات جمع آورى كنم و پردازش كنم اين موضوع مشكلى نداره كه بخوام چاپش كنم تو بين المللى من زياد تو اين مورد وارد نيستم يكم راهنمايى ميخوام با تشكر
-> سلام درست متوجه نشدم میخواید دیتاست بسازید اگه آره نه مشکلی برای چاپ مقاله ندارید فقط نکته مهم در مقاله این هست که باید کارتون رو با دیگران مقایسه کنید وقتی یک دیتاست ساختید چطوری میخواید روی این دیتاست کار خودتون رو با دیگرام مقایسه کنید باید این مسائل فکر کنید
-> بله درسته ديتاست ميخوام بسازم اها ولي يه مقدار راجبش تحقيق كردم تا حالا كسي ديتاستش رو نساختن"
"-> این یکی مخصوص دانشجوها ۲۲ ساعت GPU رایگان اجرا توی بکگراند بدون قطع شدن حافظه و رم هم رایگان هست نیاز به Credit Card هم نداره اگر از ایمیل edu یا org استفاده کنید درجا دسترسی برای شما باز میشه اما اگر ندارید هم مشکلی نداره نهایتا ۲۳ روز طول میکشه تا دسترسی شما باز بشه بد نیست با چندتا ایمیل درخواست دسترسی بدید همین حالا
-> سلام کسی توانسته ثبت نام این را انجام بدهد من در مرحله وریفای شماره تلفن جلوتر نتوانستم برم از شمارههای کانادا انگلیس هم استفاده کردم ولی باز هم ایراد میگیرد کسی تجربهای دارد برای این قسمت"
"-> خب بازم میتونین صرفا یه عکس از نرم افزار بدید و عکسای متفاوت از پروژه واقعی و مثل قبل ادامه بدین
-> ممنونم ازتون"
"-> سلام به نظرم پرسیدن این سوال در گروه هوش مصنوعی در مورد شروع استارتاپ برای یک مبتدی و راهنمایی سایر افراد باتجربه خیلی هم راهگشا هست اینکه بدونیم چجوری در پروسه یادگیری دید بیزینس داشته باشیم برای اجرای ایده خودمون وapplied mlچه فرقی داره و درصنعت چی میگذره در مقایسه با دانشگاه فیلد من nlpو ریکامندیشن سیستم هست و بین رشته ای کار میکنم در حال حاضر در حال تحقیق هم هستم در مورد کمپانیها فیلد خودم اشتراک تجربیات ارزشمنده
-> بله درست میفرمایید البته در صورتیکه صورت مساله شما در مورد بیزینس در هوش مصنوعی باشه یا سوال کلی بیشتر راههایی که برخی افراد هنوز می خوان برن بقیه رفتن اینکه گفته میشه جواب سوال رو از اصل فرد باید پرسید به این دلیله که به مشکل نخورید فقط یک امار بهتون میدم تا متوجه مشاوره اشتباه در حوزه بیزینس بشین طبق امار پارک از هر ۱۰ استارتاپ ۹ تا به شکست منجر میشه خودمون تجربه داشتیم همیشه بدونید در ساختار بیزینس جایگاه فرد فنی و بیزینسی متفاوته حالا خودتون میدونید اما تجربه چندباره تلخ برای ما ثابت کرده که قسمت بیزینس کارتون رو بسپارید به اهلش تا مثل ما نشید
-> ممنون از راهنماییتون"
"-> نمونه ش این دو عکسه که باید با هم مقایسه کنم
-> سلام در خصوص این موردی که فرمودید چیزی که به نظر من می رسد می گویم نمی دانم چقدر درست است فقط در حد یک فکر است دو تا عکس را حالت باینری کنید و از هم کم کنید آنچه می مانند مانده کار است بعد برای هر قسمت که مانده یک درصد تعیین کنید و اعلام کنید البته یک رشته به اسم کنترل پروژه است که صرفا کار آنها این است می توانید از آنها هم راهنمایی بگیرید"
"-> سلام یه سوال مهم داشتم برای ایده های استارتاپی چجور میتونیم Investor داشته باشیم برای ثبت استارتاپ خارج از ایران بهتره یک ایده رو ارایه داد به یه کمپانی بزرگتر با بیزینس پلن مربوطش و به عنوان مدیر پروژه همون ایده شروع به کار کرد و از کردیت و امکانات از پیش آماده استفاده کرد یا خودمون شروع به کار کنیم و بعد ازینکه مدل اولیه تا حدی استیبل شد جهت گسترشش جذب سرمایه کرد بین این دو مورد شک دارم مدیر پروژه ایده خودم در یه شرکت بزرگتر یا شروع از صفر به عنوان استارتاپ امروز خوندم که برای مثال canva بنیانگذارش صد بار ریجکت شده تا نهایتا جذب سرمایه گذار داشته ممنون میشم راهنمایی کنید اگر تجربه ای درين زمینه دارید
-> خب اگه به بزرگ کردن دنیامون هست واقعا نیاز هست که بگیم چرا یک نفر دیسلایک زده در واقع شما هم میتونید رد بشید ازش
-> بایاس دارید دیگه از یک طرف ملاحظه یک نفر رو میکنید که خب درسته از یک طرف به دیسلایک یک نفر حمله کور میکنید خب شاید این طرف هم دیسلایک زده و میخواد جواب بده اما فرصت نکرده شاید دهها دلیل دیگه داره ضمن اینکه اون کسی که دیسلایک زده رو میشناسم محسن یکی از افراد با دانش گروه هست و معمولا به کمک بچهها میاد احتمالا جواب میده به هرحال به نظرم دیسلایک هم خوبه منم توی زندگیم یک عالمه دیسلایک بیناظر گرفتم و بهش فکر کردم که چرا حالا من و شما کاسه داغتر از آش نشیم
-> سلام خیلی ناراحت نشید این دست قبیل سوالات جایگاهش جای دیگری است اینکه در گروه هوش مصنوعی پرسیده بشه عیب نیست اما باید دانست که این قبیل سوالات رو باید از منتور مرتبط پرسیده بشه ارتباط با سیستم خارج اصول و قضایا خودش رو داره پرسیدن و جواب خواستن خارج از حوزه منتور سبب اسیب به آینده و شخصی میشه دیسلایک بی ادبی یا به چالش بد کشیدن طرف نیست مثالش منتورینگ نوشتن قراردادهای کاری با خارج هست می توان این قبیل سوالات رو با بیان کردن در پارک های علم و فناوری و جویا شدن منتورش بهتر مطمئن شد این اتفاق در زمینه های مختلف برای شرکتها و افرادهای استارتاپی رخ میده و با پرسیدن در جاها و افرادهای نا مرتبط به تصمیم اشتباه میرسن خودمون در پارک شرکت داریم و می دونیم اشتباه تو این زمینه یعنی تباهی ایده و اینده طرف
-> اتفاقا من هم معتقدم افرادی که توی گروههای تلگرامی هر سوالی رو میپرسن ممکن هست آسیب ببینن چند سال پیش یک نفر از من پرسید من چهارده سالمه و میخوام پردازش تصویر یاد بگیرم من اولش تعجب کردم ولی یکسری راهکار بهش دادم در کمال تعجب یک هفته بعد دیدم توی گروهی یک نفر سوال مشابهی پرسیده و این فرد چهاردهساله داره راهنماییش میکنه این یک نمونه هست به وفور از این موارد راهنمایی نادرست میشه دید
-> درود پی وی"
"-> در مورد شبکه UNET کسی اطلاعی داره که چیست یا چطور است اگر هم دوره آموزشی خوبی از ان سراغ دارین ممنونم میشم معرفی کنین
-> سلام دوره دیپ لرنینگ و بینایی کامپیوتر و همچنین دوره دیپ کاتالیست هوسم عالیه خودتون برین توی سایت هوسم ببینید کدوم براتون بهتره"
"-> سلام دوستان این ویدئو خیلی قشنگ و مفهومی مکانیزم توجه رو بیان کرده اگه با مکانیزم توجه آشنایی ندارید یا میخواهید بیشتر در رابطه باهاش دید بگیرید دیدن این ویدئو رو توصیه میکنم در کل این کانال مقالات روز و همچنین مباحث پایهای یادگیری ماشین و یادگیری عمیق رو مرور میکنه مفاهیم رو توضیح میده و درآخر با جزئیات پیادهسازی میکنه میزنه شدیدا توصیهاش میکنم
-> سلام عزیزم احساس کردم کانال خودتون هست اگه اینطوره خب بگید کانال خودمه تولید محتوا کار خوبیه و خود فرد تولیدکننده محتوا بیشتر از بقیه یاد میگیره عنوان ویدئوهای توی کانال رو نگاه کردم خیلی زحمت کشیدید با آرزوی موفقیت بیشتر
-> سلام ممنونم از فیدبک و ساپورت شما نظر لطف شماست بله کانال خودم هست
-> برای اتنشن این ویدئو رو ببینید نیازی به ویدئوهای دیگه نخواهید داشت سوالم داشتید بپرسید من شخصا راهنمایتون میکنم"
"-> راستی بچه هایی که در مفهوم اتنشنattention و ترانسفورمورز _ مشکل دارید مثل خودم یه مجموعه از پلی لیست یوتیوبرهای منتخب میفرستم در آینده نزدیک در پردازش تصویر وnlp مهمه
-> در آینده میخواید بفرستید چرا الان اطلاعرسانی کردید
-> چون فقط یه ویدئوش مونده این هفته آپلود میشه در کانال مربوطه
-> یه توصیه براتون دارم اگر جایی حس کردین دیگه مفهموم ترنسفورمر رو فهمیدین بیشتر پیاده سازیهاشو کار کنید چون تجربه کردم ویدئو و فیلم زیاد چون طرز بیانها متفاوت میشه تاثیرش معکوس میشه و بعضی وقتها به چیزیم که درست میدونید شک میکنید"
"-> میدونم ولی باز نیست اخه
-> برنامه tesk manager باز كن شماره ايدي هم نوشته رو پيدا كن و اون برنامه ببند
-> میتونه برنامه بسته باشه ولی سرویس فعال مونده باشه توی task manager تب سرویس همون شماره سرویس خواسته شده رو ببندید و اصلا هم کاری به اینکه خود anydesk اپش در جه حالته نداشته باشین"
"-> سلام وقت همگی بخیر موقع دانلود دوره Open CV با این خطا مواجه شدم میشه راهنمایی کنید لطفا
-> برنامه any desk ببند"
"-> با مرور کامنتهای قبل اگه درست فهمیده باشم این جا روشهای کلاسیک پردازش تصویر با open cv هست و در ادامه در تابستان دوره بینایی کامپیوتر با شبکه های عصبی سوال گذراندن Open CV برای درک بهتر بینایی کامپیوتر با شبکه های عصبی لازمه یا تاثیری نداره دوم اینکه بینایی کامپیوتر که اینجا به ما دادین این دوره جدید تابستان اپدیت همین دورست و نهایت دوره کاتالیست رو برای تکمیل دوره deep learn پیشنهاد میدین ممنون
-> کارآموزی سه ماهه داشتم جهت پردازش تصاویر پزشکی ازمون open cvرو میخواستن یه کتاب ترجمه شده خوب فارسی ازش هست و دوره های یوتیوب"
"-> سلام دیتاست محلی دارم که قراره به سه قسمت تقسیم بشه اما هر سری که ران می کنم چون رندومه داده ها متفاوت میشه دستور from torchutilsdata import DataLoader Dataset random_split train_set valid_set test_set random_splitdataset lengths train_len valid_len test_len هست اما سید پیدا نکردم براش کسی میدونه چیکار باید کنم تنها راهی که به ذهنم میرسه بعد تقسیم سیوشون کنم و سیو شده ها رو بخونم ولی می خوام یه چیزی شبیه سید داشته باشم
-> سلام"
"-> درود و وقت بخیر آیا از دوستان کسی دوره ای سراغ داره که سرتیفیکیت بده البته که انگلیسی و معتبر در سطح جهانی باشه به مدرکش نیاز دارم برای دوره های ماشین و دیپ لرنینگ ممنون
-> سلام کورسرا
-> Udemy"
"-> سلام روز همگی بخیر تصویر اول مدل اصلی است و تصویر دوم مدل ساده شده است مشکل دیتا ندارم و میدونم over fit نشده کسی میتونه علت اینکه خطای valid چنین رفتاری داره را توضیح بده
-> به نظر من محتمل ترین حالت توزیع متفاوت دیتا train و valid هست مطمئنید مشکل در دیتا ندارید
-> بله اندازه دیتاست مشکلی ندارن دیتاست خودش تقسیم به ولید و ترین را انجام داده و جدا کرده با آین دیتاست کلی مدل ترین کردن و مشکلی توزیع نداستن
-> ایده من در تصویر اول هم به اندازه تعداد ایپاک تصویر دوم حدود هجده تا هم لاس ولید بالا رفته بعد اومده پایین بنظر در مدل ساده شده اجازه دهید ایپاک های بیشتری بروند تا بهتر تصمیم گرفته شود
-> بررسی میکنم ممنون
-> نه بنظرم اشتباه قضاوت کردم ببخشید من که میگم اورفیت شده
-> حتی اگه اورفیت شده باشه باز هم باید چند تا اپک بیاد پایین و بعد اورفیت بشه ولی توی تصویر اول خطای ولید حدود ۳ میشه توی مدل دوم هم فقط دو تا اپک مقدار loss کم شده و بعد شروع به افزایش کرده
-> بتظرم همون دوتا کافیه قرار نیست خیلی جلو هم بره
-> دیتاست هم ویدیو و ۲۰۰۰۰ تا ویدیو که هرکدام حدود ۴۰۰ تا فریم داره هر نمونه هم از ۱۰ تا فریم تشکیل شده پس مشکل دیتاست نیست مدل هم دو تا لایه lstm با سایز ۲۵۶ داره از رسنت هم استفاده که فقط لایه سوم fine tune میشه
-> خیالت از داده ک راحته پس هیچ اموزش هم ک زیاد نخورده و اینجوری شده فقط میمونه ظرفیت مدل از لحاظ تئوری البته من lstm کار نکردم و پس همینجا ساکت میشم
-> ممنون از زمانی که گذاشتید
-> سلام هر ویدئو ۴۰۰ تا فریم داره چرا فقط ۱۰ تا فریم به مدل دادین بعدش هم برای تست و ترین نمونه ها رو بر بزنید یا اصطلاحا shuffle کنین دقت کنین از هر کلاس مثلا x درصد تو ترین باشه و 1x درصد تو تست یعنی کلاسی از ترین جا نمونه وگرنه چجوری میتونه تشخصیص بده تو ولیدیشن
-> برای کارم ۱۰ تا فریم کافی است دیتاست خودش قسمت ترین و ولید داره"
"-> از این جهت که خرابی کمتری نسبت به تاف داشته
-> منم سوال داشتم آیا تاف خرابی زیاد داره از این جهت که مدل های کاستوم شده ی تاف فراوان شده و تشخیص اصل از غیر اصل سخته
-> من فقط طبق آمار تعمیراتی میگم که زیادن نه تایید میکنم که خوبن نه میگم بده
-> همین منم همین رو شنیدم و کلا موندم برای خرید چون تو بازار الان ایران رو هرچی دست میزاری یه مشکل داره
-> سری لیجن خرابی کمی داره
-> آهان ممنونم"
"-> درود به شما یک سوال داشتم چه نوع سیستمی بخرم برای انجام پروژه ها از لحاظ گرافیک cpuو رم وحافظه ممنون میشم راهنمایی کنید
-> سلام علیکم چقدر میتونی هزینه کنی برادر
-> ۵۰ تومن
-> خوبه که یک تاف گیمینگ i7 11800H 16گیگ رم یک ترا ssd میتونی بخری
-> ترجیحا از سری لنوو لیجن نشد تاف
-> لنوو یک تبلت داشتم سوخت
-> با تبلت چیکار دارم خیلی سری خوبیه و البته گرون
-> ممنون بابت راهنمایی ت"
"-> سلام روز بخیر کسی کد یا راه حلی برای بچ جنریتور توی ویندوز نداره که مشکل num workers رو پوشش بده
-> سلام میتونید کد اصلی را تابع کنید و داخل Main فراخوانی کنید اینطور ورکرها کار خواهد کرد
-> خیلی ممنون همونطور که گفتید انجام دادم و کار کرد ولی یه مشکل اساسی وجود داره پس از اینکه آخرین داده ی موجود در جنریتور رو تحویل گرفتم تا تمام ورکر ها آماده نباشند داده ی جدید به من تحویل نمیده
-> من به این ارور نخوردم راستش ولی یه سری تنظیماتی را خود ورکر داره که چندتا بچ به صورت دیفالت اماده کنه هر ورکری ایو بیشتر کنید مثلا 4 تا ببینید بازم اوکی میشه"
"-> سلام و عرض ادب من تا انتهای فصل ۱۰ دوره یادگیری عمیق بینایی کامپیوتر را دیده ام بین این دو راه به لحاظ آموزشی کدام یک را پیشنهاد میدین ۱ دوره یادگیری عمیق را تمام کنم اول ۲ الان پروژه صفر و سوم UNet دوره دیپ کاتالیست را ببینم که مربوط به ویژن و فصل ۱۰ یادگیری عمیق هست خیلی ممنون میشم راهنمایی بفرمایین
-> سلام روش 2 خیلی خوبه"
"-> امیررضا تو که عاشق فوتبال و لیورپولی بیا این مقاله رو پرزنت کن
-> سلام استاد چه جالب باید بخونم مقاله رو به روی چشم
-> من یک مقداری مطالعه کردم رویکردشون این هست که بازیکنها توی موقعیت کرنر و رابطه بینشون رو به صورت یک گراف درنظر میگیرن
-> پس دیگه خوندنش واجب شد کلا هر چیزی مربوط به لیورپول باشه نباید ساده ازش گذشت
-> امیررضا جان تهش میان همه تیماا از رئال کامبک میخورن
-> بله متاسفانه
-> البته دیگه نه همه اما لیورپول هست که همیشه بازندست"
"-> سلام دوستان من این کتاب رو نتونستم جایی دانلود کنم اگر دارید لطف میکنید برای من هم بفرستید Data structure and algorithms in python
-> از کتابخونه libgen استفاده کن انشاءالله بتونی پیداش کنی
-> منظورتون کتابی هست که پروفسور گودریچ نوشته
-> سلام ممنون از توجهتون نه این کتاب منظورمه
-> اهلن متوجه شدم فکر کنم اینو داشته باشم براتون می فرستم
-> لطف میکنید
-> 
-> ممنونم لطف کردید
-> آپلود شد
-> متشکرم باعث زحمت شدم ممنونم
-> خواهش می کنم اختیار دارین"
"-> سلام و عرض ادب ابتدا سال نو را به استاد گرامی و سپس به مجموعه هوسم و همچنین دانش آموختگان هوسم تبریک میگم استاد من بی نهایت بی نهایت از اموزش هاتون راضی هستم دوره دیپ لرنینگ را خرید کردم و تا انتهای فصل سوم مشاهده کردم اینقدر این اموزش ها کیفیت داشت که گفتم باید دوره یادگیری ماشین هم ببینمهرچند رودمپ را از اول درست نرفتم واقعا دست مریزاد باعث افتخارم هست من که سه سال اموزش پایتون میدادم الان شاگرد شما هستم به امید موفقیت های بیشتر و بیشتر
-> سلام ممنون سال نو شما هم مبارک برای من هم حضور و رضایت شما افتخار هست"
"-> درود استاد سال نو مبارک یعنی امسال منتظر دوره NLP باشیم فکر میکنم فرمودید نیمه دوم سال
-> نه سال بعد برگزار میشه"
"-> برنامه آموزشی امسال یک راهاندازی یوتوب هوسم دو Computer Vision Career سه دوره متوالی پردازش تصویر بینایی کامپیوتر و ویژن کاتالیست از فروردین پردازش تصویر شروع میشه البته در واقع کریر شامل چهار دوره هست که فعلا چهارمی رو نمیگم
-> استاد فقط یه موردی دورههای NLP و ماشین کاتالیست که گفته بودید در حال ایده پردازی براش هستید تو برنامهی امسال نیست
-> و RL فکر میکنم
-> نه امسال نمیرسیم بعد اینها دیگه نوبت این دو موردی هست که گفتید این دو مورد رو داریم روش کار میکنیم در هردو پیشرفت هم داشتیم
-> سلام استاد سال نوت مبارك باشه و روز خوبى داشته باشيد ببخشيد يه چيزى دوره ى يادگيرى تقويتى همين ماه برگزار ميكنيد
-> سلام ممنون سال نو مبارک نه همچین برنامهای نداریم اصلا یک ماهه نمیشه چنین دورهای برگزار کرد چند وقت پیش صحبت سر فصل یادگیری تقویتی یادگیری عمیق بود که گفتم آخر اسفند میرسونیم متاسفانه آخر اسفند نرسوندیم ولی داریم کار میکنیم یک بخشهایی از کدنویسی رو هم آماده کردیم تئوری رو آماده کنیم میریم برای ضبط
-> خيلى ممنون استاد بى صبرانه مشتاقيم با تشكر
-> سلام خیلی عالیه این مسیری که شما برای کامپیوتر ویژن اسپشیالیست اماده کردین احتمالا تا تابستون ۱۴۰۴ طول بکشه
-> سلام وقت بخیر سال نو مبارک قرار است در کانال یوتیوب به چه محتواهایی پرداخته شود
-> فعلا فقط محتواهای آموزشی هست احتمالا اولش هم با پایتورچ شروع میکنیم
-> با محتوای دوره شبکه عمیق متفاوته
-> سلام استاد اگر اشتباه نکنم سطح آموزش در حد همون مقالات رایگان سایت باید باشه درسته البته اگر تایم داشته باشید خیلی محتواهای متفاوت تری مثل کار با AWS و google cloud و هم توش گذاشت
-> آره قطعا در سطح وبلاگ هست
-> درسته در سطح همون وبلاگ محتواهای متفاوت هم توی لیست گذاشتیم از این بحثهای جدید مثل RAG LLM و غیره
-> بسیار عالی
-> توی دوره دیپ لرنینگ 40 ساعت پایتورچ گفتیم اینجا در اون حد گفته نمیشه و خیلی کمتره ولی در عین حال برای تازهکارها میتونه شروع خوبی باشه
-> سلام هر سه دوره امسال برگزار میشه
-> تلاشمون این هست که هرسه امسال برگزار بشه
-> استاد سلام وقتتون بخیر امکانش هست دوره opencv رو زودتر به اتمام برسه و اون دو دورهی دیگه رو شروع کنید
-> سلام نه دیگه برنامه ریزی کردیم و نمیتونیم زودتر شروع کنیم
-> اهانممنون
-> سلام توی هیچ کدوم از دوره های پردازش تصویر پردازش تصویر با ویولت هستش یا کسی کورس خوبه در این بارهه میشناسه
-> کتاب گونزالز هست کلا حرف خاصی نیست ویولتها مدام سیگنال رو به فرکانس بالا و پایین تقسیم میکنن و همین کار هم روی تصویر انجام میشه اگه ویولت رو بلد باشید دیگه نحوه اعمال روی تصویرش کاری نداره
-> سلام استاد یه سوالی وقتی سیگنال رو میخایم بدیم بعنوان ورودی به یه شبکه دیپ بدیم میایم سیگنال رو یه جورایی با fft به تصویر تبدیل میکنیم حالا بجای تبدیل فوریه بخایم از ویولت استفاده کنیم آنچنان فرقی نداره که از cwtویولت پیوسته یا dwt استفاده کنیم
-> نمیدونم چند سالی هست که ویولت کار نکردم
-> درسته باشه ممنون"
"-> سلام دوستان گلم عیدتون مبارک امیدوارم کنار هم کلی پیشرفت کنیم
-> سلام سال نوتون مبارک"
"-> انتشار دیتاست بزرگ محصولات و کامنت های دیجی کالا بیش از ۱۲ میلیون محصول در چندین ویژگی از جمله قیمت امتیاز محصول تعداد آرا دسته بندی نام تجاری و موارد دیگر مجموعه ای گسترده با بیش از ۶ میلیون کامنت انتشار توسط تیم Rade Ai لینک دانلود از kaggle لینک دانلود از huggingFace
-> من امروز دانلودش کردم و یکمی باهاش کار کردم جالبه"
"-> سلام جناب اشرفی امکانش هست تخفیف دورهها رو تا آخر تعطیلات داشته باشیم
-> سلام آقای رحمانی عزیز اول که از شما ممنونم بابت معرفی هوسم دوستان دیگهای هم اومدن خصوصی این درخواست رو داشتن احتمالا انشالله تمدیدش میکنیم"
"-> سلام و عرض ادب دوستان ببخشید یه سوال داشتم راجب یادگیری عمیق و بیشتر گوگل کولب گفتم شاید کسی بتونه کمک کنه من میخوام با CNN کار کنم البته سوال بیشتر کلی است و برای بقیه شبکه های عصبی هم صدق میکنه بنده داده ها رو از توی درایو میخونم میریزم توی VM گوگل کولب تصویر بالا کدش هست ولی بحثی که هست ۱۶۸۰۰ تا تصویر است در مجموع که تعداد کمی نیست البته چنانچه عرض کردم سوالم کاملا کلی است و داده های دیگه هم کم و بیش این مشکل رو دارند که به حجم داده و نوعشون بستگی داره شخصا اجرای این سلول از کد ۲ ساعت زمان میبره برای بنده ۲ ساعته اگر داده های شما هم زیاد باشه از هر نوع زمان قابل توجهی رو میبره حال اینکه GPU در پلن free خیلی کم و در پلن pro نیز اسما ۱۲ ساعت البته میگن رسما ۷ ۸ ساعت اگر زیاد استفاده شود در دسترسه و بعد کولب VM رو ریست میکنه و یک ماشین مجازی دیگه میده تا اونجایی که بنده فهمیدم امکانم نداره این بخش کد رو با CPU انجام بدم بعد ببرمش روی GPU چون دوباره ریست میشه خیلی آزاردهنده شده راهی نداره آیا یک طوری با یه تکنیکی چیزی توی vm نگهشون دارم چون واقعا آموزش شبکه ام به این اندازه زمان نمیبره
-> سلام کل ترین چند تا اپک است میتونین چک پونت هر اپک را توی گوگل درایو ذخیره کنید که زمانی که GPU از دسترستون خارج شد بتوننین مجدد چک پونت را فراخوانی و از ادامه ترین را انجام بدین
-> خیلی خیلی ممنون اینو بلد نیستم حتما یاد میگیرم مرسی از توضیحاتتون
-> سلام با این تصاویری که میبینم 16 17 هزار تصویر چندان عدد زیادی محسوب نمیشه به نظرم مشکل توی بخش کپی کردن تک تک تصاویر هست فرضا اگه کپی کردن هر تصویر 05 ثانیه طول بکشه 17 هزار تصویر حدودا 8500 ثانیه میشه میشه چند ساعت به نظرم کل دیتاست رو بهصورت زیپ توی گوگل درایو داشته باشید بعد اون زیپ رو کپی کنید توی vm و نهایتا آنزیپ احتمالا نهایتا ده دقیقهای انجام میشه
-> سلام جناب دکتر امیدوارم سال خوبی رو در پیش داشته باشیدراهنمایی تون کار کرد دیگه ۲ ساعت نیست شد همون ۷ ۸ دقیقه خیلی خیلی ممنونم"
"-> سلام من الان بعد چند روز برسی دوره ماشین ثبت نام کردم انشالله بتونم عملکرد خوبی داشته باشم در کنار استاد و شما دوستان عزیز
-> سلام چه جالب که چند روز وقت گذاشتید مشتاقم بدونم که توی این بررسی به چه چیزهایی رسیدید
-> سلام و عرض ادب مجدد حقیقتش سایت های زیادی رو مشاهده کردم وسرفصل ها و دمو های اون هارو رو دیدم و بنظرم هوسم کامل ترین اونها بود دوره ای که نیاز نباشه بعدش مجبور بشم دوره های متعدد دیگه ای در خصوص ماشین شرکت کنم بلکه مجبور بشم یک دوره رو صدبار ببینم که هوسم دقیقا همین بود"
"-> سلام روز بخیر کولب وقتی GPU خودشو در اختیار قرار نمیده چطوری میتونم دوباره ازش استفاده کنم دیروز مدلم با GPU ران شد اما امروز نه
-> سلام کولب به دلیل رایگان بودن دسترسی و محدودیت زمان ترین GPU رو موقتا غیر فعال میکنه باید صبر کنید تا دوباره بهتون بده برای من و دیگران رخ داده زمان تقریبی رو نمیدونم اما یک روز بعد برای من دسترسی داده بود ۲۴ ساعت
-> ممنونم"
"-> دوستان گلم خواهشی ازتون دارم لطفا اطلاعات این گروه رو در اختیار افراد خارج از هوسم نذارید سوال بالا رو ببینید جوین و بعد هم سوال بی معنی ما هم عضویت در گروه رو نیازمند Approve کردیم که قبل از عضویت اکانت افراد بررسی بشه این گروه مخصوص شماهایی هست که دانشجوی هوسم هستید و سوالهای غیرمرتبط با دورهها دارید یا اینکه میخواید فعالیت کنید و خودتون رو محک بزنید یا میخواید فعالیتهاتون رو به اشتراک بذارید که دیگر اعضای گروه حمایت کنن و از فعالیتهاتون هم خیلی خیلی استقبال میکنیم و حتما حمایت هم میکنیم چه خوب که گروه رو ترک نکنید چون دوست داریم همدیگر رو گم نکنیم
-> الان دیگه هرکی بخواد عضو بشه باید درخواست بده و ما بررسی میکنیم"
"-> سلام کتابخونه lazypredict با چند خط کد ساده مدل های پایه رو روی دیتاست از لحاظ دقت و عملکرد و مدت زمان اجرا بدون اینکه تنظیم هایپرپارمتر انجام بده ارزیابی میکنه که میتونه درک اولیه بهتری نسبت به اینکه کدوم مدل ها روی دیتاست جواب بهتری میدهند به ما بده
-> سلام بله اين كتابخانه عاليه"
"-> گروه اسمش دانش آموختگان هوسم هست ولی دانش نیاموختگان هوسم هم حضور دارن
-> دانش نیاموختگان دانش آموختگان آینده هستند"
"-> 
-> استاد بنظرتون باید به فکر عوض کردن حوزه کاری باشیم
-> نه"
"-> سلام دوستان من چند هزارتا عکس شبیه عکس بالا دارم البته بعضی عکس ها اون قسمت وسط تصویر که دیتای اصلی هست حالت مورب داره من میخوام بالا و پایین اون تصویر اصلی رو کراپ کنم و فقط همون قسمت تصویر اصلی رو داشته باشم به حالتی که بزرگترین تصویر اصلی سایزش در بیاد و بعد بقیه تصاویر هم به همون مقدار کراپ بشن ممنون میشم راهنمایی م کنین
-> سلام لبه یابی انجام بدهید و براساس موقعیت پیکسل های کانتور لبه بزرگترین تصویر بدست می آید
-> ابتدا با فیلتر کنی یا فیلتر لبه یابی ساده افقی لبه یابی کنید و سپس برای رسم باکس از این داکیومنتیشن که دوستمون فرمودن بهره ببرید"
"-> سلام خدمت استاد و دوستان شب بخیر مسئله جدید مسابقات کگل یک مسئله multivariate هستش که ۷ تا تارگت با نام های Pastry Z_Scratch K_Scatch Stains Dirtiness Bumps Other_Faults داره که انواع خرابی های فولاد هستن چالش تارگت های Pastry و Bumps و Other_faults از بقیه بیشتره روی بقیه میشه به دقت های خوب و بالایی رسید من از ساده ترین حالت یعنی ترین مدل روی هر کدوم از تارگت ها به صورت جداگونه شروع کردم و اونطور که خودم تست کردم و نوت بوک های بقیه رو دیدم بنظر میرسه xgboost روش بهتری باشه روی این دیتاست رنگ قرمز دقت مدل LGBM هستش همچنین یک دیتاست اورجینال هم هست که می تونید اضافه کنید به دیتاست ترین بالاترین دقت بدست اومده تا الان 089706 هست امیدوارم شما هم شرکت کنید و ایده ها و نظراتتون رو مطرح کنید
-> اسم نوت بوک چیه
-> عالی مهدی جان پیامت رو توی کانال پایتورچ میذاریم اگه نوتبوکت رو منتشر کردی لطفا لینکش رو بده که معرفی کنیم
-> بله حتما
-> هنوز نوت بوک رو نذاشتم کامل بشه میذارم
-> تشکر"
"-> سلام استاد وقتتون بخیر استاد یه سوال غیر علمی داشتم بنده به خاطر مشکلاتی که برای لپتاپم پیش اومده میخوام از اول روی لپتاپم ویندوز و لینوکسی که استفاده میکردم را نصب کنم و خب طبیعتا اپلیکیشن spotplayer هم پاک میشه و دوباره باید نصب کنم ولی من دوره های زیادی چه از شما مثل ماشین لرنیگ دیپ لرنیگ کامپیوتر ویژن حرفه ای دوره قبل دیپکاتالیست و و همین طور از سایت ها دیگه مربوط به موضوعات دیگه دارم الان آیا اون ها ازبین میرن با توجه به اینکه لایسنس محدود دارند راهکاری داره بشه دوره ها م را نگه دارم
-> سلام به پشتیبانی پیام بدید
-> سلام شاید این بدردتون بخوره استفاده مجدد از لایسنس بعد از نصب ویندوز این پیام خیلی خیلی مهمه حتما با دقت بخونید روزانه چندین نفر پیام میدن که بعد از نصب سیستم عامل جدید لایسنس شون به مشکل خورده ولی طبق کاری که میگم انجام بدین تا همین لایسنس هایی که الان دارید روی سخت افزاتون ست بشه و بعد نصب سیستم عامل جدید مشکلی نداشته باشه قبل اینکه مک یا ویندوزتون رو تغییر بدید 1 نسخه جدید نرم افزار رو نصب کنید 2 بعد نصب آخرین نسخه یه فایلی از دوره ها که تاحالا مشاهده نکردید یعنی تاحالا باز نشده روش بزنید تا باز بشه که سویچ بشه رو آخرین نسخه از نرم افزار اگه همه فایلهارو مشاهده کردید یه فایل رو حذف کنید رو فایل راست کلیک کنید حذف فایل بعد مجدد بذارید لود بشه مهم این مراحل رو برای همه لایسنس ها و دوره ها باید انجام بشه بعد خواستید مک یا ویندوزتون رو عوض کنید مثلا اگر لایسنس شما دارای سه تا دوره ABC هست لازمه که از هر کدوم از این دوره ها حداقل یک جلسه رو اجرا کنید فقط یک جلسه از هر دوره کافیه و اگر لایسنس های دیگه ای دارید برای همه اینا همین کارو انجام بدین حتما محل ذخیره سازی تو درایو c نباشه دیگه درایوی باشه که ویندوز روش نیست تا بعد تغییر ویندوز لایسنس استفاده بشه __ الان کل لایسنس هاتون روی سخت افزارتون ست شده و تمام __ این مراحل فقط برای تغییر ویندوز هستش نه تغییرات سخت افزاری"
"-> سلام وقت بخیر این دوره جدید opencv هنوز گروه تلگرامی نداره
-> سلام داره توی اسپات پلیر بخش پیش گفتار رو ببینید"
"-> سلام دوستان برای لپتاپ در حد ۱۲۰ میلیون بهترین لپتاپ که GPU خوب برای deep learning داشته باشه چه مدلی رو پیشنهاد میدید
-> سلام با این بودجه چرا به پیسی فکر نمیکنید
-> سلام همونطور که استاد گفتن بااین بودجه لپ تاپ اصلا بهینه نیست به هرحال اگرم خواستید بخرید فقطو فقط سمت i9 نردید چون فقط الکی پول میگیرن و پرفورمنسشون چنان تفاوتی نداره بیشتر فوکس کنید روی کارت گرافیک i7 کفایت میکنه لپ تاپای HP سری omen توی ۱۰۰ به بالا کیفیت خوبی دارن
-> ممنون چون برای ریسرچ دانشگاهی هست و باید حتما لپتاپ باشه به نظرتون چه کارت گرافیکی از همه بهتر هست جدول به روزی وجود داره که من چک کنم
-> سلام با این هزینه برای خرید لپتاپ asus سری rog به نظرم بهتره برای مقایسه هم میتونید توی گوگل benchmark مدل هایی که مد نظرتون هم سرچ کنید و مقایسه بینشون داشته باشید فکر کنم لینک های اول و دوم توی سرچ گوگل باشه مثلا یه نمونه"
"-> سلام دوستان یک سوال ازتون دارم من رشتم عمران هست و میخوام شروع به یادگیری ماشین لرنینگ و کنم بعد روند یادگیری زمانبر هست خواستم بدونم به نظرتون میشه بعد یادگیری توی ایران کسب درامد داشت و کار کرد
-> سلام به عنوان کسی که این مسیر رو رفته چنتا نکته رو بگم مهمه که از کی یاد میگیری و چی یادمیگیری دوره های زیادی تو سطح اینترنت وجود داره که ارزش زیادی ندارن ممکنه 6 ماه یک کورس رو تموم کنی و در آخر چیز زیادی به جز دات فیت بدست نیاری نمیتونی وارد این حوزه بشی و دیدگاهت کسب در آمد باشه چون فرایند زمان برهست میتونه از یک تا دوسال زمان بگیره اگه بخای که یادبگیری شاید هم بیشتر بستگی داره به زمانی که میذاری و یادگیری هوش مصنوعی تقریبا تمام وقتتون رو میگیره پیوستگی تو یادگیری مهمه نباید وقفه بندازینمیتونی پیشرفت کنی اگه پیوستگی وجود نداشته باشه توی دنیای برنامه نویسی و به خصوص هوش مصنوعی برای اینکه بخواید شروع کنید اول باید پایتون رو یادبگیرید و خوب هم یادبگیرید شاید 3 یا 4 ماه زمان بذارید و برنامه نویسی رو در سطح خوبی یادبگیرید و قدرت حل مساله اتون رو تقویت کنید شی گرایی در سطح بالا و الگوهای طراحی رو باید بلد باشین هرچقدر توی برنامه نویسی و حل مساله و ریاضیات قوی تر بشین توی یادگیری مفاهیم سطح بالاتر راحت تر هستین دکتر اشرفی توضیحات خیلی خوبی همیشه راجع به این موضوع میدن و همیشه هم تاکید میکنن که برنامه نویسی و ریاضیاتتون رو تقویت کنید هرچند که خودشون این مطالب پیش نیاز رو خیلی خیلی خوب تدریس میدن
-> چه پیام پرطرفداری
-> سلام منم رشته م عمران هستش و گفتم شاید تجربه کوچیکم به دردتون بخوره تو شرکت های عمرانی عموما راه و پل کار می کردم و اذیت بودم حقوقم بد نبود اما بالاخره اومدم بیرون و رفتم رایگان ۶ ماه کارآموزی برنامه نویسی پایتون حتی دقیقا نمی دونستم کدوم گرایش قراره برم تنها چیزی که می دونستم این بود که فاز هکری و فرانت اند ندارم حتی مدت زمان کارآموزی رو نمی دونستم و کنارش هم کلی رایگان لیبل زدم اینم اضافه کنم که وضعیت مالی خوبی هم از قبل نداشتم که بگم بیخیال درآمد ولی واقعا علاقمندی برام در اولویت بود خلاصه بعد دوره عمومی وارد داده کاوی و ماشین لرنینگ شدم و واسه دیپ دوره پایتورچ هوسم رو تهیه کردم الان هم شغلم و درآمدم از همین مسیر هستش نمیتوم بگم برنامه نویسم چون هنوز خیلی راه مونده و هر روز از همکارام یاد می گیرم موفق باشی
-> پیامتون اینطور بود که جالب شروع شد و هرچه جلوتر رفتم جالبتر و جذابتر شد برام آموزنده بود بابت مسیری که طی کردید بهتون تبریک میگم
-> فراموشی دستورات به این خاطر هست که تمرین لازم برای تثبیت مطالب و دستورات انجام نشده توی برنامهنویسی دو تا فاکتور مطرح هست 1 سینتکسها و دستورات 2 مهارت حل مساله و تفکر الگوریتمی مورد 1 با دیدن دورههای رایگان و غیررایگان به راحتی حاصل میشه اما مورد 2 با دوره دیدن بدست نمیاد نیاز به تمرین و کدزدن داره اکثرا توی همون مرحله 1 گیر میکنن و به همین خاطر کدنویسیشون همیشه میلنگه و همواره دنبال کد آماده هستن
-> دریافت این متن از شما برام خوشایند بود امیدوارم کلی اتفاقات خوب برای شما و مجموعه هوسم اتفاق بیوفته
-> بنظرم پول در هرکاری که درش مهارت داشته باشیم هست پس چه بهتر که با شناخت از توانایی هامون وقت رو مسئولانه روی علاقه مون بزاریم که اگر روزی درآمد مورد نظر حاصل نشد بگیم خب حداقل کلی خوش گذشت واقعا
-> وقتی یه نفر میخواد از خارج حوزه کامپیوتر وارد شه تقریبا هیچ اطلاعاتی نداره و فقط اگر خیلی زرنگ باشه زمان تحقیقشو برای انتخاب مسیر مناسب طولانی میکنه و بالاخره راه پیدا میکنه آذر پارسال وقتی شروع کردم ۴_۵ تا دوره پایتون رو دیدم که فقط در حد سینتکس بودن و هیچکدوم هیچ درکی از تفکر الگوریتمی و حل مساله نمیدادند و زمانی که میخاستم ۲ خط کد بنویسم حتی نمیدونستم باید چیکار کنم و چالش های زیادی داشت اما در آخر فهمیدم که باید ترکیبی پیش برم تئوری و عملی باهم حتی توی برنامه نویسی و به دنبال منبعی گشتم که خوب پوشش بده هردو مورد رو اونجا بود که باهوسم هم آشنا شدم مشکل اساسی تبلیغات گسترده سایت ها و محل های آموزشی که اصلا در حد آموزش نیستند توی سطح اینترنت هست خواه یا ناخواه آدم بعد از دیدن ۱۰ بار از یه تبلیغ به این سمت کشیده میشه که شاید اون ها هم درست بگن و میشه توی چند روز یا چند هفته یاد گرفت
-> باورم نمیشه تو هم مشکل کدزنی داشتی من برداشتم این بود که چند سالی هست کد میزنی الان خیلی هیولایی درسته تبلیغ گسترده خیلی موثر هست مثلا یک موسسه ممکن هست هزینه تبلیغاتش بیشتر از هزینه تهیه آموزش باشه یک آموزش معمولی اما تبلیغ بسیار زیاد
-> سلام عالی بود و موفق باشید دقیقا درسته دوره ها و اموزش های توخالی و بدون محتوا در سطح اینترنت زیاد شده یکی عاملی موجب موفقیت در یک زمینه میشه مسیر یادگیری درست هستش یا بقولی roadmap در هر زمینه باید دنبال مسیر درست و سرفصلها رفت بعد وارد جزئیات شد خودم تا فوق دیپلم کامپیوتر بودم کارشناسی رفتم تکنولوژی الکترونیک الان سه سال تخصصی طراحی برد الکترونیک انجام میدم اولش همه بچه های الکترونیک میگفتن که نمی تونی جلو بری و پیش نیاز نداری اما نمی دونستن که مسیر یادگیری از قبل اماده کرده بودم الان هم الکترونیک خوبی یاد گرفتم و هم کد نویسی و الگوریتم این قضیه باعث شد سمت آنالیز و طراحی زیرساختهای هوش مصنوعی برم
-> متاسفانه یکی از بدبختبها اینکه برای حفظ برنامه نویسی باید همیشه جالشی باشهیبار جالش پروژه داانشگاهیه چالش تز هستبدبختی الن برای من باید تحقیقات و کار فری لنس باشه چون کار اتوماسیون خودم درسته گاها برنامه نویسی داره ولی اصلا به این مقوله کاری ندارهچه خوب میشد همیشه چالش میبودمن الان توی diffusion برای خودم چالش ساختم و فعلا بند کردم به اون"
"-> سلام دوستان توی بحث شبکه های CNN ما تصاویر رو اول با مین مکس میاریم بین صفر و یک و بعد با داشتن میانگین و std استاندارد میکنیم میخوام بدونم وقتی دیتاهای عددی رو هم میخوایم بدیم به mlp باید هر دوی این ها رو انجام بدیم یا فقط مین مکس کافیه
-> سلام برای mlp همون استاندارد اسکیل کافیه
-> سلام آقای دکتر خیلی ممنونم فقط استاندارد درسته یا مین مکس خالی هم درسته
-> هردو تا میتونه باشه توی دوره دیپ گفتیم که اگه ورودی تماما مثبت باشه حرکت زیگزاگی در رسیدن به نقطه بهینه حین بهینهسازی رو ممکنه ببینیم"
"-> دوستان برای مشاوره ساعتی با استاد باید چیکار کنم کسی میدونه
-> سلام اگه مشکلتون جزئی هست که همینجا بگید هم من هستم هم بقیه دوستان اگه نمیشه ایمیل بزنید موضوعتون رو بگید اگه بتونم زمان تنظیم میکنیم howsammailcom در مورد این مساله سوال یا ابهامی داشتید به پشتیبانی بگید"
"-> بله با یولو هم میشه مثالی دیده بودم که طرف اومده بود با ایجاد دیتاست با ابزارهای bounding box و دادن اون دیتا ست به یولو شبکه رو اموزش داده بود
-> بله
-> اگر وجود داشته باشه قبلا نه نیازی نیست"
"-> سلام دوستان یه سوال داشتم تو کولب دارم کار میکنم اینجا من سعی میکنم یه تعدادی ماسک رو باهم مرج کنم و بعد تو درایو سیوشون کنم Merge masks and move files to train folder for img in train_imgs printimgimg printimg typetypeimg if imgendswithjpg annotation_file ospathjoinextracted_path waterPuddleannotations subfolder imgreplacejpg _0png elif imgendswithpng annotation_file ospathjoinextracted_path waterPuddleannotations subfolder imgreplacepng _0png printannotation_fileannotation_file i 0 index of annotation Check if the annotation file exist label annotation_file while ospathexistslabel label s_dpng annotation_filesplit_00i1 printlabellabel if ospathexistslabel printexist if i 0 mask annotation_file i i 1 mask nphstacklabel mask else img_path ospathjointrain_path imgs img printimg_path img_path shutilmoveospathjoinsubfolder_path img ospathjointrain_path imgs img annotations_path img_pathreplaceimgannotations printannotations_path annotations_path mask maskastypenpuint8 cvimwriteannotations_pathmask printsaved تو قسمت رایت کردن این تصویر جدیدی که ایجاد شده به این ارور میخورم error OpenCV480 1 error 5Bad argument in function imwrite Overload resolution failed img data type 19 is not supported Expected PtrcvUMat for argument img کسی ایدهای داره
-> سلام به نظر میرسه مشکل از دیتاتایپ ماسک هست فکر کنم باید تصویر unit8 باشه تا بتونه ذخیره کنه دو تا خط مونده به آخر یک دستور astype دارید که کامنت شده شاید اون از کامنت دربیاد مشکل حل بشه یک چیزی هم طلب نکرده بگم خطاب به شما هم نیست کلی میگم که شاید به درد دوستان بخوره خیلی احتمالش کم هست که اینجور سوالها توی گروهها جواب بگیرن شاید به این خاطر که کسی حوصله نمیکنه این همه کد رو توی این فضا به صورت شلوغ ببینه دو تا پیشنهاد دارم اول اینکه حتما مهارتتون رو در دیباگ و تریس کد تقویت کنید مثلا اگه توی پایچارم هستید با بریک پوینت میتونید به خوبی دیباگ کنید اگر هم از نوتبوکها استفاده میکنید دائما توی جاهای مختلف دستور پرینت بذارید و مدام خروجی رو چک کنید اینطوری که خب ببینم خروجی خط 1 درسته آره درسته خب حالا خروجی خط 6 رو ببینم و مورد دوم هم اینکه فرضا اگه خواستید چنین سوالی هم بپرسید به این فکر کنید که چطوری میتونید کد و نوشتهتون رو کوتاه و هدفدار کنید اینجوری احتمال جواب گرفتنتون بیشتر میشه مطمئن نیستم اما این پیغامهای طولانی رو با چت باتها هم مطرح کنید شاید بد نباشه
-> خیلی ممنون کامنت کردم چون حدس اولم اون بود ولی کار نکرد
-> شاید منظور شما رو بد متوجه شدم قاعدتا نباید کامنت باشه چون باید ماسک تبدیل به uint8 باشه قبل و بعد از این دستور ماسک پرینت کنید و مقادیرش رو چک کنید شاید مثلا nan داشته باشه یا مشکل دیگهای داشته باشه"
"-> سلام دوستان اگر بخواین با 100 میلیون یک سیستم برای کارهای عمیق ویژن تهیه کنید پیشنهاد تون چی هست
-> Croe i7 gen13 32G ram 1T SSD RTX 4060 16G دیشب قیمت گرفتم با پاور و کیس ومادربورد شد ۹۸ تومن
-> سپاس از پاسخگویی تون
-> سلام توی گرافیکا ترجیح اینه یا rtx بالاتر از ۴۰۶۰ خریده شه یا ۳۰۷۰ و سریای ۳۰۹۰ و داخل یوتوب مقایسه های زیادی هست که حتس ۳۰۷۰ قویتر از ۴۰۶۰ عمل میکنه چه برسه ۳۰۹۰ کلا ۴۰۶۰ ورژن یکم بهینه شده ۳۰۶۰ هست و تغییرات انچنانی نداره
-> خواهش میکنم کاری نکردم
-> ممنون از راهنمایی تون
-> درست هست ۴۰۶۰ نسبت به قیمت ارزش خرید نداره"
"-> سلام یه سوال پروژه های دیپ رو فقط در کولب انجام میدین یه سرور که GPU ارائه بده و مناسب باشه معرفی بفرمایید لطفا
-> کگل
-> 
-> vastai اونجا علاوه بر RTX 4090 حتی L40 هم هست موارد ضعیفتر هم داره و در کل هزینههاش خیلی کمتره برای پرداخت هم که ایرانی کارت هست
-> دوره دیپ منظورتون هست توی دوره کولب و سیستم شخصی سیستم شخصی هم صرفا برای پایداری در ضبط نه به خاطر قدرت سختافزاری وسط ضبط کولب جیپییو رو ازمون نگیره
-> سلام باید یکسری تصویر نمونه بررسی بشه اینطوری نمیشه گفت
-> سلام فکر کنم تصاویر بررسی جوش رو با اشعه ایکس انجام میدن ابتدا تصویر برداری اشعه ایکس میکنن و بعد با تصویر با رزولوشن بالا در نقاط شکستگی دیتاست رو ایجاد میکنن البته خودم کار نکردم قبلا دیدگاهی داشتم"
"-> سلام وقتتون بخیر امیدوارم حالتون خوب باشه استاد من یه پیشنهاد داشتم در مورد دورههاتون اگه صلاح دونستید لطفا روی تبلیغات بیشتر کار کنید شما که اینقدر زحمت میکشید و بصورت جامع و شفاف ویدئوهای معرفی رو آماده میکنید علاوه بر مخاطبای خود هوسم جامعه هدف رو دانشجوهای هوشمصنوعی دانشگاهها هم قرار بدین لطفا بطور خاص دانشجوهای هوش مصنوعی رو مثال زدم چون بنظرم دورههاتون باعث افزایش دانش و ارتقای مهارتشون میشه و براحتی مسیر مناسب برای پیشرفتشون مهیا میشه من خودمم غبطه میخورم که کاش چندسال زودتر با آکادمیتون آشنا شده بودم هر چند که باز خدا رو شکر میکنم که آشنا شدم
-> سلام لطف دارید ممنونم بله درست میفرمائید یا تبلیغ نمیکنیم یا اینکه خیلی کم تبلیغ میکنیم بابت پیشنهاد ممنونم"
"-> من با اینکه بعد از ثبت نام تو دوره دیپ کاتالیست نتونستم باهاتون همگام بشم و دوره رو پیش بیام به خاطر مشغله زیادی که دارم ولی به خاطر اعتیادی که به ثبت نام تو دوره هاتون تو این پنج شش سال پیدا کردم که اونم به خاطر کیفیت بالاشه دوره OpenCV تون هم شرکت کردم چون آموزش هاتون مثل یک دیکشنری میمونه که هر وقت ادم نیاز داره به یک مبحثی میتونه بهش رجوع کنه
-> محمدامین عزیز از اینکه دوستای قدیمی هنوز توی این گروه هستن واقعا خوشحال میشم ممنون"
"-> سلام وقت بخیر دوره opencv با بینایی کامپیوتر حرفه ایی چه تفاوتی داره
-> حرفهای با الگوریتم دیپ لرنینگ هست و این با open cv و اینکه پردازش تصویر که پیش نیاز حرفهای الآن هست گفته میشه البته قراره دورهی بینایی کامپیوتر با الگوریتم دیپ لرنینگ جدیدی بعد از این دوره احتمالن تابستون برگزار بشه
-> توی این دوره درمورد پردازش و بینایی کامپیوتر کلاسیک قبل از دیپ لرنینگ یعنی قبل از سال 2012 صحبت میشه بعدش دورههای بعدی رو طبق هرم خواهیم داشت ویدئوی معرفی دوره توی سایت اطلاعات خوبی بهتون میده
-> سلام استاد دوره ي يادگيري تقويتي اين ماه برگزار ميشه
-> سلام بله انشالله
-> چه عالي اين حوزه ي يادگيري تقويتي به شدت علاقه دارم ميشه يكم راجبش توضيح ميديد ممنون ميشم
-> الان سخته یه کوچولو صبر کنید فصلش بیاد
-> این تصویر رو مشاهده کنید
-> بله پس بینایی کامپیوتر تابستون میاد جای دوره فعلی
-> بله دقیقا
-> استاد داخل دوتا ویدیو هم درباره یه سری مسائل توضیح دادن دیدن اونام خیلی مفیده داخل سایت بخش opencv هستن
-> سلام ممنون که گفتید آره اون ویدئو رو منتشر کردیم که چالش افراد کمتر بشه چون سه تا فاکتور هست که مانع پیشرفت بچهها میشه ریاضی پایتون و استراتژی اشتباه در یادگیری
-> مثل فیلمهای سینمایی معروف برمیگردین به قسمتهای آغازین
-> آغازین چرا از پایین به بالاست دیگه
-> نه درست میگین فکر کردم منظورتون از بینایی توی دل همین دیپ لرن بود فقط ویژن کاتالیست چه دوره ای هست دقیقا
-> مشابه دیپ کاتالیست ولی نسخه بینایی کامپیوترش دیپ کاتالیست دوره پروژه محوره که 6 7 پروژه دیپ لرنینگ داره
-> الان دیپ کاتالیست برگزار شده
-> بله گویا ما اطلاع رسانیهامون خیلی بده که حتی رفقامون هم از دورهها خبر ندارن
-> نه من متاسفانه سرم خیلی شلوغ بود ووقت نکردم سایت رو ببینم الان دیگه وقتیکه میخوام همه اینها رو ریویو کنم حتی توی اتماسیون هم گروژه با دیگ تعریف کردم برای بهبود پارامترهای برنامه PLCولی ظاهرا هنوز صنعت و شرکتها مقاومت دارن برای این چیزا و داریم از دنیا عقب و عقب تر میفتیم
-> برای یادگیری ماشین هم چنین دوره ای در نظر دارید
-> بله علاوه بر این دورههای بینایی ماشین کاتالیست و nlp توی برنامه هست روی اینها پشت صحنه کار میکنیم موقعی که به کیفیت مطلوب برسه برگزار میکنیم
-> استاد بی صبرانه منتظر دوره NLP هستم لطفا یکمی توی اولویت قرارش بدین حسابی هایپم براش بالاست اگر هم براتون مقدوره یکم سرفصلاش رو هم بفرمایید مثلا مباحث و روش های آماری و کلاسیک رو هم در بر میگیره یا نه و یا مثلا مباحثی مثل مدل های زبانی تا چه عمقی قراره بررسی بشه لطفا اگر کتابخونه transformers هم در مباحث وجود داره یکم توش عمیق بشین مثلا چطور بیام کارهای اساسی کنیم مثلا چطور مدلها رو می تونیم دستکاری و اختصاصی کنیم خیلی ممنون
-> سلام خب اول بگم ممنونم پیامتون خیلی حس خوب به من منتقل کرد همه اینهایی که گفتید رو توی nlp میخواییم بگیم یعنی اون بخش قبل دیپ لرنینگ رو هم خواهیم گفت بخش یادگیری عمیقش رو در سطح دورههایی مثل یادگیری ماشین و عمیق میگیم یعنی سعی میکنیم دید خوبی از مبانی داشته باشید همیشه مبانی برامون مهمه دیگه در مورد ترنسفورمرز یا کلا هاگینگ فیس هم که خب باید بگیم هرچند ما برای یادگیری عمیق پایتورچ و برای قبل یادگیری عمیق هم nltk رو هم استفاده میکنیم در مورد زمانش هم الان آماده نیستیم داریم پشت ضبط صفر انجام میدیم کلا میدونم که دوستان خیلی منتظر میمونن و من عذر میخوام ولی خب جمعآوری و تولید محتوا کار خیلی زمانبری هست"
"-> دوره OpenCV از پردازش تا بینایی 60 درصد تخفیف برای ثبتنام زودهنگام فقط تا 18 اسفند شروع دوره 18 فروردین 1403 مدرس دوره سیدسجاد اشرفی اطلاعات بیشتر و ثبتنام کلیک کنید راههای برقراری ارتباط با ما 09025469248 howsammailcom
-> سلام استاد وقتتون بخیر جسارتا این دوره تا کی به اتمام میرسه
-> سلام احتمال زیاد اواخر تیر
-> با سلام و عرض تبریک سال جدید ببخشید من گرفتار این ویروسها بوذمامکانش هست سیلابسهای دوره open cv رو بفرمایید و اینکه تا کجا قرار باشه همون سیلابسش و روزهای برکزاری دقیقا کی هست اگه مشخص شده"
"-> سلام استاد بابت تخفیف خیلی ممنون واقعن خیلی مناسب بود و واقعن من خودم شرمنده شدم دنبال تخفیف بودم درواقع همون قیمتهای سال پیش رو با ماها دارید حساب میکنید منکه الآن ثبت نام کردم امیدوارم دوره ی خوبی باشه خوشبختانه طبق پیش نیاز میتونم هماهنگ شرکت کنم
-> سلام خواهش میکنم من از شما و سایر دوستان بابت حمایتها و تشویقها ممنونم
-> سلام عالی من هم خریدم تشکر
-> سلام ایشالا دوره خوبی در کنار هم داشته باشیم"
"-> سلام استاد میشه بفرمایید چجوری میتونیم aws رو یاد بگیریم وبسایتی که بشه راحت اجراش کرد اصلا کی باید بریم سراغش منظورم کیا باید یاد بگیرنش
-> 
-> دست شما درد نکنه
-> سلام عالی بود برای کار در این زمینه هم می توان از azure مایکروسافت قسمت AIML استفاده کرد کتاب اموزشی Mlops کار با اون محیط وجود داره
-> این لینک رو تا آخر ببینید نکات جالبی داره
-> جی پی یو رایگانش از جی پی یو رایگان گوگل کولب قوی تره
-> کدوم"
"-> سلام کسی از استفاده کرده یه توضیحاتی بده ممنونم
-> توی Examples برید همه چی رو نوشته با جزئیات
-> با محیط cloud ai google فرق میکنه"
"-> سلام من یکی از متغیرهای ورودی که میخوام به شبکه عصبی mlp بدم مقادیرش فقط ۵ تا عدد ۳۲ و ۶۴ و ۱۲۸ و ۲۵۶ و ۵۱۲ هستند این ها رو چطور نرمال کنم و به شبکه عصبی بدم
-> سلام به نظر میرسه همون نرمالیزه یا استاندارد کردن خوب باشه
-> سلام خیلی ممنونم آقای دکتر یعنی از فرمول مین مکس استفاده کنم
-> تقسیم به ۵۱۲ کن
-> یا minmax scaler یا standard scaler باید تست کنید ببینید کدوم بهتره البته این رو هم بگم که روش انتخابی ما به اون مفهومی که پشت این فیچر هست ربط داره یعنی باید درک درستی هم از اون فیچر و این اعداد داشته باشیم"
"-> سلام استاد من ازپشتیبانی پرسیدم که تقویتی کی منتشر میشه گفت آخر اسفند خواستم بدونم که دیپ ار ال هم منتشر میشه یا فقط ار ال
-> سلام یک فصل از یادگیری ماشین و یک فصل از یادگیری عمیق مربوط به یادگیری تقویتی میشه ما اینجا یک مقدمهای بر یادگیری تقویتی خواهیم داشت از دیپ RL هم میگیم اما چون دیدم توی خصوصی چند بار درمورد این فصل سوال پرسیدید لازم هست یک نکته بگم ببینید کل مطالبش نهایتا 5 ساعت میشه یادگیری تقویتی اصلا موضوعی نیست که با 5 ساعت بشه بهش تسلط پیدا کرد یک دوره جداگانه میطلبه یک فصلی شبیه فصلهای 10 11 12 یادگیری عمیق هست که مربوط به بینایی متن و صوت هستن ما صرفا میخواییم شما رو با شاخههای مختلف آشنا کنیم و بعد خودتون برای متخصص شدن سراغ دورههای تخصصی برید حالا اگه نیاز جدی به یادگیری تقویتی دارید میتونید توی پیامهای پین شده کورس امیررضا رو نگاه کنید امیررضای عزیز دانش و تجربه بسیار خوبی در یادگیری تقویتی دارن"
"-> سلام وقت بخیر توی ترنسفورمرها یک اسکیپ کانکشنی بود که خروجی فیدفوروارد و همچنین MHA رو با ورودیشون جمع میکرد فکر میکردم بخاطر انتقال اطلاعات مکانی باشه این اسکیپ کانکشن ها ولی توی یه بلاگی نوشته بود برای جلوگیری از محو گرادیانه دلیلشو میشه بگید استاد
-> فقط یک دلیل که نداره حفظ اطلاعات لایه قبلی محو گرادیان راحتتر ترین کردن و خیلی دلایل دیگه میتونه وجود داشته باشه با الگوگیری از رزنت اینجا هم اضافه شده توی شبکه رزنت بود که اولین بار این تکنیک پیشنهاد شد
-> ممنون برای انتقال اطلاعات هیچ واضحه ولی برای محوگرادیان من سرچ کردم ریاضیات خاصی پیدا نکردم که قشنگ درکش کنم چرا واقعا از محو گرادیان جلوگیری میکنه چون صرفا یک تنسور رو با تنسور دیگه جمع میکنه
-> خب فکر کنید مسیر اصلی که شامل لایههای فولیکانکتد و غیره هست گرادیانش صفر بشه ولی اون مسیر شورتکات گرادیان رو بدون مشکلی به لایههای قبلی برمیگردونه البته کلا توی رزنت این لایه رو برای محو گرادیان نیاورده بود چون محو گرادیان با لایه نرمالیزیشن حل شده اونجا میگه که degradation داریم با افزایش تعداد لایهها عملکرد مدل افت میکنه اما به هر صورت داشتن مسیر شورتکات با تحلیل ریاضی میتونه مشکل محو گرادیان رو حل کنه حالا فارغ از اینکه ما به شکل دیگه محو گرادیان رو حل کرده باشیم از دید ریاضی میگم توی lstm همین هست ما یک مسیر شورتکات داریم که سر راه این مسیر المانی نیست و شبیه به همین شورتکات هست و همین عامل باعث کاهش محو گرادیان میشه چون rnnها محو گرادیان داشتن"
"-> دانش پردازش تصویر چقدر نیازه
-> هیچی دیگه قراره با این دوره دوستان پردازش تصویر یاد بگیرن
-> عالیه امیدوارم زودتر نام نویسی شروع شه"
"-> استاد ببخشید پروژه من در مورد بازی سازی هستالبته هنوز استارت نزدم میخواستم بدونم پردازش تصویر و بینایی در بازی سازی چقدر مورد توجه قرار میگیره و اینکه در دوره های جدید آیا میشه با دید و محوریت بازی سازی و پروژه های مربوط به اون آموزش دید
-> بازی سازی یعنی چی
-> سلام ببخشید من بدون اجازه استاد دخالت کردم البته اگر اشتباه متوجه نشده باشم بله من دیدم که cvzone که یک کتابخونه پردازش تصویر هست برای بازیسازی کاربرد دارد و اگر اشتباه نکنم mediapipe هم همینطور
-> game programming و کلا توسعه بازی
-> نمیدونم چرا پروژهتون همه چی داره
-> پروژه های هوش مصنوعی بیشتر حول شخصیتهای بازی پیاده سازی میشه برای مثال بازی در طول طی شدن مراحل توسط گیمر سطح بازی رو سنجش میکنه و سطح یا level بازی رو دینامیک تغییر میده
-> فکر کنم به علت اینکه هنوز شروع نکردیم به طراحی و الان در مرحله ایده پردازی هستم با دوستم داریم تمامی راه های ممکن برای پیاده سازی را بررسی میکنیم تا به بهترین ایده که رسیدیم پیاده سازی را دنبال کنیم نمیدونم کار درستی هست که وقت زیادی را صرف بررسی کردن موضوعات مختلف برای گسترش پروژه ام داشته باشم یا نه از یادگیری تقوتی گرفته بود تا پردازش و بینایی کامپیوتر و واقعیت مجازی وو دلیل همه چیز داشتن و به نظر رسیدن اینکه پروژه همه چیز داره اینه ولی خودم اصلا راضی نیستم چون هنوز به مرحله پیاده سازی نرسیدیم
-> خب پروژه رو کوچیک تعریف کنید و از شاخ و بالهاش بزنید تا در سطح تواناییهای شما بشه و بتونید پیادهسازیش کنید داشتن استراتژی درست در انجام پروژه بسیار مهم هست با چیزهایی که میبینم استراتژی شما ایراد داره"
"-> سلام دوستان فهرست مطالب دوره OpenCV که 90 درصد راه رو رفته رو فرستادم که ببینید و نظر بدید فصل اول مقدمهای بر بینایی کامپیوتر فصل دوم تصویر دیجیتال فصل سوم ویدئو فصل چهارم تبدیلهای هندسی فصل پنجم هیستوگرام فصل ششم فیلترگذاری مکانی فصل هفتم فیلترگذاری فرکانسی فصل هشتم پردازش تصاویر باینری فصل نهم استخراج ویژگی و بازشناسی فصل دهم تشخیص اشیا فصل یازدهم سگمنت تصویر فصل دوازدهم پردازش ویدئو فصل چهاردهم سختافزار امبدد دیوایسها فصل سیزدهم نور و دوربین فصل پانزدهم وب اپلیکیشن با بینایی کامپیوتر توی این دوره دنبال دیپ لرنینگ نباشید این دوره روی پردازش تصویر و بینایی کامپیوتر کلاسیک قبل دیپ لرنینگ تمرکز داره توی این دوره نگاه عملی و حل مساله در قالب یکسری مینی پروژه ساده خواهیم داشت همه فصلها هم تئوری دارن و هم کدنویسی
-> استاد ثبت نام کی هست
-> احتمالا از شنبه 90 درصد کارها انجام شده میخواستم قبل از انتشار رسمی نظرتون رو بدونم و بعدش دیگه نهاییش کنیم
-> سلام آقای دکتر از الگوریتم های اروژن و دایلیشن و کانتور ها هم بگید خیلی خوب میشه
-> یا همچین چیزایی که کاربردی بودنشون محرزه
-> آره اینها رو میگیم فصل پردازش تصویر باینری و فیلترگذاری مکانی اون مطالبی که گفتید پوشش داده میشه
-> 
-> استاد بخش استخراج ویژگی یعنی میخوایین LBP Hog Sift و بگین
-> اینها رو توی فصل استخراج ویژگی و بازشناسی میگیم
-> درسته متوجه شدم پس کلا بر مبنای کلاسیک هستش به نظرتون چند ماه این دوره طول میکشه که بعدش بینایی کامپیوتر مدرن آغاز بشه
-> در مورد فصل چهاردهم درباره برد های انویدیا هم گفته میشه اینکه چطوری بشه راه اندازیشون کرد و مدل آموزش دیده او چطور سبک کنیم برای اجرا
-> حداقل چهار ماه طول میکشه حدود 2 3 فصلی هست که میخوام بیشتر از یک هفته براش وقت بذاریم
-> متوجه شدم احتمالا همین فصل الگوریتم های استخراج ویژگی زمانبر باشن باز دستتون درد نکنه من حس میکنم خیلی کامل هستش هنوز به ذهنم نکته ای نرسیده که بتونم بگم
-> استاد فقط یه سوال دیگه هم داشتم با توجه به اینکه اینکه کامپیوتر ویژن قبلی برای اونایی که ثبت نام کردن حفظ میشه تخفیف یا مزیتی برای ثبت نام در سه دورهی جدید دارند یا خیر
-> رزبری پای رو میگیم ما برد انویدیا نداریم دوست داشتم که بررسیش کنم اما خب در دسترس نیست به خاطر کمیاب بودنشون غیرواقعی هم دارن باز اگه پیشنهادی یا نظری در این خصوص دوستان دارن حتما بگن
-> محمد ما دورهها رو موقع معرفی با 60 درصد تخفیف میذاریم این بیشترین میزان تخفیف هست حالا هزینش که شنبه بیاد میبینید کمه
-> خیلی ممنون نه اینو میدونم چون گفته بودید بیشتر توضیح میدید پرسیدم
-> بغیر از تخفیفها همینکه روره رو تقسیم کردید خودش عالیه
-> سلام عالیه خصوصا ۳ فصل آخر و فصل نهم اولین نفر شرکت میکنم انشالا
-> سلام با استفاده از الگوریتمهای یادگیری ماشین کلاسیک مثل svm و ویژگیهای که از تصویر با الگوریتمهای کلاسیک استخراج شدن مواردی مثل تشخیص اشیا و سگمنت انجام میشه
-> سلام تصاویر دیجیتال منظورتون تصاویر پزشکیە در این دورە چقدر بە آنالیز تصاویر پزشکی پرداختە شدە
-> هم بله هم خیر الگوریتمها متنوع هستن دیگه
-> نه تصویر دیجیتال همین تصاویر روزمره هست همین تصویر رایج که اساس پردازش و بینایی رو تشکیل میده آنالیز پزشکی هیچی ولی از دل سوالتون یک پیشنهاد خوب در اومد
-> سلام استاد وقت بخیر امکانش هست مباحث کالیبراسیون دوربین ها هم به دوره اضافه بشه
-> سلام فصل نور و دوربین
-> خیلی عالیه ممنون
-> درود استاد عالیه در فصل سگمنت تصاویر ازچ داده هایی استفاده می کنید آیا از تصویرهایی که لیبل دارند استفاده می کنید یا از تصویرهایی استفاده می کنی که خودتون لیبل دارش کنید
-> سلام چه اهمیتی داره
-> سلام وقت بخیر ببخشید من این سوال رو میپرسم با وجود روش های قدرت مند بینایی ماشین نیاز به استفاده از الگوریتم های سنتی پردازش تصویر داریم به جر کارهایی که صرفا برای پیش پردازش استفاده می کنیم در حال حاضر دانشجو دوره بینایی کامپیوتر هستم
-> سلام میخواستم یه پیشنهاد کلی بدم و اون این هست که اگه دورهها از نظر تدریس گسسته بشه عالی میشه مثلا وقتی کسی میخواد فقط یک موضوع رو از دوره دنبال کنه بتونه بدون نیاز به یادگیری سایر فصول دوره این کارو انجام بده برای این کار میشه مطالب جنرال رو که برای تمام فصول یکسان هستند رو توی یک یا چند فصل قرار داد و دانشجویان اول اون فصل رو نگاه کنن بعد بره به فصل مدنظرش البته این حس رو دارم که شما نمیخوایین اینجوری باشه و میخوایین کامل یاد بدین اما از طرف دیگه باید محصولات بر اساس نیاز جامعه هدف هم باشن
-> سلام استاد ردیابی اشیا داخل این دوره بررسی نمیکنید الگوریتم ژنتیک چطور
-> سلام الگوریتم ژنتیک که جز مباحث بینایی نیست ردیابی رو توی فصل پردازش ویدئو میگیم
-> منبعی برای اموزش الگوریتم ژنتیک سراغ دارید
-> سلام و خدا قوت به شما استاد عزیز و تیم هوسم یه پیشنهاد کلی داشتم در مورد رویکرد تدریس مباحث در دوره ها اونم اینه که به صورت کل به جز وارد مباحث شیم به باور منم وقتی که از جز میام به کل مباحث مهم توی جزییات گم میشن و باعث سردگمی میشند ممنونم از شما
-> سلام من دو سه ماهی درگیر این مساله بودم و میخواستم قانع بشم که هنوز به کلاسیکها نیاز داریم دلایل من 1 پروژههای عملی زیادی داریم که با کارهای بسیار ساده پردازش تصویر به جواب میرسن فکر کنید خط تولید یک کارخونه که شیرخشک تولید میکنه میخواد چک کنه که داخل قوطیها قاشق قرمز رنگی هست یا نه 2 در دیپ لرنینگ دقت خوب داریم و شکی در این نیست اما باید هزینه هم بکنیم 3 ما جعبه ابزارمون رو کامل میکنیم هم مدرن هم کلاسیک به یک مساله جدید رسیدیم بسته به ابعاد مختلف اون مساله از ابزار مناسب استفاده میکنیم 4 کلاسیک و مدرن در تضاد با هم نیستن در خیلی از مسائل ترکیب اینها ما رو به جواب میرسونه مثلا سگمنت تصاویر و کارهای پسپردازشی روی نواحی سگمنتشده 5 دونستن اوپنسیوی ارزشمنده 6 دونستن پردازش تصویر و بینایی کلاسیک درک شما رو در حوزه بینایی کامپیوتر عمیقتر میکنه مثلا ارتباط مستقیم فیلترهای مکانی با شبکههای کانولوشنی
-> سلام یعنی چی متوجه نمیشم
-> نه
-> درسته بعنوان مثال روی پروژه ای که بصورت واقعی مشغول به کار در صنعت هستم بعنوان پبش پردازش از روش های حذف نویز سنتی استفاده کردم اما دانش کافی نداشتم و فقط در حد سرچ بود بماند که بعدا روش های جدید دینویزینگ اتوانکدر رو بعنوان بیس پیش پردازش استفاده کردیم مرسی بابت زحماتتون خیلی با ارزش یک نکته هم راجب دوره بینایی بگم دوره کاملیه از این جهت که مخاطب رو با مفاهیم آشنا میکنه و مخاطب برای پیدا کردن مقالات جدید آمادگی داره
-> سلام وقت بخیر یک سوال از استاد گرامی داشتم من رشتهام سنجش از دور هستش و خب کار با تصاویر ماهوارهای آیا این دوره میتونه برای پردازش این تصاویر مورد استفاده قرار بگیره
-> سلام آره فکر کنم
-> بله یکمی بیشتر بازش میکنم مثلا برای مشتق گیری از یه تابع نیازه که با مفهوم تابع و حد و چنتا دیگه آشنا باشیم اما تویه یه سطح پایین تر با یه آشنایی کوچیک با این مفاهیم میتونیم از قواعدش استفاده کنیم اما برای اینکه این موضوع رو عمیق تر درک کنیم باید با مفاهیم پایه ش آشنا بشیم الان فقط این مثال به ذهنم رسید امیدوارم منظورم رو رسونده باشم منظورم از این حرف بیشتر برای اینه که فرآیند یادگیری برای مغز و هموار تر کنیم
-> سلام پیش نیاز ها برای این دوره چی هستند بجز پایتون
-> سلام جناب استاد پردازش سیگنال هم شامل این قضیه می شود
-> سلام خداقوت تمرین آخر فصل بخشی از کارتون باشه خیلی میتونه موثر باشه اینکه جمع بندی هر فصل با تمرینات آخر همون فصل دوره بشه به نظرم به یادگیری خیلی کمک میکنه و مطالب بهتر جا میوفته
-> سلام استاد به نظرم اهمیتش اینه که ببینم اگه داده هایی رو داشته باشیم که بدون لیبل باشند رو چگونه میشه یک دیتاست رو آماده کنیم برای سگمنت کردنش
-> سلام سرفصل ها عالی هستند به خصوص سه فصل آخر ممنون که این مباحث هم در نظر گرفتید خسته نباشید یه سوال در دوره در مورد تصاویر multi spectral صحبت میکنید
-> سلام سوال پرسیدید اگه سوال هست که باید بگم بله پردازش سیگنال هم همینطور هست اتفاقا پردازش سیگنال درس بسیار بسیار مهمی هست مفاهیمی که توی پردازش سیگنال مطرح میشن واقعا مهم هست بدرد فیلد هوش مصنوعی میخوره مثلا علیت در سیگنال خیلی خوب توضیح داده میشه و
-> سلام ممنون خیر
-> سلام دوره احتمالا چه زمانی شروع میشه
-> سلام احتمالا 18 فروردین
-> سلام ضروریها پایتون نامپای متپلات ریاضی دبیرستان اختیاری کمی یادگیری ماشین شاید این یک ماه تا شروع دوره فرصت خوبی باشه که اون ضروریها تقویت بشن
-> "
"-> سلام استاد وقت عالی متعالی استاد دوره ماشین کاتالیست قراره عید رونمایی بشه
-> سلام نه عزیزم ماشین کاتالیست فعلا زیر خاکه و حتی از خاک درنیومده
-> استاد شما تبعیض زیادی بین ماشین و دیپ قائل میشید
-> ماجرا این هست که باید دوره به اون سطحی که میخوام برسه ما الان در سطحی هستیم که به راحتی میتونیم 10 تا پروژه یادگیری ماشین تعریف کنیم و دوره رو برگزار کنیم ولی این پروژههای الان اصلا در سطح خود دوره یادگیری ماشین نیست در واقع باید یک دوره رو به جلو باشه و دستاوردهای خیلی بیشتری نسبت به اون مینی پروژه توی یادگیری ماشین داشته باشه یک ایده مهمی برای دوره ماشین کاتالیست دارم که اگه الان برگزار کنیم با اون 10 تا پروژه محقق نمیشن پشت صحنه براش وقت میذارم ولی دیده نمیشن
-> خیلی منطقیه و عاقلانه همونطور ک از شما انتظار میرفت"
"-> سلام استاد وقت بخیر من یه مقاله ای رو در نظر گرفتم برای پیاده سازی توی قسمت سخت افزار نوشته با rtx 3090 ترین شده ولی من 3070 8G در اختیار دارم تو این شرایط امکانش هست همچین شبکه ای رو با gpu که دراختیار دارم ترین کنم راه درویی داره
-> می تونین با amp پایتورچ مدلتون رو روی float16 ترین کنین
-> کدوم شبکهای رو نمیشه صرفا براساس سختافزاری که استفاده کرده قضاوت کنیم باید تحلیل کنیم ببینیم روش پیشنهادی چی هست چه تعداد پارامتر داره سایز دیتاست چقدر هست و
-> ببخشید استاد کلمهی inductive bias رو من خیلی زیاد دیدم سرچ زیادی کردم و از جی پی تی پرسیدم ولی حس میکنم هنوز درکش نکردم شما اطلاعی درموردش دارید
-> قبلا توی گروه بینایی بهش جواب دادم"
"-> در ترنسفورمر وکتور های qkv با عملیات fully connected از روی هر کلمه ورودی ایجاد می شوند
-> بله"
"-> سلام من ی سوال دارم و از هرکس پرسیدم نتونسته کمکی بکنه امیدوارم شما بتونید من ی سری داده تصویر میخوام از سایت ADNI لود کنم داخل کولب با دستور wget نمیشه چون سایتش ثبت نامی هست و یوزر و پسوورد میخواد ایا راهی وجود داره ک بتونم این داده هارو مستقیا لود کنم و نخوام روی سیستم دانلود کنم و روی گوگل درایو اپلودش کنم چون حجمش یکم زیاده تقریبا ۱۵۰ گیگه
-> سلام اینجا یه چیزایی نوشته اما بستگی داره اون سایت چجوری ازتون یوزر و پسورد رو بخاد ممکنه با کپی پیست کردن یک راه حل که کار میکنه به جواب نرسید
-> ممنونم
-> سلامدیتاست من این مدلی بودتو گوگل زدم wget cityscapes که اسم دیتاستم بودبعد یه کد گیت هاب اومد که کد رو میزدم میتونستم دانلود کنمتو گوگل سرچ کن ببین گیت هابی براش پیدا میکنی ک دانلودشو با کد مقدور کرده باشه این لینک ببینمن با این کد تونستمشاید بتونی تغییرش بدی اسم سایت و یوزر و پسوردتو بزنی اوکی شه
-> سلام گوگل درایو حجمی که بهتون میده ۱۵ گیگ هست داده fmri هست
-> ممنونم
-> MRI هستش ولی میخواستم همزمان از چنتا درایو استفاده کنم
-> چه کاری میخوان روش انجام بدین میشه دانلود کنین و روی سیستم خودتون تبدیل به فایل نام پای کنین و بعد آپلود کنین تو گوگل درایو اینطوری حجم ش خیلی کمتر میشه
-> فرمت ش دایکام هست دیگه
-> میخوام پردازش تصویر کار کنم و دقت سیستمای تشخیص بیماری آلزایمرو بهبود بدم میتونم امتحان کنم اگه نشد از شما کمک بگیرم
-> بله
-> با دیپ لرنینگ میخوان کار کنین خوشحال میشم اگه کاری ازم بر بیاد
-> بله خیلی ممنونم"
"-> سلام استاد خواستم بدونم که تقویتی و تقویتی عمیق به کجا رسیده کی منتشر میشن
-> سلام شاید این اسفند بیاد
-> منظور همون reinforcement learning هستش
-> بله"
"-> سلام دوستان وقتتون بخیر در خصوص رقابت های کگل بعد از اینکه مدل رو ترین کردیم باید دیتای تست رو هم وارد کنیم و از روی مدلمون برای دیتاست تست خروجی پیش بینی رو پیدا کنیم و بهش اضافه کنیم در ابتدای نوت بوک هدر هم باید یک کد دستوری که توی کگل بود آورده بشه بعد از انجام اینکارا میتونیم نوت بوک رو آپلود کنیم درسته
-> سلام آره توی همون صفحه مسابقه سابمیت بزنید به شکلهای مختلف سابمیت داره هم میتونید نوتبوک خودتون رو آپلود کنید هم اینکه میتونید توی کگل نوتبوک بسازید و همونجا دیتاست رو به نوتبوک ادد کنید و به نظرم راحتترین کار همون استفاده از خود نوتبوک کگل هست
-> بعد از اینکه روی داده های تست پریدکشن رو انجام دادیم باید همونا رو بذاریم جای ستون NObeyesdad از فایل ساب درسته
-> آپلود شد پس از سختی های فراوان
-> نمونه کدهای بقیه رو ببینید چطوری سابمیت کردن زیاد کار سختی نیست فیچر id از دیتاست تست رو جدا کنید با داده های پیش بینی خودتون با اسم NObeyesdad بعد تبدیل به دیتافریم کنید یک فیچر id باشه و فیچر دیگه NObeyesdad که خروجی مدل شما روی داده های تست است و در قالب فایل csv ذخیره کنید و سابمیت کنید
-> اره اشتباه من این بود انکودشو برنگردونده بودم برا همین هی ۰ میشد
-> پس بالاخره مبارکه"
"-> سلام استاد در ادامه کارهایی که برای مسابقه انجام دادمدیدم دو تا از کلاس های تارگت برخلاف بقیه کلاس ها که با دقت خوبی پیش بینی شده بودنددر هر فولد ارزیابی با دقت پایین ۸۰ تا ۸۴ درصد پیشبینی میشدند بررسی کردم و نمودار توزیع وزنهای کلاس های مختلف رو که ترسیم کردموزن مهمترین فیچر توی این دیتاست محسوب میشه دیدم این دوتا کلاس کلاس overweight level I و overweight level II خیلی توزیع وزن هاشون شبیه به همه و یکی از کلاس ها مشخصادر نمودار اوتلایر داره اوتلایرهاش رو تقریبا ۳۰۰ سمپل که با کپ بالا و پایین توی دیتاست جایگذاری کردم دقت روی داده های ولید خودم تا ۹۳ درصد هم میرفت اما خب مسئله اینه که برای داده های تست لیبل ها رو نداریم بازم حالا از یکسری روش هایی استفاده کردم مثلا اومدم از داده های تست با تشابه کسینیوسی اون داده هایی که شباهت بیشتری با اون اوتلایرها دارن رو پیدا کنم اما خب منجر به دقت بهتر نشد انتظار هم میرفت چون خود تشابه کسینوسی خطا داره و باز میشه تاریکی روی تاریکی اینا رو گفتم که هم کارهایی که کردم رو به اشتراک بذارم و اگر شما یا دوستان پیشنهادی داشتید با من درمیون بذارن ممنونم
-> سلام تحلیلت عالی بود ممنون چیزی به ذهنم برسه حتما میگم"
"-> سلام ببخشید سوال بی ربط میپرسم ممنون میشم برای ویندوز یا اکستنشن کروم VPN معرفی کنید کارم واجبه نمیتونم وصل شم
-> Veepn
-> خیلی مچکرم"
"-> سلام دوستان از کجا باید بدونم برای ترین مدلم از چهcpu یا gpu ای استفاده کنم
-> سلام بستگی به سایز مدل و ورودی داره از طریق پارامتر و فلاپس مدل و سایز ورودی میشه تا حدی فهمید
-> استاد جدولی وجود داره که از اون بفهمیم چی بدرد کارمون میخوره یا تجربیه
-> تجربیه یک جاهایی فرموله کردن خوبه و جواب میده یک جاهایی هم جوابگو نیست با همون پروژههای دوره یادگیری عمیق میتونید به درک خوبی برسید
-> سلام استاد خسته نباشید وقتتون بخیر ببخشید مزاحمتون شدم استاد ببخشید برای محاسبه receptive field و effective receptive field میشه فرمولش رو بگید یا ی منبع معرفی کنید تو اینترنت سرچ کردم ولی فرمولای متفاوتی دیدم از چت جی پی تی هم پرسیدم ی فرمول جدید ارائه کرد
-> سلام فکر کنم جزئیات زیاد داره مقاله سایت distillpub عالی هست"
"-> سلام استاد لطفا ایمیل مشاوره رو چک بفرمایید درخواست مشاوره دارم ممنون
-> سلام لطفا از پشتیبانی چک کنید ایمیلها رو من چک نمیکنم"
"-> سلام استاد روز بخیر یه امکان خوب متلب قابلیت دیباگ کردنش هست با وی اس کد چجوری میشه شبیه متلب دیباگ کرد میشه اصلا استاپ پوینت بزنیم و بریم داخل کد ببینیم چه اتفاقی داره میوفته
-> بله همین کار رو با وی لس کد یا پایچارم میشه انجام داد بریک پوینت بذارید و خط به خط اجرا کنید"
"-> سلام وقتتون بخیر باشه یک سوال دارم از خدمت شما عزیزان من چند تا مقاله در زمینه ویژن دارم و حالا ی ژورنالی الزویر برای داوری بهم ایمیل زد و منم قبول کردم مقاله یسری تغییرات تو yolov3 دادن و نتایج رو با مقیاس iou نوشتن همچین چیزی ارزش چاپ شدن داره
-> سلام شما داورید دیگه ما چی میتونیم بگیم واقعا اینطوری چیزی بگیم ممکن هست حق اون مولفهای مقاله ضایع بشه"
"-> سلام دوستان ی سوال دارم ی دیتا دارم که فقط برای بضی سطرها وارد شده و برای بقیه nan هست میانگین ش رو گذاشتم تا nan ها پر بشه اما نتایجم خوب نمیشه چکارش کنم بنظرتون
-> 
-> خیلی ممنونم ازتون خیلی لطف کردین"
"-> مطابق این کد با هر بار رفرش کردن model از ابتدا لود میشه یا فقط یک بار این اتفاق میوفته
-> 
-> از خود جی پی تی بپرس یا با پست من چند تا ریکوئست بزن ببین جیپیو چقد درگیر میشه یا چقد زمان میبره پاسخ بیاد یا بالا پایین لود مدل یه پرینت یا لاگ بذار ببین دوباره از اونجا رد میشه یا با دیباگ ide چک کن از اونجا رد میشه تو ریکوئست های مختلف
-> اینو پیشنهاد داده
-> تستش کن با روش هایی که گفتم اما به نظرم یه کرش کورس فلسک رو ببین اول ۴ ساعت زمان میبره نهایتا اما کار فعلیتو خودت میزنی
-> مرسی کار دیپلوی انجام ندادن تا الان به هرحال مرسی وقت گذاشتید
-> از ترایتون هم میتونی استفاده کنی و به ترایتون بگی اینفرنس بده اونجا هم ی بار لود میشه"
"-> سلام یک مدل yolov7 آموزش داده ام و وزن های آموزش دیده را ذخیره کردم الان قصد دارم این مدل را روی سرور اجرا کنم درحالی که فقط یک بار مدل وزن ها را لود کند و همیشه در حال اجرا باشد در واقع قصد دارم هربار که کاربر از طریق وب درخواست می دهد لود کردن وزن ها اتفاق نیوفتد بلکه فقط یک بار لود شده باشد و به درخواست ها پاسخ دهد امکانش هست راهنمایی کنید چگونه میتونم اینکارا انجام بدم
-> سلام از سوکت میتونید استفاده کنید و برنامه ای بنویسید که منتظر ورودی باشه و در صورت وجود داشتن ورودی پاسخی رو هم ارسال کنه برای اپ های بلا درنگ استفاده میشه ولی فکر کنم برای شما بهتره که توی قسمت main یه اینستنس از مدل لود کنید و توی view اون مدلی که لود شده رو ایمپورت کنید و بگید اینفرنس انجام بده برای اینکار از fastapi flask استفاده کنید"
"-> سلام استاد وقت بخیر توی 3D convolution گفته شده مزیتش نسبت به 2D اینه که میتونه درراستای عمق هم فیچر در بیاره چرا توی تصاویر rgb مثلا یدونه فیلتر ۳ در ۳ با تعداد چنل برابر تصویر ورودی میزاریم و مثلا یدونه فیلتر سه بعدی باهمون ابعاد ۳ در ۳ نمیزاریم
-> "
"-> سلام استاد فکر میکنم قسمت انکد فیچرها خیلی تاثیر داره روی نتیجه نهایی من امروز با یک تغییر ساده توی قسمت انکد فیچرها افزایش دقت داشتم
-> سلام وان هات انکدینگ من نتونستم به 92 برسم
-> سلام آره استاد طبق یک نوت بوک دیگه اون فیچری که مقادیرش غیرترتیبی بود رو به جای وان هات انکدینگ از لیبل انکدینگ استفاده کردم نتایج خیلی بهتر شد اون نتیجه تنظیم هایپرپارامترها برای یک مدله الان ۲ تا مدل دیگه از همین LGBM هم که با تنظیم هایپرپارامترها دقت بالایی بدست آوردن اضافه کردم و از VottingClassifier برای ۳ تاشون استفاده کردم دقت بازم بهتر شد"
"-> در مقاله selfknowledge distillation via dropout ذکر کرده معلم بعنوان دانش آموز عمل می کند اما طبق این شبه کد دانش آموز دانش رو از کجا یاد میگیرد ظاهرا شبیه آموزش های سنتی مثل رزنت و یا من اشتباه متوجه شدم
-> 
-> خیلی ممنون واقعا لذت بخش بود اون قسمتی که فرمودید backbon هر دو نقش بازی میکنه استدلالتون اینکه دانش رو به feat منتقل میکنه و درحال یادگیری است درسته
-> بله"
"-> سلام وقتتون به خیر داشتم ویدئوهای شبکه بازگشتی را در اسپاتپلیر نگاه میکردم با مثال شبکه بازگشتی مواجه شدم ماتریس وزن Wx ابعادش اشتباه شده ماتریس ویدئو ۳ در ۲ هست در صورتیکه باید ۲ در ۳ باشه میشه لطفا یه نگاهی بهش بندازید ممنون
-> 
-> ممنون از توضیح شما و با عرض پوزش چشم حتما مواردی که گفتید را رعایت میکنم"
"-> در ضمن به دلیل دقت بالاتر در محاسبات اعشاری گزینه مناسبی برای رندرینگ و محاسبات سنگینه از اونجایی که دیپ هم محاسبات و پارامترهای زیادی داره quadro rtx گزینه مناسبی هستش اما در کل این مدل گرافیکها نسبت به rtx گرونتر هستن
-> درسته خیلی ممنون"
"-> سلام دوستان وقت بخیر من یک سوال داشتم برای gpu مدل های سری quadro برای دیپ بهترن یا سری rtx
-> سلام این مقایسه رو نمی توان انجام داد چون گرافیک quadro rtx هم داریم اما در کل به خاطر داشتن حافظه ecc و قابلیت grid از گرافیک های سری quadro rtx در سرورهای هوش مصنوعی استفاده میشه البته مقدار حافظه قابل قبولی دارن"
"-> سلام وقت بخیر استاد توی فصل دیتکشن کامپیوتر ویژن گفتید برای بررسی دقیق یولو داخل سایت دوره دارید ولی چک کردم پیدا نکردم
-> ما این رو از فروشگاه حذف کردیم بخش کدنویسیش دیگه بی استفاده شده تئوریش خوبه اگه تئوریش رو میخواید ببینید توی گوگل سرچ کنید آموزش یولو بعد از گوگل وارد هوسم بشید
-> البته تو خود سایت سرچ کنی Yolo هم فکر کنم میاره
-> بله میاره ممنون"
"-> بله
-> برای ثبت نامه"
"-> سلام دوستان اینجا کسی هست ایمیل اکادمی داشته باشه
-> دانشگاهی منظورتونه"
"-> منم به مرحله دوم مسابقات که حضوری هست دعوت شدم
-> عه چه خوب تو میتونی
-> يه سوال من قبلا شرايط قوانين وبسايت كگل رو خونده بودم يه خورده سختگير بوده كه ما تحريميم شما چجوري تونستيد شركت كنيد تو اين مسابقه
-> یه شماره مجازی بگیرید از نامبرلند میتونید شرکت کنید شما هم
-> بعد چيز ديگه نميخواد
-> نه چیزی نمیخاد شرکت میکنید سابمیت میکنید رتبه اتون رو میبینید"
"-> سلام استاد وقت بیشتری روی تنظیم هایپرپارامترها گذاشتم و همچنین از روش soft votting استفاده کردم دقت بیشتر شد الان فیچر bmi رو هم اضافه کردم دوباره دارم هایپرپارامترها رو تنظیم میکنم
-> 
-> بله استاد منم دیتاست اصلی رو اضافه کردم به داده های ترین استاد من موقع تنظیم هایپرپارامتر ها با ترین تست اسپلیت تقسیم بندی نکردم به داده های تست و ولیدیشن چون اونطوری عملکرد مدل روی داده های ولیدیشن یکمی نوسانی میشد به جاش بعد از objective از کراس ولیدیشن با ۵ فولد استفاده کردم تا میانگین دقت ها رو توی خروجی بده فقط dropfirst رو من true گذاشتم اصلاحش میکنم
-> برداشت من این بود که شاید یک ترین و تست ثابت برای همه هایپرپارامترها داشته باشیم بهتر باشه ولی خب با فولد هم میشه سید رندوم استیت ست کرد و دیگه اون نگرانی رفع میشه
-> سلام منم دیروز و امروز برای مسابقه وقت گذاشتم اینم نتیجهش ولی نیاز به بررسی بیشتر داره فعلا از روش LGBMClassifier استفاده کردم نوتبوکهای قبلی رو که بررسی کردم اونایی که رتبههای برتر رو کسب کردن از autoencoder هم توی روشهای آنسامبلشون استفاده کردن
-> به به عالی اتوانکودر رو ندیده بودم بچهها آیدیهاتونو بدید همدیگر رو فالو کنیم و امتیاز بدیم
-> "
"-> دوستان سلام وقت بخیر یه راهنمایی میخاستم اگر بخایم یه کیس ببندیم تا ۴۰ ۵۰ تومن چ چیزایی پیشنهاد میدین برای همین کارای کامپیوتر ویژنی و اصلا ایا این عدد کافیه
-> سلام ۱۰ گیگ جی پیو سری ۳۰ رو میخاید پیشنهادم حداقل ۳۰۸۰ ایسوز یا msi هست با cpu i5 11 هم میتونی به عنوان حداقلت داشته باشی پاور هم ۸۵۰ وات لازمه که گرین رو هم پیشنهاد نمیکنم احتمالا بتونید استوکشو جمع کنید و کاراتون رو تا حد خوبی راه میندازه البته یکم بیشتر از مبلغ بالا میشه
-> سلام من ی مادربورد asus tuf gaming b760 wifi d5 و cpu i5 13400 و ram d5 5200 بعنوان پایه انتخاب کردم درمورد گرافیک هم بسته به بودجه انتخاب کنید اما بنظرم اگر محدودیت مالی دارید بیس سیستم مثل برد اصلی و پردازنده رو تاجایی که میتونید قوی بردارید و کارت گرافیک رو بعدا بهبود بدید با این مبلغی که گفتید سیستم که گفتم به اضافه rtx 3060 بشه جمع کرد
-> ایا این کارت گرافیک مناسب هستشچقدر پول ببستر نیاز هستش یک چیز بهتر بسه خرید
-> ی نکته خیلی مهم بگم اگر خواستی سیستم ببندی و کارت گرافیک برات مهم بود پوسته اولیه کیس رو طوری انتخاب کن که هر کارت گرافیکی بشه روش بست من خودم این اشتباه کردم و پوسته کیسم کوچیک الان بخوام کارت مثل ۴۰۸۰ ببندم نمیشه
-> سیستمی که پیشنهادم دادم با هزینه شما اوکیه و از لحاظ سخت افزاری هم من تا الان رضایت داشتم"
"-> سلام دوستان ما در یادگیری متریک دو تا فولدر داریم گالری و prob شبکه سیامی را چطوری آموزش میدهیم که ویژگی ها استخراج شود یعنی داده تریپلت لاس از کدام پوشه انتخاب می شود باید هر کدام جدا باشند یا با هم آموزش میدهیم
-> سلام دوستان جواب این سوال رو کسی نمی دونست"
"-> سلام استاد این دوره بینایی کامپیوتر که در سایت هست با این دوره جدید که از فروردین قرار هست بذارید خیلی فرق داره من میخواستم تهیه اش کنم نمیدونم منتظر دوره جدید بمونم یا نه
-> سلام بستگی به اهدافتون داره همراه شدن با دوره جدید خیلی خوبه ولی مساله این هست که زمانبره شاید 6 7 ماهی طول بکشه که تکمیل بشه اگه قید زمان مطرح نیست دوره جدید درغبراینصورت همین دوره رو ببینید محتوای دوره حاضر سطحش بالاست دوره خوبیه و آپدیت کردنش به معنای بد بودنش نیست"
"-> سلام و عرض ادب کسی هست که مقاله ای در حوزه لرنینگ تا حالا شبیه سازی کرده باشه ممنون میشم که مقاله رو هم در اختیار من قرار بده
-> سلام برای رپلیکیت دانشگاه میخواید
-> سلام بله"
"-> دوستان کسی کورس خوب در حوزه سیگنال و سیستم سراغ داره
-> سلام دورهی سیگنال و سیستم دکتر بابایی زاده رو از مکتب خونه ببینید
-> دوره خود اپنهایم رو از یوتیوب ببینین کتابش البته بنظرم کافیه خیلی خوبه
-> برای اپنهایم ترجمههای خوبی تو بازار هست کتابهای کنکوری سیگنال و سیستم هم خوبن
-> سلام وقت بخیر کتاب سیگنال سیستم تقدسی یکی از بهترین کتابای سیگنال سیستمه هم کنکوریه هم پوشش خوبی از مطالب داره
-> متاسفانه ترجمه جبه دار دیگه تو بازار نیست
-> اتفاقا همین ترجمه خوب بود چرا دیگه نیست میدونید
-> چند وقت پیش میخواستم اینترنتی بخرم جایی پیدا نکردم سایت ناشرش که دانشگاه تهرانه هم ناموجود بود توی کتابفروشیا شاید باشه"
"-> سلام دوستان علت این نوسان loss چی هست
-> اینجوری نمیشه نظر قطعی داد اطلاعاتی درباره مساله نداریم اما بهصورت کلی این مقدار نوسان رو خیلی جدی نمیگیریم
-> ممنون از راهنمایی شما"
"-> سلام برای چک کردن پشت هم بودن دو جمله چه روشی میشه انجام داد لیبل های ما یا درست هستن یعنی دو جمله پشت هم هستن یا غلط که ترتیبشون اشتباهه من از بازگشتی و transformer و روش های مختلف استفاده کردم اما شبگه نمیتونه یادبگیره هیچی
-> نمیدونم همون کاری که کردید رو پیشنهاد میکنم که خب گویا جواب نداده
-> منم توی یه مسابقه هستم راجع به nlp و llm ها گه البته نفرات برتر امتیاز بنیاد نخبگان میگیره این سوالو جواب میدادم اول میشدم الان ۱۷ ام هستم
-> اوه چه حیاتی دو جمله متوالی از نظر معنایی یک نمونه مثبت و منفی رو بفرستید ببینم
-> بله حدود ۴۵ دقیقه دیگه وقته تا ثبت کنم امتیازو الان پی وی میفرستم"
"-> سلام خدمت دوستان عزیز و استاد گرامی آیا برای مدل های یادگیری ماشین مثل Lightgbm یا Catboost و XGBoost هم میشه از ساختار self distillation استفاده کرد به صورت کلی می تونه باعث افزایش دقت بشه لطفا اگر لینک یا منبعی در این رابطه می شناسید معرفی کنید
-> سلام به نظرم به سراغ چنین ایدهای نرید بهتر هست حالا آخرین رتبه چنده
-> Score نفر اول ۰۹۲۲ هستش
-> آفرین عالیه چه کارهایی کردید و چه چیزهایی رو در کدهای دیگران بررسی کردید بگید شاید پیشنهادی داشته باشم
-> از optuna برای تنظیم هایپرپارامترها استفاده کردم بحث که میکردن انگار مدل lightgbm روی این دیتاست جواب بهتری میده منم همه مدل ها رو تست کردم lightgbm جواب بهتری داد از تب نت هم استفاده کردم اما جواب خوبی نداد دقت با تب نت نهایت به ۸۸ درصد میرسید رندوم فارست به دقت ۹۰ درصد میرسید همین سه تفنگدار catboost و lgbm و xgbosst عالی بودن فقط من نتایج این ۳ تا رو هم گرفتم از آنسامبل استفاده کردم ولی دقت توی حالت آنسامبل یکمی کمتر از دقت روی خروجی مدل lightgbm به تنهایی بود توی قسمت پیش پردازش هم از استانداردیزیشن استفاده کردم بهتر از نرمال سازی جواب داد و همچنین فیچرهای باینری و ترتیبی رو انکد کردم و فیچرهای نومینال هم وان_هات کردم اوتلایرها رو دتکت نکردم فیچرهایی که corr پایینی داشتن رو حذف کردم بعد ترین کردم مدل رو دقت اومد پایین تر آیا بعد از هر تغییری مثل همین حذف فیچرهای کم اهمیت باید هایپرپارامترها دوباره تنظیم کنم
-> آره به نظرم دوباره هایپرپارامترها رو تنظیم کنید کارهای خیلی خوبی انجام دادید پیشنهادم این هست که مسابقههای قبلی که زمانش گذشته رو پیدا کنید و نگاهی به قسمت کد و دیسکاشن بندازید گاهی اوقات بعد از مسابقه اون رتبه بالاییها روششون رو میگن مثلا مسابقه سگمنت تصاویر پزشکی که توی دوره دیپ کاتالیست داشتیم نفر اول دوم سوم و اومدن روش کارشون رو گفتن بازم اینجا بیایید صحبت کنیم هم از شما یاد میگیریم و هم اگر چیزی به ذهنم برسه میگم
-> راستی feature engineering رو هم مدنظر داشته باش اگه درک خوبی نسبت به ویژگیهای دیتاست داشته باشی میتونی ویژگیهای جدید از روی ویژگیهای کنونی بسازی مثلا وقتی قد و وزن رو بهت دادن میتونی با فرمول bmi مقدار BMI هر فرد رو حساب کنی و اون رو به عنوان یک فیچر جدید به دیتاست اضافه کنی هرچقدر از این نوع ویژگیها بیشتر خلق کنی بهتر"
"-> سلام استاد من یه مدل یولو آموزش دادم که مستطیل رو دتکت کنه ولی نمیتونه برای دتکت الگوهای فراکتالی که الگوها در دل همدیگه تشکیل میشن از چه شبکه ای باید استفاده کرد ممنون میشم راهنمایی کنید
-> الان اینجا چنتا مستطیل تو تصویر هست که باندینگ باکس خورده بعنوان لیبل
-> باید چهار تا دتکت کنه ولی سه تا کرده که درست باکس نکشیده
-> ۴تا میشه یکی کوچیکه ۲تا سمت راستی بصورت کاملا روهم افتاده و یکی سمت چپی
-> کوچیکع دوتا داخلی یکی هم کلی هست
-> دیتابیس عمومی استفاده کردین
-> نه خودم ساختم حدود چهل تا عکس میدونم خیلی کمه
-> پس خوب تشخیص داده شما با ۴۰ عدد عکس میخاین کوه رو جابجا کنین
-> میخوام بدونم با یولو میشه الگوهای فراکتالی رو تشخیص داد یا نه اگه میشه خب دیتا ست درست میکنم
-> شما هرچی چیزی رو بخاین دیتکت کنید میتونید از الگوریتم های تشخیص اشیا استفاده کنین حالا چالش ها باتوجه به دیتاستی که دارین میتونه متفاوت باشه و نوع رفتاری که شما برای رفع چالس انجام میدین البته شما فقط مستطیل دارین تو این شکل جز اشکل منظم هندسی تعریف میشه که بطور قطع الگوریتم های تشخیص اشیا میتونن تشخیص بدن فراکتال ها یعنی اشکالی با شکل نامنظم هندسی قطعا کارتون سختره ولی بازم قابل تشخیص هستن شاید دیتاهای بیشتری برا اموزش لازم باشه و مسائل دیگه که میتونیم باهم بحث کنیم بعنوان مثال تشخیص بافت سرطانی مثل پولیپ های روده میتونن از لحاظ شکل متفاوت باشن و شکل منظمی ندارند که جز اشکال فراکتالی درنظر گرفته میشن که با روشهای دیتکشن با دقت بالا درحال حاضر قابل تشخیص هستند"
"-> دوره آنلاین هستش یا افلاین
-> آفلاین یادگیری ماشین و دیپ کاتالیست هم آفلاین بود فکر نکنم دیگه دوره آنلاین برگزار کنیم"
"-> سلام استاد عزیزم حالا ک بحث فیدبک از دوره به میان اومده اینو باید بگم ک من خیلی وقته پیش میخواستم ازتون تشکر کنم بابت دوره هاتون ولی کلمه مناسبی برای توصیف شما شخص سجاد اشرفی و دوره هاتون ماشین و دیپ پیدا نمیکردم ولی تشکر ساده اما از صمیم قلب منو پذیرا باشید
-> سلام ممنونم"
"-> 
-> دیتا ش کجاس
-> "
"-> سلام وقتتون بخیر بنده بالاخره کد شبکه ی EEGNet خودم رو برای پایان نامم به اتمام رسوندم و داخل گیت هاب گذاشتم اجازه دارم لینک اون رو داخل این گروه بذارم تا هم نظرتونو دربارش بگید تا اگه موردی هست بهبودش بدم و هم اینکه اگر خوب بود با ستاره دادن بهش حمایت بشه
-> سلام بله حتما خوشحالم که کارتون به نتیجه رسید"
"-> بنده دارم دوره ماشین رو طی میکنم و واقعا دوره کامل و غنی هستحتی خیلی کامل تر از دوره های مستر دیتا ساینس توی اروپاطراحی و پیاده سازی این دوره بی نظیر بودهسپاس از شما
-> ممنون از اینکه این آموزشها دیده شدن و شما و سایر عزیزان اعتماد کردید بسیار خوشحالم این فیدبکها هم به ما انگیزه میده و هم کمکمون میکنه که در دورههای آینده بهتر بشیم"
"-> سلام استاد وقت بخیر من دوره ماشین لرنینگ تایه جایی و دیپ بطور کامل شمارو دیدم کامپیوتر ویژنم دارم میبینم کامپیوتر ویژن مطالبش عالیه و کیفیت خوبی داره ولی اگه کیفیت و ترتیب کدها و خصوصا اسلایدها مثل دیپو ماشین بود واقعا خیلی عالی میشد
-> سلام خوشحالم که از آموزشها راضی هستید بله درسته ساختار دوره جدید بینایی شبیه یادگیری ماشین هست
-> استاد کی آماده میشه دوره جدید
-> از فروردین شروع میشه مثل بقیه دورهها هفتهای مطالب رو منتشر میکنیم احتمالا از اول اسفند ثبتنامش رو شروع میکنیم
-> سلام استاد افرادی که قبلن ثبت نام کرده بودندورهی قبلی چطوری میتونن دسترسی داشته باشن
-> محمد جان فعلا دقیق مشخص نکردیم سعی میکنیم شرایط خیلی خوبی براتون درنظر بگیریم
-> سلام استاد استاد دوره پردازش تصویر هم خواهید داشت
-> سلام توی همین دوره بینایی پردازش تصویر هم میگیم توی این دوره رویکردمون این هست که هرچه از ابتدای بینایی نیاز هست گفته بشه
-> سلام وقت بخیر این دوره حدودا چه مدل طول میکشه
-> تخمین ما 120 ساعت هست 40 ساعت پردازش تصویر 80 ساعت دیپ مثبت 6 ماه
-> دوستان بازم اگه برای دوره بینایی جدید کامنتی نظری داشتید لطفا بگید
-> با توجه به توصیفاتی که داشتید هزینش احتمالا یکم سنگین باشه ترجیحا پارت بندی کنید مثل بینایی کامپیوتر پردازش تصویر کلاسیک و مطالب بیسیک یه پارتیشن باشن مطالب پیچیده و به روز تر یه پارتیشن به لحاظ بیزینسی خرید ها هم بیشتر میشه هرکسی متناسب با نیازش خرید میکنه
-> سلام وقت بخير استاد منظورتون از ديپ چيست ما دوره ي ديپ لرنينگ كه گذرونديم فرق دارن باهم
-> بله احتمالش هست که بخشبندی کنیم درمورد هزینه هم فکر نکنم سنگین باشه هنوز قطعی نشده و نمیتونم عددی بگم ولی معمولا حین برگزاری دوره مناسبترین زمان برای شرکت در دوره هست هم از لحاظ هزینه و هم کیفیت دنبال کردن مطالب دوره ممنون
-> متدهای بینایی رو میتونیم به دو فاز کلاسیک hand crafted based و مدرن Deep based تقسیم کنیم توی این دوره سعی میکنیم به هر دو فاز بپردازیم طبق ساعتهایی که گفتم ساعتهای بیشتری برای مبتنی بر دیپ در نظر گرفتیم اما برای کسانی که میخوان در حوزه بینایی کار کنن لازم هست از کلاسیک هم بدونن شما در دوره دیپ لرنینگ بیسیک دیپ لرنینگ رو دیدید اینجا میخوایم ببینیم روشهای دیپ برای تسکهای مختلف بینایی مثل دیتکشن سگمنتیشن ترکینگ جنریشن و غیره چطوری هستن
-> خيلي ممنون خوب توضيحاتت اميدوارم اين دوره ي شما زودتر برگزار شه با تشكر استاد
-> سلام آقای دکتر یعنی مثلا تسک های کلاسیفیکیشن یا سگمنتیشن با روش های کلاسیک رو هم میگین اگه اینطور باشه که دیگه محشره
-> سلام چقدرعالی کی دوره برگزار میشه انشالله
-> سلام محدود میگیم
-> سلام احتمالا نیمه دوم فروردین شروع میشه اوایل اسفند هم ثبت نام رو شروع میکنیم"
"-> سلام استاد وقت بخیر من دز دیروز برای لایسنس به مشکل خوردم به پشتیبانی پیام دادم هنوز سین نشده میشه بی زحمت رسیدگی بشه چون نیاز دارم از دوره استفاده کنم ولی دسترسی ندارم
-> سلام بهشون گفتم امروز بچههای پشتیبانی در دسترس نبودن عذر میخوام که منتظر موندید"
"-> سلام وقت بخیر یک مدل دسته بندی تصویر برای مجموعه داده هایی با ده کلاس آموزش داده شده است دقت آموزش و ارزیابی بالای ۹۵ درصد است در مرحله تست پس از انجام عملیات تحلیل خطا روی برخی از داده ها معمولا تصاویر نویزی طبقه بندی اشتباه وجود دارد چه پیشنهادی دارید برای اینکه مدل در برابر این داده ها هم خوب عمل کند یا بطور کلی جنرالیزیشن بهتری داشته باشد نکته روی داده های آموزشی عملیات های داده افزایی مانند کراپ چرخش نویز تغییر کنتراست انجام شده است
-> 
-> ممنونم بابت توضیحاتتون بعد از تحلیل خطا به این نتیجه رسیدیم که تصاویر blur اغلب خطا دارند یا تصاویری که به هر دلیلی کراپ شدند نصف تصویر داخل داده های کاربر هست بعد از تحلیل خطا مسله blur بودن تصاویر درصد بیشتری رو به خودش اختصاص داده بود من پیشنهادم اینکه بخشی از داده های آموزش را blur کنم و به داده ها اضافه کنم راه درستیه یا افزونگی ایجاد میشه
-> میتونه کار خوبی باشه
-> ممنونم
-> ببینید ی مسئله ای که وجود داره اینکه اگر تعداد داده های غیر blur زیادی دارین ب قولی clean data هستن اول رو اون گروه اموزش بدین شبکه را بعدش بیاین رو داده های نویزی حالا از جنس blur یا هرنوع دیگه مدل رو fine tune کنین احتمال داره این روش وزن های بهتری رو برای شبکه تون انتخاب کنه درواقع انگار شما به یه بچه ای اول تسک های اسونتر رو میدین یاد بگیره بعدش کم کم سختش میکنین در حین tune کردن شبکه سعی کنید learning rate کوچکتر هم انتخاب کنید که شبکه بتونه بهتر برسه به مینیمم ها و سریعا overfit نشه از طرفی شبکه تون هم رگولاریزه بشه مثلا جاهایی که نود زیاد دارین لایه های dropout یا رگولاریزیشن مثل L2 میتونه کمک کنه یا حتی برای fine tune کردن مدل تون میتونید از gradient clipping استفاده کنید و شاید تغییر بهینه سازتون در زمان fine tune کردن مدل میتونه موثر باشه مثلا از adamW استفاده کنید که از بزرگ شدن زیادی گرادیان ها جلوگیری میکنه و حتما برای جنرال شدن مدلت bachnormalization استفاده کن بعد لایه های cnn شاید پرفرمنس مدل از جهت سرعت رو یکم کم کنه ولی مدلت بخوبی جنرالیزه میشه بعدشم میتونی مدلت cross valid بکنی اگر میبینی تصاویرت خیلی باهم متفاوت ان اینطوری بهتر میتونی تحلیل خطا بکنی که شبکه داره چه الگوهایی رو یاد میگیره و چ اثری رو داده های تست داره این یادگیری هاااا اگر اینکارارو نکرده من حتما پیشنهاد میکنم تست کنی به قول استاد چون دقت خوبی گرفتی دیگه از اینجا به بعدش خیلی بازی کردن با دیتا و مدل خیلی مهم شاید هردرصدی که بذاری رو دقتت بشع تولید یک مقاله کرد
-> بسیار عالی ممنونم از شما"
"-> وقت بخیر استاد استاد برنامه داشتین که برای پایتون و بخش متپلاتلیب ویدئو های 3d plot و همچنین 3d contour قرار بدین خواستم بپرسم کامل نشده و اینکه opencv و بقیه برنامه های آیندتون به چه شکل هست و زمانبندیش به چه صورته
-> سلام محتواش رو آماده کردیم انشالله قبل عید به همراه پانداس منتشر میکنیم ما داریم آماده میشیم برای دوره بینایی که شامل کلاسیک و دیپ هست در بخش کلاسیک از پردازش تصویر و اوپن سی وی میگیم سعی ما این هست که از فروردین شروع کنیم"
"-> سلام دوستان یه سوال داشتم راجب سگمنت کردن اگه ماسک های ما مثل این پروژه ای که انجام دادید به صورت اکسل نباشه و به صورت عکس هایpng باشه نمیشه که ماسک ها رو روی تصویر نشون بدیم یا میشه استاد نمیدونم در این مورد سردرگمم ممنون راهنمایی کنید
-> سلام میشه شما هر n تصویر دلخواهی رو میتونید با ترنسپرنت روی هم بندازید و نشون بدید
-> ممنونم استاد"
"-> دوستان کسی با دیتاهای گیتهاب تا حالا مقاله چاپ کرده دوستان انتهای ی مقاله لینک دیتا و کدی که واسه مقاله استفاده کردن گذاشتن من اون دیتا رو برای مقاله خودم دوباره استفاده کردم و کار مقاله قبل رو تعمیم دادم مقاله هم تقریبا امادست الان خیلی نگرانم ممکنه گیر بدن که چرا دیتای یک مقاله دیگه رو استفاده کردم
-> این که کار مرسومی هست از دیتاهای یک مقاله دیگه استفاده کنید اصولا دیتا رو به اشتراک میذارن به خاطر همین کارها اینکه شما به دیتای یک کاری به صورت غیرقانونی برسید و ازش استفاده کنید مشکل داره من مطمئن هستم ولی بازم شما از دیگران بپرسید
-> خیلی ممنونم ازتون"
"-> سلام دوستان دیتا رو پیش پردازش میکنم اخرین مرحله تو ذخیره دیتا مشکل داره کسی هس راهنمایی کنه MemoryError Traceback most recent call last Cell In1 line 107 103 continue 105 proc_ecg_senkey valuelabellengthnumseries_labelnpnandata_type1i 107 npsaveproc_ecg_sentence_1500npyproc_ecg_sen
-> مموری اررور داره احتمالا رم پر میشه
-> اره دقیقا داره رم پر میشه راه حل نداره هرمقداری پردازش کرد بیاد ذخیره کنه بعدش هرقسمت یک فایل واحد کنم
-> حتما راه حل داره راه حل خاصی ندارم"
"-> استاد سلام من یه مشکلی با تابع predict از مدل تنسورفلو دارم وقتی ابعاد دیتام بالاست با اینکه توی محیط کولب پرو هستم اما RAM کم میاره و نمیتونه اجرا کنه دستور predict رو میخواستم بدونم شما یا دوستان راهی به نظرتون میرسه
-> از کلاس sequence کراس استفاده کنید و دادتون رو مثل کلاس دیتاست در پایتورچ وقتی فراخوانی میشه از دیسک بخونید و وارد رم نکنید
-> Kerasutilssequence
-> ممنون البته من از این روش استفاده نکردم تا حالا ولی بعدا حتما باهاش کار میکنم من الان یه چیزی رو تست کردم نصف سمپل هام رو دادم به تابع و تونست اجراش کنه به نظر شما اشکالی داره من هر بار نصف سمپل هلم رو بدم بهش و برام پردیکت کنه و دوباره بچسبونمش به هم
-> دادتون چیه
-> اگه برای پیش بینی بهم وابستگی ندارند اشکالی نداره ولی اگه تاثیری روی پیش بینی دارند مشکل داره
-> من یه داده 18545612048 دارم
-> 
-> نه دیگه مدل ترین شده و اینها هرکدوم یه سمپل 2048 بعدی مجزا هستند"
"-> سلام روز بخیر برای اپلیکشن های ریل تایم CV مثل تشخیص چهره یا پلاکبا احتساب اینکه بجز تشخیص کارهای دیگه ای هم انجام میشهحداقل مقدار fps قابل قبول چقدر میتونه باشه
-> به اهدافتون بستگی داره باید دقیق مشخص بشه که توی یک تسک دنبال چی هستید مثلا برای بحث فیس ممکن هست 10 فریم بر ثانیه کافی باشه
-> توی تسک فیس شناختن فرد و ترک مهمه ۱۰ فریم به ازای هر ورودی یعنی ۲۰ یا ۳۰ ورودی همزمان داشته باشم باید همینقدر رو نگه دارم الان با یه سیستم 3090 و i7 12700 فقط تونستم روی ۳۰ ورودی ۵ فریم بگیرم و واقعیتش نمیدونم چقد با حالت بهینه فاصله داره اون هم با کلی زحمت و استفاده از چندین پراسس برای موازی سازی این مقدار به دست اومده
-> 5 فریم هم بد نیست هر 200 میلی ثانیه یک فریم باید حساب کتاب کنید دیگه باید ببینید متریکهاتون چقدر افت میکنه
-> ممنون از پاسخگویی راه حلی که به ذهنم رسید و انجام دادم این بود که موازی سازی ها رو به ازای هر ۱۰ دوربین به ۵ دوربین کاهش دادم و پردازش فریم به ۸ توی هر پراسس رسید اما ضررش این بود که مصرف سیپیو زیادتر شد و نهایتا ۲۰ دوربین با i7 نسل ۱۲ میتونه بالا بیاد که به نظرم بد نیست و کار رو میرسونه"
"-> سلام در روش تقطیر دانش دوتا ابهام دارم آیا شبکه معلم عمیق در طی آموزش شبکه دانش آموزکوچک آموزش می بیند داده های معلم و دانش آموز از یک توزیع هستند
-> خیر بله"
"-> سلام وقت بخیر ببخشید توی کارای تحقیقاتی برای اینکه بهترین مدل رو انتخاب کنیم مد نظر دادن کدوم خروجی ارجحیت داره MAPE R2 MSE
-> بستگی به این داره که کارهای گذشته از چی استفاده کردن
-> کارهای گذشته همگی از یه پارامتر خاص استفاده نکردن هرکسی از یه پارامتر استفاده کرده
-> دیگه بالاخره یک یا دو متریک باید باشه که بیشتر از بقیه استفاده شده باشه ببینید اونهایی که معتبر هستن چیکار کردن بالاخره حالا توی مقاله یا پایاننامه دو تا فاکتور تا بیشتر رو گزارش کنید جای دوری نمیره شایدم کمتر گیر بدن
-> R2score خوب نداشته باشيد Mse mae بدون ارزش
-> نه منم نمیگم نداشته باشم میگم کدوم ارجحیت داره برای انتخاب مدل
-> دوتاي ديگه با ار اسكور ارزيابي ميشن ار اسكور بالاتر با mse بيشتر يه خورده بيشتر ارجح به mse كمتر با ار اسكور كمتر
-> فرموله نکنید تحلیل کنید"
"-> سلام وقت بخیر استاد تو بحث شبکههای کانولوشنی معمولا از این شبکهها برای استخراج ویژگیهای محلی استفاده میشه درست میگم حالا طول کرنل ما تا چه اندازه میتونه بزرگ باشه مثلا اگر طول دیتامون ۴۰۹۵عه طول کرنل لایه کونولوشنالمون تا چند باشه منطقی هست
-> سلام ما با گذاشتن لایههای کانولوشنی و پولینگ متوالی receptive field رو زیاد میکنیم پس تک فیلتری به قضیه نگاه نکنید روی دادههای زمانی معمولا سایز فیلتر رو بزرگتر از داده تصویری میذارن مثلا فیلتر سایز رو میتونید بزرگتر 10 یا 20 بذارید
-> استاد مثلا طول کرنل رو ۹۹ بذاریم منطقیه
-> سلام ببخشین ی سوال دارم دیتا با با پسوند m هستش از چ نوع دیتای هستش میشه جواب بدید
-> نمیدونم باید تست کنید
-> نمیدونم"
"-> سلام وقتتون بخیر ببخشید یه سوال داشتم من مدتی هست گوگل کولب بدون استفاده از vpn داخل لب تابم کانکت نمیشه برای همه این حالت شده
-> من با dns میتونم وارد کولب بشم غیر اون برای منم باز نمیکنه dns بزنین راحت تره
-> زیاد استفاده کنیم این حالت پیش میاد
-> نه تحریمه خب
-> مندیروز وصل بودم بدون vpn برای منکار میکرد
-> تحریم نیست من بدون وی پی ان به راحتی وصل میشم
-> عه نمیدونم چون برای من بدون dns نمیومد
-> برای من امروز کلا قطع شده حتی با vpn هم کار نمی کنه ولی مثلا داخل گوشی چک می کنم قشنگ بدون vpn بالا میاد"
"-> سلام ببخشید یک سوال داشتم من یک مدلی طراحی کردم و الان که میخواهم مدل آموزش دیده ام را به کاربر نهایی بدهم باید روی CPU اجرا بشه وقتی اجرا میشه بشدت با کمبود قدرت CPU مواجه میشه و هنگ میکنه میتونم تبدیلش کنم به onnx تاثیری دارد یا خیر چون مطالعه کردم نسبتا سخت هست تبدیلش بی زحمت اگر کسی در این مورد آگاهی دارد راهنماییم کنه ممنونم
-> سلام می تونید این دو مرحله رو انجام بدید ۱ تبدیل مدل به ONNX ۲ تبدیل مدل ONNX به OpenVINO دومی چارچوب بهینه شده برای پردازنده های اینتل هستش و توسط اینتل توسعه داده شده برای افزایش کارایی مدل بر روی پردازنده های اینتل هستش
-> البته مستقیم میشه مدل پایتورچ رو به OpenVINO تبدیل کرد
-> من از ماژول mmlab استفاده میکنم ولی این فقط مدل های خودش را نوشته قابل تبدیل کردن هست پس اصل کار تبدیل به openvino هست و میتونم تبدیل کنم به onnx میتونم نکنم و مستقیم تبدیل به openvino کنم خیلی ممنونم بابت راهنماییتون
-> اموزشش در یوتیوب هست
-> بله مستقیم میشه تبدیل کرد
-> در اصل OpenVINO میاد و مدل رو بصورت بهینه بر روی cpu و gpu مجتمع شده بر روی cpu های اینتل اجرا میکنه برای مثال اگر مدل رو با پایتورچ بر روی cpu اجرا کنید کندتر از حالت OpenVINO اجرا میشه بخصوص در مدلهایی که با فریم ریت سروکار دارن مثالها و اموزشها در یوتیوب موجوده
-> سلام آیا این روش برای مدل های سنگینی مثل Style GAN که روی GPU آموزش داده شدن هم جواب میده یعنی میتونه مدل رو با سرعت خوبی روی CPU اجرا کنه خیلی ممنون
-> نمیشه کامل گفت اما چون از gpu مجتمع روی cpu هم استفاده میکنه و مدل در مد inference هست می تونه نتیجه خوبی بده یکی از مثالهای کار شده با آن پیاده سازی یولو بود فریم ریت سریعتری نسبت به حالت معمولی داشت"
"-> سلام استاد دوره پردازش تصویر جدید کی آماده میشه و میتونیم تهیه کنیم
-> سلام سوال بنده هم هست سرفصلهاش رو هم بفرمایید لطفا
-> سلام احتمالا این کار رو انجام بدیم"
"-> سلام وقت به خیر یه سوال داشتم در مورد شبکه Unet آیا این شبکه براساس اون پیکسل های ماسکی که توی دیتا ست وجود داره سگمنت میکنه ویاد میگیره مثلا اگه ما یه تصویر داریم که شامل اشیا مختلفه مثلا درخت هست ماشین هست و غیره ماسک های ما توی دیتا ست ما کامل اشیا رو پوشش نداده باشه این شبکه میتونه تشخیص بده که این شی به عنوان مثال درخته
-> سلام من تا جایی که استفاده کردم و میدونم نمیتونه و پیکسل بیس عمل میکنه نمیتونه کلیت رو تشخیص بده اگه میخواین که اینکارو کنه میتونین از مدل هایی استفاده کنین که boundry هم در نظر میگیره حالا شاید دوستان تجربه ی دیگه داشته باشن
-> ممنونم ازتون"
"-> سلام من یک سوالی داشتم من میخوام عکس روتیت کنم با زاویه ای که حالا مد نظرمه ولی از opencv که استفاده میکنم کیفیت رو کم میکنه پارامتر هاشو هم دستکاری میکنم باز کم میکنه یکمی کتابخونه ای هست که به کیفیت تصویر دست نزنه
-> منم قبلا به این مشکل خوردم از PIL استفاده کردم
-> ممنون استفاده کردم ازش و از ImageBICUBIC استفاده کردم ولی بازم یه کوچولو کیفیتو کم کرد"
"-> سلام دوستان تو لود کردن دیتا مشکل دارم کسی میتونه کمک کنه دیتا هستش کدشم هستش تو اون دیتا اون فایل نیس ecgm باید چیکارکنم با پایتورچ نو شتن
-> کسی پایتورچ کار کرده"
"-> سلام استاد وقتتون بخیر من برای یک پروژه مربوط به image retrieval که کدش در گیتهاب موجود هست رو train میکنم ولی هر بار دقت هایی که میگیرم با هم متفاوت میشه مثلا یکبار شده 90 یکبار میشه 895 و یبار میشه 892 آیا این طبیعیه و آیا مثلا اگر ما بخواهیم یک مقاله بنویسیم و به همین صورت دقت ها متفاوت شوند چیزی که در مقاله باید گزارش کنیم بهترین دقت است آیا اصولا مقاله هایی که چاپ میشوند بهترین دقت ها را گزارش میکنند
-> 
-> ممنون بابت راهنمایی تون اتفاقا تو خود پروژه در کانفیگش گذاشته seed0 حالا دقیقا نمیدونم یعنی چی بله دوره دیپ کاتالیست رو دارم میشه بفرمایید کدوم جلسه اش هست"
"-> سلام استاد وقت بخیر موقع استفاده از تنسوربور روی یسری پورت ها نمیاد بالا ولی پورتو تغییر میدم اکی میشه سرچ کردم ولی چیزی سردرنیاوردم از پورت خواستم ببینم شما بااین موضوع رو به رو شدید تاحالا
-> "
"-> استاد ی سوال اگه وزن های ما از ۱ تا ۱۰ باشن پیاده سازی از صفر و رفاقت با سایکیت لرن هرکدوم وزنشون چند میشه تا بهترین دیتاساینتیس رو تو خروجی مدلمون داشته باشیم در یادگیری ماشین
-> منظورتون رو متوجه نشدم
-> اهمیتشون منظورم بود به بیان هوش مصنوعی
-> آخه محمدرضا من اصلا جمله رو میخونم حس میکنم چت جی پی تی گفته رفاقت یعنی چی بهترین دیتاساینتیس یعنی چی بهترین دیتاساینتیس چه ربطی به خروجی مدل داره وزنهای ما 1 تا 10 باشن یعنی چی
-> اگ دوتا متغیر ورودی توانایی از صفر پیاده کردن و خیلی خوب بلد بودن سایکیت در نظر بگیریم برای اینکه بخوایم تهش ی دیتاساینتیست خوب دربیاریم ازش به این دوتا متغیر چ وزنی بدیم برای کدومش وقت بیشتری بذاریم مثلا اگ بگید پیادع سازی از صفر ۲ و بلد بودن سایکیت ۸ خب این یعنی باید وقت زیادی روی سایکیت بذاریم و پیاده سازی رو تا حدی بلد باشیم تا ی دیتاساینتیست خوب دربیاد ازش
-> این سوالو از این جهت پرسیدم که به هرحال ما همیشه نمیتونیم تنها منبعمون دوره های هوسم باشه و از طرفی کلی الگوریتم هم هست که باید بنا به مسئله براش ی فکری بکنیم و خودمون دست به سرچ باشیم به خاطر همین خودمون هم باید به حد خیلی خوبی از خودکفایی برسیم از طرفی اینکه از همین فردا ی نفر بخواد فقط با سایکیتیا لایبرری های دیگ جلو ببره خودشو هم خیلی راحت نیست و نیازمند مدت زمانیه تا قشنگ باهاش خو بگیره رفاقت با سایکیت از این جهت میخواستم بدونم الان رو کدومش بیشتر تایم بذارم تا شش ماه بعد راحت تر بشه برام"
"-> سلام وقت بخیر این قسمت num workers رو توی سایت خود پایتورچ نوشته مربوطه به subproces هست که متوجه نشدم منظورش چیه میشه توضیح بدید
-> 
-> ممنون استاد فقط ادرس پیجتونو بزارید ممنون میشم چون اینستا گرام شمارو ندارم
-> 
-> استاد سلاموقت بخیر ی سری سوال داشتم و راهنمایی میخواستم من تو حوزه face detection recognition مطالعه میکنم سوال اول و کلی ام راجب این بود ک ی راهنمایی و رودمپ میخواستم برای کار تو این حوزه محیط پروداکشن و این ک چطور میشه چالش ها رو هندل کرد مثل چالش ملیت و حجاب افراد ایا ما میتونیم دیتاست افراد داخل اروپا یا امریکا استفاده کنیم یا نه مدل متوجه جنسیت و ملیت ایرانیمون نمیشه اگر نمیشه استفاده کرد پس مدل های pretrained هم نمیشه استفاده کرد مورد بعدی بحث ماسک زدن افراد تو این مورد نصف لندمارک های طرف از بین میره ایا میشه این جا از انبیه چشم استفاده کرد برای تشخیص یا نه به چه صورت باید استفاده کنیم مورد بعدی بحث تعداد زیاد افراد داخل ویدیو هستش ک مثلا بالای 100 نفر سمت چ الگوریتم هایی میشه رفت و سوال اخر این ک عملکرد مدل ما در طولانی مدت مثل روز اول میمونه یا ممکنه مدل ضعیف بشع و این مورد چطور بایذ رفع کرد ممنون
-> علیرضا جان نمیتونم به سوالت جواب بدم ببخشید
-> سلام بچ هایی که ساخته میشه و آماده میشه با هم همپوشانی دارند
-> نه دیگه اونطوری که همه چی میره رو هوا ایپوک و خیلی مطالب دیگه زیر سوال میرن
-> سلام استاد وقت بخیر درراستای همین num workers من روی سیستم خودم نه کولب عدد ۲ میزارم لود کردن بچ هام چندثانیه طول میکشه کند میشهوقتی میخوام با دستور next iter یه بچ بگیرم ولی صفر میزارم خیلی سریع انجام میشه توی این چنین موردی صلاحه ۲ باشه یاهمون صفر
-> نه عدد 2 مناسب نیست لینوکس
-> نه استاد توی ویندوز مث اینکه میگفتن توی ویندوز کار نمیکنه ولی من تست کردم الان اثر که داره چون عددشو دو گذاشتم خوب کارنمیکنه سیستمم بااون فردی که توی اینستا صفحه مدیومشو نشون دادید یکم فرق میکنه یعنی یکم ضعیفتره ولی روی سیستم اون اقا حتی عدد ۱۴ هم جواب داده بود ولی بدا من ۲ ام جواب نمیده
-> ویندوز کار نمیکنه بدتر شدنش هم فکر کنم به همین دلیل هست"
"-> استاد سلام خوب هستین من یه دیتا ستی دارم به این شکل که مسله رگرسیون است اما نمیدونم چطوری باید فیچرم رو mutation کد گذاری کنم یعنی اینکد ش کنم پیشنهادی دارین
-> سلام ممنون سلامت باشید من نمیدونم مساله دیتاست و اون فیچر چی هست معمولا به دو روش فیچرهای کتگوریکال رو نومریکال میکنیم لیبل انکدینگ و وان هات انکدینگ البته روشهای دیگهای هم داریم اگه مقادیر ترتیب دارن میتونید از لیبل انکدینگ استفاده کنید اگه هیچ ترتیبی ندارن و نمیشه ارتباطی ایجاد کرد که باید وان هات استفاده بشه
-> خیلی ممنون به جز این روش ها چیز دیگه ای هم هست معرفی کنید اون فیچر البته کتوریکال تعدادش خیلی زیاده از یه بخشی ش عکس گرفتم من این دوتا رو تست میکنم خیلی خیلی محبت کردید استاد
-> ممکن هست گمراه بشید به نظرم حتما از کسی کمک بخواید که مساله شما رو بشناسه ولی باز یک روش دیگه هم میگم شاید امبدینگ هم خوب باشه یعنی شما برای هر کدوم از یونیک ولیوهای این ستون یک بردار امبدینگ بسازید ولی بازم تاکید میکنم که روشهایی که گفتم مرجع شما نباشه اگه بدون شناخت مساله ازشون استفاده بشه ممکنه جواب نده و وقتتون هدر بره
-> ممنونم استاد برای embedding توی sikit learn توابعی داریم اینکار رو انجام بده وان هات انکدینگ هست ولی برای این ندیدم
-> نه نداره"
"-> سلام استاد وقت بخیر اولا ممنون بابت دوره های عالی و مطالب بسیار مرتب و خوبتون میخواستم بدونم برای مبحث سیگنال ها بعدا دوره ای برگزار میشه من پروژه ای که میخوام کار کنم به شدت با سیگنال ها درگیره و نمیدونم باید از کجا بخونم آیا دوره های دانشگاهی برق سیگنال و سیستم کمکی میکنه یا مطالب مرتبط به اونها نیست پروژه ام در حیطه مغز هست ولی جدای یه سری بخش های سیگنال های مغزی بقیش کاملا مربوط به کانوولوشن ها و فوریه و غیره است از دوستان عزیز هم اگر کسی دوره خوبی برای سیگنال ها میشناسه ممنون میشم معرفی کنه
-> "
"-> سلام استاد خسته نباشید من تقریبا اواخر دوره یادگیری عمیق هستم برای ادامه مسیر میخام ازتون مشاوره بگیرم من علاقه دارم وارد حوزه بینایی کامپیوتر بشم اما فکر می کنم زوده که بخوام دوره بینایی کامپیوتر رو استارت بزنم و شاید بهتر باشه برای تقویت چیزایی که یاد گرفتم اول دوره دیپ کاتالیست رو شرکت کنم بنظر شما بهتره این روند یادگیری رو ادامه بدم و دوره بینایی کامپیوتر رو تهیه کنم یا مدتی صبر کنم و زمانی رو اختصاص بدم برای تقویت چیزهایی که یاد گرفتم و دوره دیپ کاتالیست رو تهیه کنم
-> سلام نظر من رو بخواید اول دیپ کاتالیست چون بینایی هم تا وقتی مسلط نشید به دیپ خیلی فایده نداره بازهم استاد نطر بدن بهتر هست
-> سلام بله بهتر هست اول دیپ کاتالیست رو ببینید با شناختی که از شما دارم اصلا جای شما توی دوره دیپ کاتالیست خالی بود شاید تا موقع شروع دوره بینایی بتونید خودتون رو برسونید"
"-> سلام استاد دوره MLOps تو برنامتون نیست
-> سلام نه فعلا در سطحی نیستم که دوره برگزار کنیم
-> سلام خود مبحث MLops شامل چندین سرفصل اموزشی میشه برای یادگیری مبحث MLops باید با مباحث کوبرنتیز داکر روشهای ایجاد API مباحث CICD و از قبل اشنا بود تا بتوان در مورد MLops به جمع بندی رسید اموزش موارد فوق در یک دوره اموزشی تقریبا سخته یا اینکه دوره رو طولانیش کرد که مخاطب سردرگم و خسته میشه تازه از اینها گذشته ابزارهای MLops برای هر حوزه هوش مصنوعی دوباره خودش متفاوته"
"-> سلام به همگی وقت بخیر در مورد حل مسائلی که در اونها با تصویر یا متن سروکار داریم میشه یک نمایشی از اونها داشت مثلا بردارهای embeddings از کلمات و به مدل داد سوال من این هست که اگر دادههای ما جدولی باشه چطور میشه با الگوریتمهای یادگیری عمیق حلشون کرد مثلا تخمین یک مقدار بر اساس ویژگی های دیگر که برای رکوردهای مختلف ارائه شده است راستش من هنوز دیدم برای حل این مسائل همون روشهای یادگیری ماشین هست خیلی ممنون از راهنماییتون
-> سلام ما توی دوره یادگیری عمیق فصلهای اول چند بار از mlp برای حل مسائل تبولار استفاده کردیم اونها رو دیدید معمولا از mlp استفاده میشه البته کارهایی مثل tabnet استفاده از ترنسفورمر هم وجود داره
-> خیلی ممنون اونا رو دیدم بیشتر منظورم رویکردهای جدیدتر مثل ترنسفورمرها بود خیلی ممنون استاد"
"-> سلام استاد ایا با image annotationروش keypoint ترجیحا میشه حمایت و مقاومت های چارت خطوط زردی که مشخص کردم رو لیبل گذاری کرد تا در لایو بازار یک مدل یادگیری عمیق حمایت و مقاومت ها را تشخیص بده میخوام از keypoint استفاده کنم که دقیق باشه و محل دقیق حمایت و مقاومت درست تشخیص بده
-> سلام نمیدونم اصلا ایدهای ندارم"
"-> سلام وقت بخیر در transformsRandomRotation این عملیات روی همه تصاویر batch اعمال مبشه یا بصورت تصادفی روی آن ها من بعد از داده افزایی خروجی گرفتم ظاهرا روی همه تصاویر بچ اعمال شده
-> با یک مقدار رندوم بین اون بازهای که مشخص کردید میچرخونه روی همه اعمال میشه اما با مقدار چرخشهای متفاوت"
"-> سلام وقت بخیر مشخص شد که دوره پردازش تصویر کی منتشر میشه
-> سلام از اینکه دوستان رو در انتظار نگه داشتیم عذر میخوام ما هنوز تصمیممون رو نهایی نکردیم ولی همش درگیر این دوره هستیم چیزی که فعلا تیم هوسم نظر مثبتی روش داره رو در ادامه میگم دوست دارم نظر دوستان رو هم بدونم ما به این نتیجه رسیدیم که دوره بینایی کامپیوتر رو آپگرید کنیم و این بار هم متدهای کلاسیک و هم دیپ رو پوشش بدیم یک دوره واحد که ابتدا کلاسیک باشه و بعدش دیپ طبیعتا در بخش کلاسیک از اوپن سی وی هم در سطح خوبی گفته میشه تئوریها هم که پوشش داده میشه پیشنهادم این هست که فعلا درمورد اینکه کسانی که قبلا بینایی رو گرفتن چطوری به دوره جدید میتونن دسترسی داشته باشن صحبت نکنیم چون این چیزایی که توی این گروه میگم طرح اولیه هست نهایی که بشه درمورد مسائل جانبی هم صحبت میکنیم فعلا لطفا نظرتون رو در مورد اصل مطلب یعنی خود دوره مطرح کنید منتظر نظراتتون هستم ممنون
-> سلام وقت بخیر عالیه
-> به نظر عالی میرسه ممکنه طولانی تر باشه و حجم مطالب خیلی زیاد بشه اما فکر کنم در آخر یه دوره به شدت کاربردی یشه im in
-> به نظر من دوره چند تیکه باشه بهتره چون اینجوری هم هزینه هر بخش کمتره و هم این که شاید کسی فقط مثلا نیاز به پردازش تصویر کلاسیک داشته باشهمثلا برای دانشگاه یا یک پروژه خاص و اینجوری خریدنش سخت میشه البته میدونم وقتی دوره چند تیکه باشه پشتیبانی سخت میشه
-> سلام استاد پیشنهاد خیلی عالیه دوره بینایی گسترده تر بشه اگر امکانش هست پروژه ها و مباحث ب سمت فضای کاری باشه ومتود و تکنیک هایی ک در حال پیاده سازی هست ممنونم
-> درود استاد راستش اگر اشتباه نکنم چند ماه پیش تو گروه بینایی گفته بودید میخواید دورهی جدید بدید که مطالب گسترده باشه و از عمق کاسته بشه بنظر من این تصمیم جدیدتون خوبه بنظر خودم بجای آپگرید یه دورهی جدید بذارید و دورهی حرفهای هم نگه دارید بنظر من برای کسایی که دورهی حرفهای رو خریدند کد تخفیف بذارید تا راحت تر دسترسی به دورهی جدید داشته باشند
-> سلام همون مطالب دوره بینایی رو بازهم در سطح خوبی پوشش میدیم سعی میکنیم بهینهترش کنیم به نظرم اگه اون دوره با این دوره جدید جایگزین بشه کسی چیزی از دست نمیده نیازی نیست هردوشون باشن
-> درواقع بیشتر برای راحتی کار خودتون گفتم تو اون موضوع دسترسی پیشنهاد دادم به هر حال در کیفیت کارتون کع شکی نیست به هر حال برگزاری دورهها طول میکشه بخاطر همین برنامه ریزی خوب و دقیقتونه
-> ممنون لطف دارید چه اموجیهای باحالی کنار اسمت هست
-> سلام وقت بخیر ایده خیلی خوبیه چون دوره دیپ دوره خوبو جامعی بود فرد بادیدن این دوره این دوره کامپیوتر ویژنی که شما گفتید میتونه اشنایی خوبی پیدا کنه یه دوره صرفا شامل اوپن سی وی یا صرفا الگوریتم های دیپ شاید به اندازه ای که این دوتا کنار هم باشن جذاب و مفید نباشه"
"-> سلام روز بخیر برای تشخیص رنگ یه ابجکت چه روشی رو پیشنهاد میدین
-> سلام یک روش رایج تبدیل rgb به hsv و بعد تعیین رنگ هست با اوپن سی وی میشه انجام داد"
"-> سلام وقت بخیر برای قسمت کاستوم دیتاست مثالی که برای نوع دادههایی که ساختار مشخص ندارن زده شده برای خوندن فایل های متلب هست نوع فایلی که میخوام براش کاستوم دیتاست بنویسم csv هست با استفاده از pdread_csvلود کردم و توابع init len getitem رو کامل کردم اما این خطا رو No such file or directory دارم در صورتیکه تو هر دو فایل هم تصویر و هم csv این تصویر و لیبل تصویر موجوده ممنون میشم راهنماییم کنید کلی کلنجار رفتم متاسفانه نتونستم مشکل رو حل کنم
-> مشکل از آدرس دهی هست میتونید کدهای داخل اینیت و گت آیتم رو بیارید بیرون از کلاس و توی یک سلول ساده بذارید بعد چند تا تست ساده انجام بدید ببینید آدرس چطوری هست کلا اشتباه های آدرس دهی زیاد پیش میاد حتی ممکن هست با چشم قابل دیدن نباشه مثلا یک اسلش یا فاصله اضافه یا
-> اینکارو انجام میدم اما تمپلیتی وجود نداره بتونم از روی تمپلیت یکبار کدهامو چککنم
-> نه دیگه برای چک کردن کد که تمپلیت وجود نداره صرفا یک مهارت کدنویسی هست"
"-> سلام وقتتون بخیر من یک شبکه کانولوشنی رو با ۴ تا کلاس آموزش دادم که ۴ تا چهره مختلف رو تشخیص میده میخواستم بپرسم راهی هست که یک کلاس جدید به شبکه اضافه کنم و چهره یک فرد دیگه رو به روی این کلاس جدید اموزش بدم طوری که به کلاس های دیگه آسیبی نرسه
-> این مساله اوپن ست هست مسائلی که کلوز ست هستن رو با دسته بندها حل میکنن برای مسائل اوپن ست معمولا سراغ متدهای مبتنی بر سیامیز یا تریپلت لاس یا موارد دیگه میرن توی مساله اوپن ست شبکه بجای دسته بندی داده یاد میگیره که برای داده های از یک کلاس خاص یک بردار ویژگی یکتا بسازه برداری که شناسنامه یا اثر انگشت یا هویت اون کلاس هست یکی از شاخص ترین مثالهای اوپن ست شناسایی هویت براساس چهره هست
-> خیلی ممنون از راهنماییتون"
"-> سلام استاد من دیتاستی دارم و کد mlp رگرسیون رو براش اجرا میگیرم اما کد ناپایداره دقت r2 در داده های train و validation تا نزدیک ۱ میره و مقدار لاس هم تا نزدیک صفر تا ۴ رقم اعشار اما داده های تست هر دفعه یک مقدار پیدا میکنن و نتایج مطلوب رو در نمیاره چه چیزی رو باید چک کنم و اینکه ممکنه نتایج کد mlp در بعضی مسایل مثلا از نتایج SVR بدتر باشه یا باید انتظار داشت که شبکه های عصبی بهتر از الگوریتمهای یادگیری ماشین جواب بدهند تشکر
-> سلام با توصیفتون نمیتونم بفهمم مشکل احتمالی از کجاست ذهنم به سمت باگهای کدنویسی رفت ممکن هست مدلهای یادگیری ماشین از دیپ لرنینگ بهتر کار کنه خصوصا زمانی که داده کم باشه از طرفی معمولا داده های تبولار رو با یادگیری ماشین جلو میرن
-> تشکر بابت پاسخگویی داده هام تبولار با ۱۱۰۰۰ داده و ۹ فیچر هست چیزی که الان متوجه شدم با تغییر تابع فعالساز از relu به prelu مشکل ناپایداری کد خیلی کمتر شد البته کاملا برطرف نشد ولی نسبت به قبل بهتر شد"
"-> سلام دوستان یه سوال داشتم من یک سری ماسک دارم که به فرمت JSON هستند آیا میشه این فایل ها رو به فرمت های دیگه تبدیل کرد مثلا به صورت یک عکس به فرمت png درآورد یا همینجوری میشه ازشون استفاده کرد برای ورود به شبکه مون
-> سلام کانورت کردن کار رایجی هست توی اینترنت هم سرچ کنید احتمالا ریپو و کد آماده وجود داشته باشه من مورد خاصی رو نمیشناسم ولی شدنی هست اون بخشی هم که گفتید همینطوری بدید به شبکه خب بستگی به کد داره دیگه ممکن هست کد از شما جیسان بپذیره ممکن هست به نوع فایل دیگه ای نیاز داشته باشه
-> سپاس گزارم استاد بابت راهنمایی تون"
"-> سلام استاد ایا میشه با یادگیری عمیق حمایت و مقاومت را تشخیص داد سطوح زرد رنگ که مشخص کردم و قیمت چرخش میکنه باید داده های اموزشی چجوری باشه که مدل بتونه اون سطح رو تشخیص بده
-> سلام نمیدونم"
"-> سلام استاد گرانقدر howsam_support شبکه gpt قابلیت این رو داره که جایگزین شبکه bert بشه یعنی به جای bert از gpt استفاده بشه
-> سلام نمیدونم به کاربردتون بستگی داره یکسری از مقالهها از چت جی پی تی کمک میگیرن"
"-> سلام دوستان من وقتی کد یک مقاله رو که در گیتهاب هستش رو اجرا میکنم چرا به دقتی که در مقاله گفته شده نمیرسم مثلا ۱۲ درصد پایین تر کسی هست مشکل منو داشته باشه
-> دلایل مختلفی میتونه داشته باشه مثلا شاید بهترین تنظیمات رو توی کد نذاشتن اون تنظیماتی که بهترین دقت رو میده و
-> سعی کردم تا جایی که بنظر میرسه همون تنظیماتی که خودش گفته رو اعمال کنم آیا تفاوت نوع gpu میتونه عامل این تغییر باشه
-> نه خیلی بعیده شاید مثلا نسخه فریمورکها و متعلقاتش مثل کودا و غیره تاثیر بذاره
-> ولی نسخه ها متفاوت باشه که همش ارور میگیری این کامل اجرا میشه ولی باشه به هر حال ممنون
-> نه همش که اینطوری نیست مثلا شما کد نوشته شده با پایتورچ 2 رو بدون تغییر روی پایتورچ 1 هم میتونید اجرا کنید ولی ممکنه این تفاوت روی دقت اثر بذاره ممکنه"
"-> دوستان یه سوالی برام پیش اومده اگر یه دیتای دو کلاسه داشته باشیم که اول لیبل های مثبت اومده باشن و بعد منفی ها تفاوتی برای مدل های یادگیری ماشین داره این موضوع
-> لاجتيك رگرسيون
-> 
-> بنظر من که آره چون مدل بچ به بچ اپدیت میشه و چند بچ پشت سر هم همه یه لیبل یکسان دارند احتمالا اپدیت وزن ها هی بالا و پایین بپره مطمعن نیستم فقط نظر شخصیمه خب حالا چه کاریه یه رندوم اسپلیت بزن خیالتو راحت کن یه خط کد بیشتر هم نیست
-> همین کارو کردم ممنون
-> خواهش میکنم
-> سلام باعث اورفیت شدن مدل میشه چون مثلا تا فلان بچ داره فقط یه لیبل میبینه
-> ممنون"
"-> یه نکته دیگه اینکه Torchcudaempty_cache رو میزنم نصف حافظه رم gpu رو خالی میکنه نه همش مشکل چیه هرچی سرچ کردم تو نت اکثرا همین دستورو گفته بودن ولی کامل کار نمیکنه انگار
-> اگه دوتا جیپیو داشته باشی احتمالا بتونی با این کد import subprocess subprocessrunnvidiasmi gpureset checkTrue ریست کنی گاهی اوقات مخصوصا وقتی که مدل توی پروسه ترین بیای با CTRLC کنسل کنی یا زمانی که گرادیان گیری رو توی اینفرنس خاموش نمیکنی مدل پر گرادیان میشه و empty cache درست کار نمیکنه چون گرادیان جزو دارایی های مدل هستن و دلیل اصلی احتمالا این هست
-> آره اون نکتههای آخری که گفتی خیلی مهم و خوبن اون دستور empty cache هم تا حد قابل قبولی که حین ترین پر شده رو خالی میکنه یعنی به نظرم کافیه
-> ببخشید فقط خط اخرو متوجه نشدم این دستور گرادیان هارو پاک نمیکنه و کلا چیزایی ک مربوطن به مدل درسته پس چیارو پاک میکنه
-> 
-> 
-> بنظرم چیزی که توی مقاله ی اصلی اومده با چیزیکه تو چت بات ها گفته میشه متفاوته یا حداقل با برداشت من از مقاله
-> این قسمتی که هایلایت کردی داره همون چیزی رو میگه که چت جی پی تی گفته depth wise pointwise اتفاقا راجع به علتش هم گفته کد آماده اش رو نمیتونی پیدا کنی و همزمان با مقاله کد رو بخونی
-> توی اون صفحه چت صرفا به depthwise و pointwise به تنهایی اشاره شده بود در حالیکه یه لایه ی depthwise دیگه هم قبلش داریم کد اماده ی این مقاله با تنسورفلو نوشته شده و بنده دارم با پایتورچ کد میزنم کد پایتورچ این شبکه هم یکی دو تا پیدا کردم که بنظرم اصلا وفادار به مقاله نبودن در واقع میشه گفت مشکل اصلی من با کد EEGNet نیست بلکه کد تابع kerasseparableconv2d در تنسورفلو هست که نتونستم پیدا کنم
-> فرق دپت وایز با سپربل چیه توی اون عکس میتونید خلاصه و ساده بگید
-> سپریل یه دپت وایزه بعلاوه ی یک پوینت وایز در واقع همون لایه ی depthwise separable convolution در mobile net هست
-> پس اول دپث وایز رو فقط بذارید خب این با همون آرگومان گروپ بدست میادا گروپ رو برابر با تعداد چنلهای فیلترتون بذارید دیگه پوینت وایز نمیخوره
-> یعنی برای EEGNet دوتا دپث وایز بذارم بدون پوینت وایز خود مقاله گفته بعد دپث وایز یه پوینت وایز با کرنل ۱۱ زده
-> من اینجا برداشت خودم رو از اون جدول شبکه گفتم راه حل کدنویسی رو نمیدونم نمیدونم بدردتون میخوره یا نه
-> 
-> ببخشید وویسم طولانی شد
-> آخر وویس رو اشتباه گفتم منظورم separable convolution بود depthwise رو تقریبا اوکی هستم تو کراس هم دنبال separableconv2d بودم"
"-> سلام استاد وقت بخیر این مرحله اخر تنظیم هایپرپارامترها توی فشل ترنسفورمر دیپ لرنینگه که بخاطر مشکلgpu انجام ندادید از بین این سه گزینه بنظر من گزینه ی اول بااینکه از دومی نتیجش درحد چند صدم بدتره ارجحیت داشته باشه بخاطر داشتن wd و هممچنین جلوگیری از نوسانی شدن اموزش خواستم ببینم منطقی هست یاباید همون که پایین ترین لاسو داره انتخاب کنم
-> نموداراش بعد ۲۰ تا ایپاک اموزشاین شد استاد مورد شماره یک توی لاس ولیدیشن از همه بهتره مورد شماره دو توی اکیوریسی ولیدیشن از همه بهتره هیچ قاعده و قانونی نداره واقعا
-> خب اعداد خیلی بهم نزدیک هستن به نظرم تا این اندازه وارد جزئیات نشید بهتره"
"-> سلام ممنون چرا کولب با هوش مصنوعی هستش تنظیم خاصی دارد
-> سلام میتونید به انگلیسی تایپ کنید مثلا کد فیبوناچی رو برای من تایپ کن اونم کد رو با هوش مصنوعی براتون تولید کنه یا ایرادات کدتون رو بررسی میکنه و دیباگش میکنه"
"-> سلام استادوقتی یک شبکه رو پیاده سازی کنیم و بعد بخوایم وزن های اولیه ای اعمال کنیم توی اون آیا راه حلی هستیعنی از وزن دهی خود پایتورچ استفاده نکنیم به عنوان مثال یک راه حل مثلا این هست که مثلا یک شبکه آماده رو import کنیم و وزن های اون رو هم استفاده کنیم ولی وقتی خودمون یه شبکه با تعداد پارامتر میلیونی رو ایجاد میکنیم آیا راهی هست یک راه این هست که شبکه رو بذاریم train بشه با یه سری داده و بعد شبیه transfer learning استفاده کنیم آیا درسته این رویکرد
-> 
-> ممنون از پاسختون بهتره یه توضیح بدم که دارم چکار میکنمالبته قبلا هم مطرح کرده بودم مشکل و مسأله رو ولی امیدوارم بتونم این سری یه کم کامل تر روشن کنم مسأله رو بنده میخوام image generation کنم با استفاده از 3d unet داده هام MRI هستن و ماه صفر و ماه ۲۴ از هر بیمار MRI گرفته شده که بین این ۲۴ ماه تغییراتی رخ داده ولی این تغییرات خیلی خیلی جزیی هستن شاید در کل دیتاست در حدود یک درصد یا mae 001 بین ماه صفر و ماه ۲۴ تفاوت داریم پس هدف اینست که با دادن ماه صفر ماه ۲۴ generate بشه از unet و u2net که receptive field رو بالا میبره استفاده کردم و بصورت خطی یادگیری انجام میشه به عبارتی ورودی ماه صفر هست و target ماه 24 تا وزن ها train بشن توی مراحل ابتدایی و epochهای اول خیلی مدل ناپایدار هست و بنده احساس کردم ممکنه بخاطر عدم initialize اولیه باشه حالا اگه مقدور هست براتون ممنون میشم در کل نظرتون رو بگید در رابطه با این موضوع و آیا چشمتون آب میخوره که نتیجه بده این روش با توجه به این تفاوت کم بین input و target آیا روش بهتری هست در ضمن تابع هزینه هم از mae استفاده کردم و یادگیری هم خطی هست داده هم داده MRI مغزی هست
-> من اینجور کاریو انجام دادم و موفقیت امیز بود نمیدونم میزان تغییرات شما چقدره البته
-> چشمم آب نمیخوره راستش من هربار که با یونت کار کردم و ایده روی وزنهای اولیه داشتم به نتیجه نرسیدم ولی به نظرم بازم ارزش داره که یک بار تستش کنید یکم مشکله برام که اینجا فکر کنم و نظر بدم چون داده رو ندیدم و"
"-> سلام برای پچ امبدینگ کسی از این روش استفاده کرده که بیاد conv2d رو به عنوان لایه ورودی و پچر انتخاب کنه آیا کار میکنه با چیزی که به عنوان پچ امبدینگ میشناختم خیلی فرق داره
-> سلام من پیاده سازی نکردم اما این مقاله اینکاروکرده بنظرم اگر باهاش تا حالا آشنا نبودین حتما یه نگاه بندازین یه بخش convolutional token embedding دارن که با پچ امبدینگ داخل vit که فکر کنم مدنظر شماست فرق داره مقاله swin هم البته اینکارو کرده
-> کدی که پیاده سازی کرده بودن با کدی که من زدم یکی هست حتما مقاله رو میخونم ممنون
-> فکر کنم اولین بار توی ویت این کار انجام شد کانولوشن با استراید و کرنل بزرگ برای امبد کردن پچها و ساخت توکن به ازای هر پچ برای ترنسفورمر
-> یعنی توی ویت اصلی این روش موجوده و از اونجا اومده"
"-> سلام استاد وقت بخیر Batch normalization یجا خوندم یکی از مزیتاش این بود internal covariate shift رو جلوشو میگیره و باعث میشه تعمیم پذیری بهتری داشته باشیم chatgpt هم اینو گفت ولی بنظر من فقط توی سرعت اموزش و همگرایی در فاز ترین اثر داره خواستم نظر شمارو در مورد اینکه روی تعمیم پذیری شبکه تاثیر داره یانه بپرسم
-> سلام خب وقتی سرعت آموزش و همگرایی رو بهتر میکنه یکی از نتایجش این میتونه باشه که تعمیمپذیری هم خوب میشه اینطوری فکر نکنید که مثلا بچ نرمالیزیشن باشه با ده ایپوک مدل ترین میشه و نباشه با بیست تا شاید مدلی بدون بچ نرمالیزیشن اصلا ترین نشه بدون بچ نرمالیزیشن ترین کردن مدلها بسیار سخت بود مثلا شیوه ترین کردن vgg و گوگل نت رو بخونید اینها بدون بچ نرمالیزیشن بودن"
"-> سلام وقت بخیر یه سوالی دارم داشتم مقاله مرتبط با شبکه ی EEGNet رو میخوندم دیدم از دو نوع شبکه ی کانوولوشنی تحت عنوان Depthwise Convolution و Separable Convolution استفاده شده است از طرفی توی بررسی شبکه ی موبایل نت در سایت هوسم با شبکه ی depthwise separable convolution آشنا شده بودم که از دو بخش depthwise و pointwise تشکیل شده بود قسمت depthwise شبکه EEgNet و موبایل نت که یکسانه حالا سوالم اینه که قسمت Separable Convolution شبکه ی EEGNet هم با قسمت depthwise separable convolution موبایل نت یکیه یا باهم فرق دارند یا چه فرقی دارند توی اینترنت سرچ کردم و بلوک دیاگرام هایی که برای این دو استفاده میشد یکسان بود ولی از چت جی پی تی و bard که این سوال رو پرسیدم میگفتند که نه باهم فرق دارند و تفاوتشون توی ترتیب دو مرحله ی depthwise و pointwise هست که جالب اینجاست این دو هوش مصنوعی ترتیبی که برای این دو شبکه عنوان کردند با هم فرق داره
-> سلام من اغلب همون موردی رو دیدم یا استفاده کردم که توی موبایلنت هست موردی که چتباتها گفتن رو ندیده بودم سورس کد نداره
-> من سورس کد مرجعی نتونستم پیدا کنم هر چند تو گیت هاب هستند افرادی که پیاده سازی کردند
-> ببخشید شما برای موازی سازی و اینکه شبکه بقول مقاله همانند FBCSP فولی کانکتد نباشه و هر شاخه جدا باشه چه حرکتی زدید با توجه به اینکه پایتورچ مثله تنسورفلو لایه ی separateable convolution ندارد
-> متوجه نشدم توی پایتورچ آرگومان گروپ توی دستور کانولوشن برای separable استفاده میشه
-> ممنونم
-> استاد سلام وقت بخیر راجب serving مدل های اموزش داده شده مثلا با فریمورک ONNXOPENVNO توضیح میدید
-> نمیدونم
-> سلام سوالاتون رو دقیق تر بپرسید چیمیخواید درموردشون بدونید
-> سلام مجدد وقتتون بخیر شما برای پیاده سازی شبکه ی EEGNet با پایتورچ برای قسمت separable convolution از یه لایه conv2d استفاده میکنید یا دوتا برای لایه اول convolution شبکه یه کانوالو معمولی میذاریم برای لایه دوم depthwise یه کانوالو همراه با group میذاریم حالا برای لایه سوم باید دو تا لایه کانوالو بذاریم یکی با group برای depthwise و یکی دیگ هم با کرنل 11 برای pointwise
-> سلام مجدد وقتتون بخیر شما برای پیاده سازی شبکه ی EEGNet با پایتورچ برای قسمت separable convolution از یه لایه conv2d استفاده میکنید یا دوتا برای لایه اول convolution شبکه یه کانوالو معمولی میذاریم برای لایه دوم depthwise یه کانوالو همراه با group میذاریم حالا برای لایه سوم باید دو تا لایه کانوالو بذاریم یکی با group برای depthwise و یکی دیگ هم با کرنل 11 برای pointwise"
"-> حالا باز استاد خودشون توضیح بدن بهتر میشه
-> خیلی ممنون"
"-> سلام وقت بخیر در ترانسفورمر kqv یاد گرفته می شوند
-> خیر ترین نمیشن بر خلاف نورون ها که وزن هاشون ترین میشن v q و k به صورت داینامیک محاسبه میشن به صورت ضرب دخلی و به دست میاد
-> ممنون از توضیحات تون پس شبکه ترانسفورمر چه چیزی را یاد میگیره
-> تا اونجا که من متوجه شدم لایه های attention feedforward و normalization با داده ها آموزش میبینن
-> درسته با داده کار می کنند اما باید kqv را یاد بگیرند به نظرم
-> مثال سرچ یک مورد در یوتیوب رو اگه در نظر بگیریم Q یا query که پرسش هست و مشخصه ویدئوها که ارزش V رو دارن و توضیحات هر ویدئو که K یا key هستن هم ثابت هستن مهم پیدا کردن وابستگی و ارتباط کلمات هست که در مقایسه Q و K در شبکه آموزش میبینن
-> سلام مقادیر k q v توسط محاسبات بدست می ایند اما چطور بدست می ایند توسط ماتریس های وزن Wk Wq Wv این ماتریس ها توسط فرایند اموزش بهینه می شوند در ترنسفورمرها شبکه های FFN و این ماتریسها بهینه می شوند
-> خیلی ممنون از راهنماییتون
-> سلام همراه با اون عکس و صحبت هایی که دوستان گفتن به نظرم بد نیست به این تیکه هم نگاه کنید بهتر متوجه میشید چه اتفاقی درون مولتی هد اتنشن میوفته و چه موقع یادگیری انجام میشه و چجوری اطلاعات متفاوت درون هد ها قرار میگیره البته گونه های دیگه ای از ترنسفورمر هم هستن که هرچند زیاد ممکنه معروف نباشن که برای هر هد تابع خطی مجزا تعریف کردند
-> البته بخواهیم اضافه کنیم emmbeding ها هم طی آموزش قاعدتا لرن میشن برای pos encoding به غیر تایپ absolute که چون فیکس هست این اتفاق میفته چون اون هم با embedding token ها جمع میشن
-> خیلی ممنونم از توضیحات تون"
"-> یعنی بعد از deepکل nlpوcv را هم ببینم
-> منظورم فصلهای ده و یازده دوره دیپ هست
-> تشکر استاد"
"-> آموزشهای شما که و دیپ لرنینگ را درحال آموزش هستم بعد از فصل ۹ هر کدام را تمایل داشتیم nlpیاcomputer vision را بخونیم ممنون درسته دیپ در هر دو گرایش مشترکه درسته تشکر
-> به نظرم یا هرسه تاشو ببینید یا حداقل دو فصل nlp و CV رو ببینید"
"-> اگه بخواهیم فقط تو یک زمینه مثلا nlp یا کامپیوتر ویژن کار کنیم لازم هسته که هر دو را بلد باشیم ممنون چون فقط تو یک زمینه فقط میخوام کار کنم
-> چه خوب میشد در مورد نقشه راه و مسیرهای شغلی هم یه توصیحاتی میدادید تا تصمیم میگرفتیم چه آموزش هایی برامون الویت داره من خودم بشخصه نمیدونم یادگیری تقویتی تقویتی عمیق nlp و تا چه حد برام الویته الطفا از دید مقاله نوشتن نگاه نکنید بیشتر کاربرد و بازار کار منظورمه
-> 
-> تشکر استاد محترم
-> "
"-> سلام وقت بخیر دستورات bash مربوط به چه مبحثی هست نحوه ران چگونه هست در کولب
-> برای سیستم عامل لینوکسه
-> درود اسکریپت رو روی گوگل درایوت اپلود کن بعدش از توی کولب اجرا کن هم با کتابخانه ی os هم ترد ها هم اینکه با میتونی انجام بدی
-> bash scriptsh روش ساده تریه
-> ذستورات Bourne Again SHell برای سیستم های یونیکس و لینوکس بیس برا ی اتومات کردن یک سری تسکهای مربوط به مدیریت فایل و اسکریپت برای اوردنش توی کولب یک علامت در ابتدا بزارین ببینین جواب میده bash run_bashrun_MAMSsh"
"-> ببخشید یک سوال داشتم من الان تایم پیدا کردم دوره کامپیوتر ویژنی که یادمه تخفیف هم داده بودین استفاده کنم با کد باید فعال شه چطوری هست
-> سلام لطفا به پشتیبانی پیام بدید
-> ممنون از شما اصلا همونجا بود و دیدم"
"-> سلام وقت بخیر من نیاز به راهنمایی دوستان داشتم یه دیتاست از مزارع ذرت جمع آوری شده دارم که توی بعضی از ردیف ها تعداد گل های ذرت خیلی کم ان میخواستم با استفاده از شبکه های GAN تعداد گل ها رو افزایش بدم به غیر از شبکه cycleGAN چه شبکه ای رو برای این کار پیشنهاد میدید
-> سلام دفیوژن"
"-> سلام وقت بخیر یه سوال توی دوره ی دیپ لرنینگ برای مسائل دسته بندی چند کلاسه از تابع هزینه cross entropy استفاده کردیم من جایی خوندم که برای دسته بندی بیش از دو کلاس از categorical cross entropy استفاده بشه میخواستم ببینم استفاده ی این دو جای همدیگه درست است یا تفاوت چشمگیری با هم دارن
-> 
-> بله مقاله با تنسورفلو پیاده سازی شده بود خیلی ممنون کامل متوجه شدم"
"-> سلام در مورد این بحث که از روی وزن های شبکه آموزش دیده بیایم فیچرهای ورودی رو از نظر اهمیت ارزش گذاری کنیم یا نه ابعاد فیچرهای ورودی رو کاهش بدیم و فقط فیچرهای ارزشمند و تاثیر گذار رو به شبکه بدیم بنده چتا مقاله خوندم که اینجا شیر میکنم امیدوارم مفید باشه
-> استاد خیلی ممنون بابت به اشتراک گذاری این منابع
-> ممنون استاد"
"-> سلام استاد در یادگیری ماشین با استفاده از رگرسیون خطی میایم بهترین خطی که از داده ها رد میشن رو به دست میاریم حالا فرض کنیم در این عکس داده ها رو به صورت عددی نداریم و فقط تصویر نمودار رو داریم میشه با استفاده از یادگیری عمیق و پردازش تصویر بهترین خط رو رسم کرد
-> یکسری داده توی تصویر که موقعیت مکانیشون رو ندارید
-> بله فک کنید فقط نقاط سبز رنگ رو داریم توی عکس نقاط دقیق xوy رو نداریم میشه خط قرمز رنگ رو با یادگیری عمیق و پردازش تصویر رسم کرد
-> آره میشه عجیبه تا حالا چنین کاربردی ندیده بودم راه حلش ابتکاری هست مثلا میتونه اینطوری باشه اول نقطهها رو دیتکت کنید احتمالا با پردازش تصویر کلاسیک دیتکت بشن بعد هم شاید با همون مختصات پیکسلی بشه یک مدل فیت کنید یا اینکه مبدا رو عوض کنید و بعد فیت انجام بدید البته ممکنه راه حلهای راحتتری هم وجود داشته باشه
-> ممنونم بابت راهنمایی"
"-> از دوستان کسی تجربه تحلیل مدل رو داشته مثلا از کجا بفهمیم مدل به کدام فیچرها توجه کرده و بهشون وزن بیشتری اختصاص داده داده های من تایم سری هست اگر منبعی چیزی هست ممنون میشم راهنمایی کنید
-> استاد این سوال منم هست راهی وجود دا ه بفهمیم توی این نوع داده ها مدل به کدام فیچرها بیشتر اهمیت داده
-> پیامهای بالا رو یکی از دوستان در گروه یادگیری عمیق فرستاده بود"
"-> با سلام روز به خیر من یک مقاله دیدم نوشته بود کانولوشن دو شاخه بعد در یک شاخه نوشته بود اطلاعات ساختار رو بدست میاره در یک شاخه اطلاعات رنگ خواستم بدونم چطوری این موارد رو تفکیک میکنه یعنی برای هر شاخه مثلا یک رزنت داره یا کل این رزنت share هست و فقط قسمت loss فرق داره
-> سلام امکانش هست مقاله رو ارسال کنید
-> بله حتما براتون ارسال کردم"
"-> ممنون بله حتما میخونم آخه این شکل رو دیدم یک جا نوشته pure attention یکجا نوشته convattention
-> من این بلوک دیاگرام رو قبلا ندیدم بعضی از شبکهها هستن که تصویر خام رو به شبکه میدن یک شبکهای از مایکروسافت دیده بودم بعصی شبکهها هم داخل بلوکهای انکدر کانولوشن دارن یکسری هم که گفتم اولش کانولوشن دارن احتمالا داره یک دستهبندی کلی از انواعش رو ارائه میده
-> درسته ممنون از توضیحات تون"
"-> سلام وقت بخیر ورودی ترانسفورمر حتما باید تصویر خام باشه یا مثلا میتونه خروجی لایه سوم رزنت یا باشه و در این صورت چه فرقی داره
-> توی شبکهای مثل ویت که خام نمیدن هزینه محاسباتش زیاده معمولا اولش یک یا چند لایه کانولوشنی میذارن بعد خروجی این رو به ترنسفورمر میدن مقاله ویت رو بخونید"
"-> سلام ببخشید استاد من دوره بینایی کامپیوتر شمارو داشتم نگاه میکردم رسیدم به فصل دسته بندی تصویر با ترنسفورمر دوره دیپ شمارو هم دارم ولی قسمت rnn و ترنسفورمر رو ندیدم فعلا با مدلای زبانی اصلا کار نمیکنم و تمرکزم روی کامپیوتر ویژنه خواستم ببینم همین مقدار که توی ویژن گفتید کافیه یا بهتره اون ترنسفرمر دیپ رو هم ببینم
-> سلام به نظرم حتما هم تئوری و هم کدنویسی دیپ رو ببینید بعدش یک نگاهی به فصل 10 دیپ هم بندازید ویت رو اونجا هم گفتم"
"-> سلام دوستان قرار دادن مقاله در arxiv چه مزایا و معایبی میتونه داشته باشه
-> سلام اگه نوآوری داشته باشه با ثبت در اونجا شما اولین فردی خواهید بود که به اون نوآوری دست پیدا کردید
-> خیلی ممنون خانم دکتر پس از این نظر خوبه پس بزاریم در آرکایو
-> سلامت باشید
-> قطعا که خوبه فقط یه سختی که داره مثلا شما داری از رشته کامپیوتر در شاخه کنترل یک مقاله کار میکنی حنما باید فردی که حالا یا در سه سال یا ۵ سال گذشته دقیقا همینجوری بوده و سه مقاله داشته شما رو تایید کنه که خوب پیدا کردن همچین فردی که دم دست باشه سخته خیلی وقتها مگه اینکه مقاله بین رشته ای ننویسین و مستقیم تو رشته خودتون باشه یکم راحتتر میشه من متاسفانه به همچین اشکالی خوردم"
"-> سلام وقت بخیر لطفا تفاوت identification detection recognition را مختصر بگید هر کی یه جور تعریف میکنه و بعضا تناقض با هم دارن خیلی گیج کننده شده سپاس
-> 
-> ممنونم"
"-> تیم LLM آزمایشگاه پردازش متن و زبان طبیعی دانشگاه تهران مدل 7 میلیاردی لاما2 که بر روی دادگان فارسی آموزش داده شده و درک مناسبی نسبت به پرسش و پاسخ فارسی داره رو برای جامعه فارسی زبان منتشر کرده است این مدل تقریبا 2 ماه پیش آموزش داده شده و بر روی هاگینگ فیس موجود بوده است بر خلاف سایر مدلهایی که عموما منتشر شده اند در آموزش این مدل از حجم مناسبی از دیتای خام فارسی نیز استفاده شده است و صرفا به ترجمه برخی دیتاست های پرسش و پاسخ بسنده نشده است فایل کولب کار با این مدل نیز در ادامه آمده است و میتوانید مدل را بارگزاری و تست کنید لینک مدل بر روی هاگینگ فیس لینک فایل کولب
-> درود نقطه ی قوتی که بخوایم ازش استفاده کنیم چه مواردی هست
-> از ایشان در مورد ویژگیها پرسیدم اگر جوابی دادند اینجا پست میکنم
-> سپاس
-> جواب سوال را اقای باغبانی دادند که مقاله در دست انتشار و کیفیت ارزیابی مدل و جزئیات فنی به زودی منتشر خواهد شد
-> سپاس"
"-> چجوری میزان پروسس شبکه فولی کانکتد با لایه های CNN رو به دست بیارمیه شبکه YOLOV8_n دارم که میخوام واسش یک GPU مناسب بگیرممیخوامومیزان پروسسش رو به دست بیارمممنون میشم راهنمایی کنید
-> سلام به نظرم اول باید مشخص کنید که مساله و کاربردش چی هست یعنی اگه روی ویدئو میخواید کار کنید مثلا سایز فریم ورودی چه سایزی هست و شما میخواید چند فریم بر ثانیه پردازش انجام بدید همچنین بجز این شبکه دیگه چه بخشهایی شامل انجین یا نرمافزاری در سیستمتون دارید و اونها چقدر زمان میبرن اینها یکسری از سوالاتی بود که الان به ذهنم رسید ولی شاید مسائل دیگهای هم مطرح باشه
-> ممنونم از پاسخ گوییتون تصاویر ۵۱۲ در۵۱۲ هستن۳۰ فریم بر ثانیه حداقلمطلوب ۶۰ فریم بر ثانیه هستشپروسس دیگه ای هم روی gpu نیست
-> حالا حساب کنید پردازش هر فریم باید چند میلی ثانیه میشه مثلا 30 میلی ثانیه پس جی پی یو باید هر فریم 512 در 512 رو روی شبکه مدنظر شما زیر 30 میلی ثانیه انجام بده براساس مقدار پارامترهای و اینفرنس تایم شبکهتون که خود یولو گزارش کرده میتونید جی پی یو رو انتخاب کنید البته من راه حل ندادم صرفا خواستم یک دیدی نسبت به کلیات کار داشته باشید بازهم سرچ بزنید و از افراد متخصص بپرسید
-> کاملا متوجه هستمحجم پردازشی اصلا کم نیستمنتهی همش به درخواست کارفرما بودهولی من سعی میکنم پارامتر هارو بهینه ترش کنماز لطف و پاسخگویی شما خیلی خیلی ممنونم
-> فریم ریت زیاد میخواد من مساله رو نمیدونم ولی طبق کارهایی که من تجربه داشتم 30 یا 60 خیلی زیاده"
"-> سلام دوستان کسی می دونه چطور میشه یک مقاله پولی از سایت Elsevier دانلود کرد
-> میتونید تو سایتهای مگاپیپر یا گیگالیب ثبت نام کنی و البته هزینه پرداخت کنید سپس دانلود کنید
-> سلام سایت sci hub
-> اگر فری پیپر و ایران پیپر ندادن چنتا ربات تلگرامی هست انجام میدن چنتا هم گروه هست که دانشجو های خارج از کشور برا بچه ها دانلود مبکنن
-> میشه لطف کنید اسم ربات تلگرامی و اسم گروه ها را برام بفرستید
-> لطفا پی وی پیام بدید براتون لینک بفرستم"
"-> سلام وقت بخیر دوستان تئوری اینکه در ترانسفورمر ورودی را از سه تا تابع خطی متفاوت رد میکنه چیه
-> سه نگاشت متفاوت ایجاد میکنه با دوتای اونها شباهت بین پچ ها رو سنجیده و درنهایت ضرب در نگاشت سوم میشه تا پچ های مشابه که این تشابه از مقایسه همون دوتا پچ بدست اومده وزن های یکسانی بگیرند این باعث میشه پچ های که از نظر معنایی یکی اند شبیه به هم بشند
-> سپاس از پاسخگویی تون ببخشید من این رو درک نمیکنم چرا با نگاشت های متفاوت از یک ورودی شباهت باید بدست بیاد و اینکه چرا باید پچ های مشابه وزن یکسانی بگیرند
-> 
-> به نظر من بخش ترنسفورمر دیپ لرنینگ رو ببینید توضیحات خوبی نسبت به خود ترنسفورمر و علت وجود اون توابع خطی داده شده و اینکه از قبل ترنسفورمر ها اتنشن وجود داشته و کارهای مشابهی انجام میدادناتنشن رو خوب متوجه بشید فکر کنم سوالی باقی نمونه براتون
-> خیلی ممنون از توضیحات خوبتون
-> سپاس از راهنمایی تون"
"-> لطفا کانال این دوستمون رو برای منم لطف میکنید بفرستید
-> آقا ول کنید"
"-> جالب هم هست که با هوش مصنوعی کارا مشکل داره همش بهشون تیکه میندازه
-> آقای ع رو میگی"
"-> سلام استاد وقت بخیر توی دوره یادگیری عمیق اشاره ای کردید به اینکه از ترنفسورمر ها در داده های جدولی هم استفاده شده کنجکاو شدم بدونم به چه شکله و عملکردش نسبت به بقیه الگوریتم های یادگیری ماشین چطوره اگر منبع یا لینک کمکی در این رابطه می شناسید ممنون میشم به اشتراک بذارید
-> سلام مهدی جان میتونی از اینجا شروع کنی اگه سخت هست سعی کن با کلیدواژههایی که لینک بالا بهت میده سرچ بزنی و آشنایی اولیه پیدا کنی به نظر میرسه توی تبولار قدرت دست مدلهای مبتنی بر درخت مثل xgboost و غیره هست همون انسامبلهای مبتنی بر بوستینگ توی دوره یادگیری ماشین لینک بالا مربوط به سایت سباستین راشکا هست سایتش رو بررسی کن بسیار فعاله و محتوای آموزنده و بروز زیاد داره"
"-> مسالهای که بیان شده رو متوجه شدید
-> تاجایی که فهمیدم اینطوریه میخوان چند تا کلمه رو ورودی بدند و ادامه کلمات رو یک مدل پیشنهاد بده شبیه language modeling منظورمه
-> آره تقریبا ولی نمیدونید ابعاد کارشون چقدر هست آیا یک مدل زبانی نیاز داره یا مثلا شبیه یک سرچ ساده توی یک سایت هست مثلا توی سایت هوسم توی بخش سرچش بنویسید یادگیری خب کلمات بعدی رو میتونه بدون مدل زبانی هم نشون بده کلا دو تا دوره داره که با یادگیری شروع میشه نیازی به کارهای پیچیده نیست دیگه درمورد راه حلش من نظری ندارم چون مساله رو شفاف نمیدونم
-> درسته باید به قول شما ابعاد مسئله شفاف بشه ولی در کل ذهنم درگیر شد که language modeling رو با درخت میشه انجام داد یا نه چون تو دوره دیپ کاتالیست با اینکه چالش ها رو فرصت نشد شرکت کنم ولی میدیدم که بچه ها خیلی درگیرن واسه دقت بالا تو بحث مدل سازی زبان و برام سوال شد کاری که اون قدر زحمت میکشیدند واسش و درگیری داشتند با ی درخت چطوری قابل انجامه
-> آخه درخت رو که برای مدل زبانی استفاده نکرده احتمالا کاربردش کوچیکتر بوده نیاز به مدل زبانی نداشته گفتم ابعاد کارش باید مشخص باشه دیگه با اون که نمیتونه یک مدل شبیه مدل زبانی با میلیارد پارامتر رو بسازه فکر کنم کل اون حرف اینه که همه جا از دیپ لرنینگ استفاده نکنید همین
-> اتفاقا پیرو پیام اولت رو اون پیام من میخواستم یک چیزی بگم ولی جلوی خودم رو گرفتم ولی الان میخوام بگم حالا شخصی یک کاری انجام داده آفرین بهش منتها اینکه شما الان یهویی تصمیم بگیرید که برم این رو بخونم اون رو بخونم منبع چی پیشنهاد میدی نظرت چیه و خوب نیست آینده شما دانشگاهی هست یا کاری اگه کاری هست شما مباحث اصلی رو بلد باش رزومه خوب هم بساز بعد یا برو مصاحبه یا از کسایی که در مصاحبه تجربه دارن سوال بپرس یا آگهیها رو نگاه کن اگه داشتههات کافی بود که استخدام میشی اگه نبود که برمیگردی و نقطه ضعفهات رو پوشش میدی شما فقط توی یک شرکت خوب استخدام شو اونها میدونن چطوری از شما کار بکشن و شما هم یاد میگیری چطوری خودت رو رشد بدی تازه توی کار به عنوان جونیور شما که راه حل پیشنهاد نمیدی لیدر داره و اونها مسیر رو مشخص میکنن مشکل من این هست که اصل کاری رو یکسریها رها میکنن و به فرعیات میچسبن الان یک روزت رو صرف همین پیام کردی من شما رو مخاطب قرار دادم ولی میدونم خیلیها درگیر چنین مسائلی هستن
-> سلام پاراگراف چهارم خیلی واسم جالب بود همیشه فکر میکردم باید همه چیز رو یاد بگیرم بعد وارد چیزی شم
-> بسیار عالی بود من اون پیام رو خوندم اینکه یکی بیاد و همه رو تخریب کنه بعد بگه من خیلی شاخم فقط یه معنی میده اونم این هست که طرف فکر میکنه استاد عالمه در کل هرچی بری منابع مختلف بخونی و فقط یه سمت راهه تجربه عملیاتی و تئوری ماجرا باهم خوبن حالا طرف فکر کرده باید بری همه چیو یاد بگیری بعد وارد کار شرکتی بشی این بزرگترین اشتباهه این رو نباید فراموش کرد متخصص عالم شدن بدون تجربه محاله چونکه منابع اموزشی انتها نداره حالا یه سوال از خود شخص خود شیفته داشتم ایا خودش فقط منابع خوانده و بعد مثل یه آدم فوق تخصص دارد شرکت ها شده یا مثل همون هایی که طبل تو خالی می دونست اولش گند زیاد زده و حالا نمی خواد بگه در ضمن ای دی طرف رو پیدا کردم و یه سوال تخصصی ازش می خوام بپرسم حالا ببینم با خودش چند چنده
-> سلام محسن شر نشه برات
-> نه قصد دارم از تجربیات دوستمون استفاده کنیم کانالش رو دیدم در توضیحات نوشته نظرات شخصی بعدش میاد همه رو تخریب می کنه خودش داره میگه نظر شخصیم هست یعنی حرفش هیچ دلیل و سندی نداره بعدش شدن دانشمند ایران کلا تو کانالش ۵۰ درصد حرفاش تخریب و نا امید کردنه فکر کنم قصدی از این کار داره کلا دور این افراد رو خط بکشید"
"-> چرا هیچ کسی نمیگه نمونه چطوریه و بفرست ببینیم
-> شاید در اون حد پیگیر نیستن
-> سلام حالا نمونه خاصی رو با استاد فیکس نکردم ولی مثلا یه بخش کراپ شده از این مسابقه قایقرانی میتونه یه جور ورودی باشه قایقا بعضی جاها پشت قایقای دیگه میرن و باز پیدا میشن"
"-> خطرناکترین گونه بچههایی که هوش مصنوعی کار میکنند آنهایی هستند که علم کامل و جزئی از دو چیز رو ندارند ۱ ریاضیات و درک ریاضی مدلها قرار نیست بشینید بصورت دستی محاسبات back propagate و رو انجام بدید اما اگر بهتون ی مسئله مشابه دادم یک نیروی فنی و حرفهای رو متقاعد کنید که چرا باید از راهکار پر هزینهتری به اسم هوش مصنوعی استفاده کنه ۲ مهندسی نرمافزار ساختمان داده الگوریتم و حتی نکات و جزئیات تخصصی زبان برنامهنویسی مورد استفاده متأسفانه توی فیلد کاری ما هزاران هزار آدم بدون تخصصهای فوق وجود داره و این ضرر بزرگی برای شرکتها هست برای خود اون آدمها هم ضرر هست چون هیچوقت آرامش نخواهند داشت یک پروژهای رو دیدم که نسخه ساده شدهاش این بود که طرف n تا داده متنی داشت که ثابت هست همیشه و میخواست بعد از چندتا کلمه تایپ شدن بهش ی لیست بده هرکدوم رو که انتخاب کرد بر اساس متن ۱ یا ۲ خطی بهش یک template چند صفحهای داده بشه برای پر کردن و شاید باورتون نشه اما برای اینکار از Transformer داشتند استفاده میکردند ۵ ماه بود که روی راهحل کار کرده بودند هنوز به پروداکشن نرسیده اما نیاز داشتند این موضوع رو optimize کنند برای همین با من تماس گرفتند و وقتی پروژه رو تحویل گرفتم بهم پیشنهاد کار مشاورهای با یک مبلغ بسیار خوب ساعتی دادند گفتم کل چیزی که میخواهید رو پیاده سازی میکنم جوری که حداقل ۲۰ برابر سریعتر باشه و البته سختافزار گرافیکی و هم نخواد بعد از صحبت و موافقت قرار شد اینکارو بکنیم ی ساختمان داده Trie توی پایتون پیادهسازی کردم و تمام n جمله رو بهش دادم توی میلیثانیه خروجی میده و دقتش از transformer ۶ ماه کار هم بیشتره و همهی این ۲ روز زمان گرفت بگذریم از اینکه هنوز پرداخت انجام نشده چون مدیر فنی روش نمیشه بگه پروژه ۲ روزه رو ۶ ماه الکی رفتن سراغ هوش مصنوعی نمیدونه من با مدیر مجموعه در تماس هستم ولی این بلایی هست که توهم هوش مصنوعی داره سر شرکتها میاره توصیه برای مدیران داشتن تیم هوش مصنوعی خیلی خوبه اما اگر خواستید نیرو استخدام کنید مطمئن باشید مواردی که گفتم رو آشنا باشند حتما مخصوصا حل مسئله مهمتر از اون هرکسی رو مدیر فنی نکنید و هوش مصنوعی معجزه نیست فرانت و بکند و هم نیست اینجوری نیست که بگید من از A میخوام برسم به B و چون گوگل اینکارو توی ۵ روز روی دیتاهای خودش انجام داده شما باید توی ۱۰ روز انجام بدید یادتون باشه استفاده از هوش مصنوعی پروسه و پروژهی زمانبری هست و برای همین هرجایی هم نباید وارد بشه
-> ببخشید در مورد اینکه چطوری از درخت برای language modeling استفاده کردید میشه یکم بیشتر توضیح بدید چون تاجایی که من میدونم درخت تابع بازگشتی نیاز داره و تابع بازگشتی خیلی هم بهینه نیست و برای اون مورد دوم که ذکر کردین میشه یکم جزئی تر توضیح بدید یعنی منی که اول راهم چه چیز های دیگه ای باید یادبگیرم
-> من خودم تو این زمینه ها آش خور هستم یکی برام فرستاده بود دوست داشتم گفتم با شما هم به اشتراک بزارم همین
-> چرا به این پیام دیس دادی
-> نگاه بالا به پایین نویسنده به چه علتچقدر ممکنه از خواننده پیام بالاتر باشه از لحاظ منطق و هوش که این شکلی مینویسهمگه کسی غیر از برنامه نویس ها و افرادی که تو حوزه هوش مصنوعی هستن توی این کانال حضور دارن اگه کسی هم مهندسی نرم افزار در حد پایه بلد نباشه و هم ریاضیات چطور مشغول به کار شده توی حوزه هوش مصنوعی درسته اینجا ایرانه و هیچی سرجاش نیست اما اون فرد چطور مدیر فنی شده اگه شرکت خصوصی باشه خب منطقا به دنبال سود خودشه و خیلی کم پیش میاد که بخاد فرد بی تجربه ای رو انتخاب کنه اومده یه کار رو توضیح داده و کلمه Transformerرو نوشته به عنوان راه حل مگه ترنسفورمر راه حله حتی به این فکر کردم که اسم این تسکی که داشتن انجام میدادن رو هم بلد نیست فیلد کاری ایشون چی هست که هزاران هزار ادم بدون تخصص وجود داره و آره اومده کلمه ی توهم هوش مصنوعی رو هم به کار برده درست میگه اینجاش رو اون هم وقتی که ندونی داری چیکار میکنی من این نویسنده رو نمیشناسم ولی میدونم این فرد هم انسان خطرناکی هست برای جامعه هوش مصنوعی چون صاحب جایی هست که صداش رو یکی دو هزار نفر میشنون
-> استاد ببخشید در رابطه با این پیام ی چیزی ذهن منو خیلی درگیر کرده اون موقع تاحالا چطوری با درخت چیزی رو که میخواستند پیاده سازی کردند شما احیانا میتونید حدس بزنید من که تجربه ای ندارم و هرچی فکر میکنم راهی به ذهنم نمیرسه
-> 
-> نمیدونم"
"-> سلام روزتون بخیر دوستان یه مقدار کمک لازم دارم اگه کسی اطلاعات داره کمکم کنه ممنون میشم من رشته تحصیلیم کامپیوتر نبوده هیدرومکانیک خوندم که توی حوزه مهندسی دریا هست و این ترم یه درس دارم توی حوزه بینایی ماشین ترم پروژهای که تعریف کردم ترکینگ و رهگیری یه کشتی مشخص توی محیط دریا و ترافیک دریاییه با توجه به وقت کمی که دارم و چیزی به انتهای ترم نمونده مستقیم رفتم سراغ اون بخش siamrpn که توی درس بینایی ماشین هوسم هست که مرتبط بخش ترکینگه چند تا بخش مبهم توی ذهنمه ۱ یکی اینکه آیا من حتما باید یه دیتاست ویدئویی مرتبط با حرکت کشتیها توی دریا داشته باشم و شبکه رو با اون دیتاست خاص آموزش بدم یا اینکه با همون شبکهای که با TrackingNet آموزش دیده میتونه هدفی که من دنبالش هستم رو انجام بده ۲ دوم اینکه اگه جواب مثبته چطور میتونم اون شبکهای که شما رو با مثلا با همون trackingnet آموزش بدم ۳ سوم هم اینکه بعد آموزش چطور میتونم یه ویدئو که مثلا از آپارات یا یوتیوب دانلود کردم رو به شبکه بدم و بهش بفهمونم مثلا برو اون کشتی خاص رو بین بقیه کشتیها دنبال کن اگه کسی اطلاعاتی داره کمکم کنه ممنون میشم
-> سلام اگه فقط یه سری فیلم دارید که یک کشتی توی یه دریاست و میخواید ترکینگ کنید و دیتاست هم ندارید میتونید احتمالا از تکنیک های ایمیج پروسسینگ استفاده کنید
-> یک مبحثی هست تحت عنوان zero shot learning که فکر میکنم شاید به درد این مساله بخوره
-> سلام به نظرم تکنیک های ایمیج پروسسینگ اولویت داره در اینجا چون به هرحال فقط وقتی لازمه باید از شبکههای عمیق بهره برد
-> سلام ممنون از توضیحتون چون استاد درس یه کم روی مباحث یادگیری عمیق تمرکز داشت پرسیدم یه شناور تنها نیست مثال بهتر بخوام بزنم مسابقات قایقرانی رو تصور کنین که تعداد زیادی شناور یا قایق با سرعت در حال حرکت هستند و میخوایم یکیشون رو دنبال کنیم
-> این ینی اینکه برای مساله من اصلا نمیشه از روشای عمیق استفاده کرد و حتما باید دیتاست شناوری داشت یا اینکه بحث سختتر شدنشه
-> سلام همون حرف آقای دکتر شد اصولا باید نمونه دیده بشه ولی اگر شرایط تصویر خیلی پایداره میتونید از ایمیج پروسسینگ استفاده کنید
-> در مورد این موضوعی که گفتین باید مطالعه کنم دیدی ندارم که چیه
-> فکر کنم نیازی نیست مطالعه کنید
-> چرا استاد
-> ترکینگها اینطوری ترین میشن که آبجکتهای درحال حرکت رو ترک کنن معمولا هم در فریم اول شما مشخص میکنید که این آبجکت رو ترک کن مثلا یک کشیدن یک باکس دور هدف و بعد ترکر هم اون هدف رو در فریمهای بعدی ترک میکنه به نظرم نیازی هم نیست که ترکر روی فرضا دریا یا هر چیز دیگه ترین بشه چون همون آبجکتی که کاربر میده رو به عنوان مرجع داره و توی فریمهای بعدی از همون مرجع استفاده میکنه از طرفی زروشات و اینجور مسائل به نظرم اینجا کارکردی نداره بنده خدا بیشتر گمراه و گرفتار میشه
-> ممنون از توضیحتون گمراهتر از این وضعیت فعلی حتی
-> آره حالا نخواستم گمراهتر بنویسم قبل از هرچیزی به فکر مساله و دیتاست باشید ارائه راهحل و متد زمانی شروع میشه که مساله و دیتاست واضح و مشخص باشه
-> درد اینه که دیتاست توی این حوزه ندیدم نمیدونم اون دیتاست حجیم TrackingNet میتونه برای مسأله ای که من درگیرش هستم پاسخگو باشه یا نه"
"-> سلام دوستان توی سگمنتیشن اگر تعداد کلاس های ما باینری نباشه و مثلا ۵ تا کلاس باشه ولی کلاس های مختلف با هم overlap داشته باشند این میشه multi label یا همون مالتی کلاس هست و اینکه ماسک اینطور حالتی رو چطور میشه درست کرد چون یک پیکسل خاص ممکنه به چند تا کلاس تعلق داشته باشه
-> سلام میشه مولتی لیبل خروجی مدل و تارگت باید به این شکل باشه که هریک از کلاسها در یک کانال جداگانه باشن یعنی اگر 5 تا کلاس دارید 5 تا هم چنل دارید اینجوری اورلپها هم پوشش داده میشه لاس فانکشن هم میتونه ترکیبی باشه میتونید از دایس هم توی مود مولتی لیبل استفاده کنید یا اینکه باینری کراس آنتروپی بذارید دایس فکر کنم راحتتر باشه کتابخونه pytorch segmentation models رو بردارید
-> سلام خیلی ممنونم آقای دکتر پس اگر مثلا ۵ تا کلاس داشتیم تصویر ماسک رو که بخواهیم آماده کنیم باید ۵ کاناله باشه و اینکه اینطور ماسکی رو که ۵ کاناله باشه میشه به فرمت های رایج تصویر سیو کرد
-> بله
-> ممنونم از لطف تون آقای دکتر"
"-> سلام دوستان دوستی میتونه راهنمایی کنه این jigsaw vit چه کاری میکنه یا چه تغییری ایجاد کرده
-> داره هریک از پازلها پچهای ورودی رو براساس موقعیتشون شمارهگذاری میکنه
-> سپاس از پاسخگویی شما مگه ترانسفورمر vit هم همین کار رو نمیکنه
-> نه ویت تکلیفش مشخصه وظیفش سورت کردن پچها نیست توی ویت باید به هر پچ پوزیشن انکدینگ هم اضافه بشه یعنی باید موقعیت مکانی پچ ذکر بشه اما همونطور که در بخش بالایی شکل نشون داده شده اینجا نیازی به پوزیشن انکدینگ نداره
-> خیلی ممنونم"
"-> سلام وقت بخیر استاد دوره پردازش تصویر مشخص نشد کی ریلیز میشه
-> سلام نه هنوز اینجا اطلاع میدم
-> سلام مجدد توی یکی از تمرین ها گفتید با cpu و gpu ران کنیم شبکه عصبی mlpرو من وقتی با cpu میزنم ۷ ثانیه ای ترین میشه وقتی باgpu میزنم ۱۷ ثانیه دلیل خاصی داره
-> شبکه کوچیکه
-> بله
-> خب الان جی پی یو شبیه ببری هست که در حد گربه داره ازش استفاده میشه محاسبات با حجم کم قابلیتهاش رو نشون نمیده"
"-> سلام استاد ایا برای شروع دوره یادگیری عمیق بلد بودن پردازش تصویر لازمه یا در خود دوره قسمت های مهم پردازش تصویر گفته شده در دوره یادگیری ماشین پیش پردازش داده گفته شده
-> 
-> واقعا ممنونم از راهنماییتون"
"-> سلام دوستان من دوتا اکانت google colab دارم به دلیل محدودیت زمانی که colab در استفاده داره مدل یادگیری عمیقم رو روی یکی از این colabها train میکنم چند epoch و اون مدل رو در گوگل درایو سیو میکنم مثلا ده epoch مدلم رو ترین میکنم و توی گوگل درایو سیو میکنم حالا میام تو اکانت دوم کولبم همون مدل رو لود میکنم و میذارم از epochعه یازده ترین بشه چالش اینکه loss و accuracy چندین پله بد میشه راه حلی دارین
-> 
-> سلام وقتتون بخیر من داده هایم که توالی بودن با one hot بردار بدست امده بود با ابعاد مختلفی که فرمودید را با مدل هایی که فرمودید را اجرا گرفتم هیچ کدوم اسپیرمن و پیرسون بهتر از 43 و 52 نشدن ویدیو های صوت را هم دیدم با wav2vac هم نمیشه با word2vec هم انجام دادم و دوباره اومدم با توکنایزر dna bert توالی ام را به 4 تایی توکن کردم و بردار 7681046710 گرفتم دادم به CNN به حدود 33و39 رسیدم ولی مدل ام اورفیت میشه هر کاری می کنم اوکی نمیشه من می تونم اینطوری به مسئله ام نگاه کنم اول بیام توالی هایم را که در دو رنج منفی و مثبت هستند را به دو کلاس تقسیم کنم ابتدا کلاسیفیکیشن کنم بعد بیام برای هر قسمت یک مدل رگرسیون بزنم این طوری ممکنه بتونم نتیجه بگیرم
-> استاد ممنونم جواب دادین تو این پروژه Optimizerی که استفاده کردم sgd هست و scheduler هم استفاده نکردم و اینکه optimizer و model رو به صورت torchsavemodelstate_dict model_PATH torchsaveoptimizerstate_dict optimizer_PATH سیو کردم و با دستور modelload_state_dicttorchloadmodel_PATH a optimizerload_state_dicttorchloadoptimizer_PATH لودشون کردم اما بازم این چالش رو داشتم
-> میتونه مورد دیگهای هم باشه که باید درنظر بگیرم
-> حضور ذهن ندارم ولی بهینهساز sgd بدون تغییر لرنینگ نیازی به ذخیره نداره شاید خطای کدنویسی داشته باشید ما این کار رو توی دیپ و دیپ کاتالیست زیاد انجام دادیم حالا کولب رو عوض نکردیم ولی ترین دوباره رو زیاد انجام دادیم
-> با همون اکانت کولب مجدد ترین میکنم همه چیز درسته اما کولب رو تغییر میدم این اتفاقات میفته شاید به خاطر تغییر کولب باشه یا توجه به نکاتی که شما فرمودین
-> جالب شد"
"-> سلام ببخشید من چند تا مقاله میخوام که در مورد مشکل overfitting در mlp راه حل داده باشن ولی متاسفانه هرچه سرچ میکنم مقاله های عیر مرتبط میاد یا از چی gpt که استفاده میکنم لینک مقاله نمیوه حتی سرچم میکنم چیزی نمیاد
-> فکر نکنم حل مشکل اورفیتینگ توی مقالهها مطرح بشه اگه دنبال مرجع هستید بیشتر توی کتابها باید دنبالش بگردید"
"-> سلام وقت بخیر شکل بالای وزنای یه شبکه mlp سه لایس قبل ترین پایینی ها بعد ترین هستن توی سایت پایتورچ نوشته فرمول وزن دهی لایه بصورت زیر یک توزیع یکنواخت در بازه sqrt1 in_features تا sqrt1 in_features بعد ترین مشاهده میشه توزیع وزن لایه ها تقریبا نرمال شده این اتفاقی اینجوری شده و اینکه از همون اول بجای توزیع یکنواخت نرمال وزن دهی میشد سریع تر همگرا نمیشد یا نتیجه بهتر نمیداد
-> 
-> ممنون استاد دوره بینایی رو دارم ولی نرسیدم بهش فعلا یک سری ازمایش دیگه انجام دادم دیدم وقتی لرنینگ ریتو بزرگ کردم سرعت همگرایی بیشتر شد ولی محدوده وزن ها هم بیشتر شد مثلا تا منفی ۲ خورده ای رفت ولی وقتی کوچیک کردم محدوده وزن ها کوچیک شد باز منتها خیلی از این نتایج رو متوجه نمیشم اتفاقی رخ میده یا واقعا دلیل داره بارها کتابای مختلف رو نگاه انداختم ولی هیچکدوم ازینا داخلش نبوده
-> سلام وقتتون بخیر این برای مسئله رگرسیون بوده یا دسته بندی
-> سلام برای رگرسیون
-> پس شاید بشه گفت که دلیل اینکه وزن ها نرمال شدند اعمال توابع اتلاف MSE یا MAE هستش چون با MLE ثابت میشه که این توابع اتلاف براساس توزیع های گوسین و لاپلاسین بدست اومدن یک قسمتی از دوره یادگیری ماشین راجع به این مسئله بود مطمئن نیستم استاد باید تایید کنند
-> تست کردم ارتباطی نداشت MAE گذاشتم ولی بازم تقریبا نرمال شد درصورتی که باید لاپلاسین میشد
-> همزمان با لرنینگ ریت باید لرنینگ کرو رو هم رسم کنید شاید وزنها که بزرگ شده اورفیت رخ داده"
"-> سلام وقت شما بخیر بنده دنبال متخصص برای تدریس هستم مباحث بینایی ماشین Deep learning هم برای تدریس هم برای رفع اشکالات برنامه نویسی ۲۰ دقیقه اولین جلسه جهت اعتماد و تسلط مدرس رایگانه حتما رزومه ارسال شود
-> "
"-> سلام خسته نباشید من توی ژوپیتر نوتبوک کد میزنم ی مشکلی داره اینه ک حتمن با tab بزنی تا کلمات رو پیشبینی کنه من پایچارمم نصب دارم میخواستم بدونم توی پایچارم نمیشه مثل جوپیتر نوتبوک کد نویسی کرد یعنی سلول ب سلول کد بزنم اونجا
-> سلام نسخه پولی پایچارم چنین قابلیتی رو داره البته به نظرم ژوپیتر برای کار با نوتبوکها بهتر هست من خیلی مشکلی با تب زدن ندارم شاید هم افزونه نصب کنید بدون تب بهتون نشون بده
-> خب مشکلاتش یکی دوتا نیست استاد ولی محیط شما خوبه تو ویدئوها اون ژوپیتر لبه یا ip
-> lab
-> ممنون استاد"
"-> سلام من شبکه های بازگشتی رو دارم روی یه سیگنال پنج کلاسه اجرا میکنم ولی فارغ از اینکه چه معماری RNN LSTM GRU با چه تعداد لایه و ابعاد پنهان همیشه بهم بدترین دقت ممکن ۲۰ رو میده حتی برای train با اینکه خیلی دارم کدمو چک میکنم ولی متوجه نمیشم ایراد کار از چه قسمتیه همین داده رو با یه لایه MLP با هزار تا نورون دقت ۴۰ میگیرم و یا همین شبکه رو بهش دیتای اموزشی دیگه میذارم دقت ۵۰ درصد میشه
-> به نظرم بدون تریس و دیباگ داده و کد نمیشه نظر داد فقط این به ذهنم میرسه که در دادههای سری زمانی یک مشکل رایج آمادهسازی داده به شکل مناسب هست داده باید به شکل BxLxF باشه
-> بله به همین صورته من حتی خروجی مدل قبل از دادن به لایه فولی کانکتد هم چک کردم و ابعاد درست بودن قسمت عجیب کار اینه که همین داده بدون هیچ تغییری با MLP یا CNN ساده داره جواب میده و حتی خود مدل هم روی داده ی دیگه هم عملکردش بهتره و جدای از این ها یک مدل بازگشتی با کلی پارامتر باید توانایی اورفیت شدن رو داشته باشه"
"-> تصویر چی هست سی تی یا ام ار
-> یک نوعی از CT هست"
"-> سلام وقت همگی دوستان بخیر من یکسری تصاویر پزشکی دارم که پزشک بر اساس تفاوت شدت روشنایی دو ناحیه مختلف از تصویر یک عددی به اون تصویر اختصاص میده که در واقع میشه رگرسیون به نظرتون میشه ما این دو ناحیه رو جدا کنیم و بعد به شبکه ای مثل سایامیز بدیم تا اون عدد رو به دست بیاریم
-> یک نمونه تصویر بفرستید شاید بهتر بشه متوجه شد
-> سلام خیلی ممنونم آقای دکتر چشم الان به تصاویر دسترسی ندارم ولی حتما خدمت تون میفرستم"
"-> سلام وقت تون بخیر برای ابجکت دیتکشن باید در تصاویر مورد استفاده باکس ابجکت هایی رو که می خواییم مشخص کنیم درسته منظورم اینه که باید دیتا باکس ها مشخص شده باشن که اگه تعداد تصاویر و تعداد ابجکت ها مون زیاد باشه خوب این تنظیم کردن باکس ها خییلی وقت گیر میشه حالا سوالم اینه که ایا روش خودکاری وجود نداره برای این کار
-> احتمالا گراندتروث منظورتون هست بستگی به آبجکتها داره اگه آبجکتهای رایج باشه شاید ابزارهایی مثل روبوفلو یا cvat کار تهیه گراندتروث رو آسون کنن یک کارهایی میشه کرد بستگی به میزان مهارت و دانشتون توی این حوزه داره"
"-> سلام استاد قسمت یادگیری تقویتی یادگیری ماشین ۲۰۲۲تا عید آماده میشه
-> سلام الان مدرس یکسری ویدئوی اولیه به ما داده که بررسی کنیم تلاشمون رو میکنیم که برسونیم"
"-> سلام استاد وققتون بخیر یک دوره دیپ کاتالیست هم برای یادگیری ماشین میخواستین ارائه بدین خواستم عرض کنم اگه یکی دوتا پروژه رو با پایچارم هم انجام بدید خیلی عالی میشهیا وی اس کد
-> سلام برنامه داریم که برگزار کنیم یکم توی دوره برگزار کردن زجرکش میکنیم متاسفانه به خاطر وسواس و ولی خب خیلی به این دوره فکر میکنیم پروژه با پایچارم رو توی دیپ کاتالیست میذاریم توی یادگیری ماشین نوتبوک بهتر هست
-> استاد ببخشید مباحث دوره کاتالیست ماشین لرنینگ مشخصه میخواستم اگر میشه ازتون بخوام روی داده هایی که tabular نیستند هم متد های ماشین لرنینگ رو اجرا کنید البته اگر شدنی هست و بار آموزشی ای توش میبینید
-> چند تا از موضوعهاش مشخص شدن ولی کامل نیست دادههای غیرتبولار مثل چی
-> بیشتر عکس استفاده svm برای داده های تصویری مثلا
-> آخه دیگه این کارها خیلی کمرنگ شده من ترجبح میدم پروژهها چیزهایی باشن که بروز باشه با چه هدفی چنین چیزی بدردتون میخوره
-> استاد دورههای بعید ترم ۲ دیپ کاتالیست و بینایی کامپیوتر باشه
-> بیشتر بار اموزشیش مد نظرم بود وگرنه همونطور که فرمودین کمرنگ شده استفادش
-> این رو احتمالا توی یک دورهای مثل بینایی کامپیوتر در حد دستگرمی میشه گفت تا الان که نظرم این هست که ارزش نداره توی ماشین کاتالیست باشه
-> سلام ترم 2 دیپ کاتالیست چی هست ماشین کاتالیست منظورتون هست
-> مثل دیپ کاتالیست چندتا پروژه جدید باشه
-> آهان همین دیپ لرنینگ با یکسری پروژه جدید نمیدونم برای دیپ کاتالیست 2 برنامهای نداریم"
"-> سلام آقای دکتر روزتون بخیر خوب هستین آموزش opencv کی شروع میشه
-> سلام ممنون سلامت باشید فکر کنم دوستان حکم جلبمو برای بدقولی اوپن سی وی بگیرن زمانش رو هنوز مشخص نکردیم بابت بدقولی عذر میخوام
-> سلام استاد دوره پایتون هم لایبرریهاش تکمیل میشه
-> بله حتما
-> انشاءالله تشکر
-> سلام استاد ارادت داریم در تدارک یه حکم جلب برای یادگیری تقویتی هم هستیم
-> سلام اختیار دارید آقای دکتر ممنونم"
"-> سلام ببخشید استاد مشاوره دیگه در سایت وجود نداره
-> سلام مشاوره ماهیانه رو کلا حذف کردیم مشاوره ساعتی داریم
-> سلام میشه لطفا هزینه مشاوره ساعتی رو هم اعلام کنین
-> بستگی به کار داره اینا رو از پشتیبانی پیگیری کنید"
"-> سلام دوستان آیا در بحث کلاسیفیکیشن تصویر راهی هست که برای شبکه مشخص کنیم به قسمت های خاصی از تصویر توجه کنه
-> معمولا با طراحی یک ماژول اتنشن میشه از این کارها کرد من قبلا در یکی از مقالاتم چنین کاری رو انجام دادم بازم از این نوع مقالات هست من هم توی رفرنسهای مقاله بالا گفتم
-> چه جالب خیلی ممنونم آقای دکتر"
"-> دوستان سلام مقالات ieee رو چطور میشه دسترسی رایگان پیدا کرد
-> معمولا سایتهای داخلی مثل freepaper و این چیزا هستند که دانلود محدود میدن
-> ممنونامتحان کردم ولی پیغام امکان دانلود نیست برام میارهدلیلش چی هست
-> معمولا این پیغام وقتی میاد که خود سایت هم دسترسی نداره به مقاله احتمالا کاریش نمیشه کرد"
"-> سلام وقت بخیر آیا در object detection سایز فیلتر کانولوشن اهمیت دارد
-> سلام یعنی چی
-> یعنی اینکه مثلا بخوایم یه برج را دیتکت کنیم یا یک گلدان را باید در سایز فیلتر های کانولوشن تغییر ایجاد کنیم شرایط scale یکسان مثلا اگر هدف مون اجسام بزرگتر باشه پنجره ۱۱ در ۱۱ بگذاریم مثل الکس نت برای اجسام کوچکتر ۳ در ۳ منظورم اینه سایز شی که میخوایم دیتکت کنیم در اندازه فیلتر دخالتی دارد
-> 
-> خیلی ممنون از توضیحات تون"
"-> استاد سلام خوبین در مورد explainable AI منابعی میشناسید من یه کاری رو انجام دادم برای اساتیدم در گروه زیست شناسی منتها میگن این دیپ لرنینگ black box هست و شما باید بتونی دقیقا توجیه کنی این مدل چیه من بهشون همون فرایند فوروارد و بک وارد رو توضیح دادم ولی قانع نشدن
-> سلام من منبعی سراغ ندارم نمیدونم دنبال چی هستن ولی کلا تحلیل مدل مهمه یعنی همونطور که تحلیل داده داریم تحلیل مدل هم داریم شما با آزمایشهای مختلف تحلیلهاتون رو به اونها نشون بدید
-> درسته تحلیل مدل شامل چی مثلا مثلا بهشون بگم چرا از این فیچر ها استفاده کردیم مثلا درسته
-> سلام جنس دیتا و شبکه ای که استفاده کردین چی هستش
-> نه مثلا مدل کدوم یک از فیچرهای ورودی رو مهمتر میدونه و بیشتر بهشون وزن میده یا برعکسش کدومها کمتر مهمن حالا شما چطوری این رو تفسیر میکنید که چرا به یکسری از فیچرها وزن بیشتری داده و یکسری وزن کمتر یا مثلا اگه داده تصویر هست کجای تصویر برای مدل مهمتره بر اساس چه نواحیای از تصویر تصمیمگیری انجام میده همینطور متن صوت و طبیعتا توی کارهای شما منطق مدل در تصمیمگیری رو باید با منطق انسانی مقایسه کنید یعنی یک متخصص برای یک داده چطوری تصمیمگیری کرده و مدل شما چطوری تصمیمگیری کرده اگه محدودیت ندارید بیشتر در مورد موضوعتون و کارکرد مدلتون بگید
-> خیلی ممنونم استاد لطف کردین پس روی فیچر ها میتونم وقت بزارم و توضیح بدم بهشون"
"-> درود استاد امیدوارم شما هم شب خوبی داشته باشید یلداتون خجسته باد
-> ممنون محمد جان یلدا مبارک ایشالا همگی شب خوبی داشته باشید"
"-> استاد سلام تجربه کار با بیگ دیتا رو دارید بجز Pyspark و Pandas پیشنهادی داریدکند هستند متاسفانه الزامی هم نداره حتما از پایتون باشه ولی قابلیت های Pandas رو تا حدودی پوشش بده
-> سلام متاسفانه تو این زمینه تخصص ندارم
-> از Dask استفاده کنید
-> این رو ببنیدی شاید بدردتون بخوره سازمانهای بسیار بزرگ از این استفاده میکنند برای کار با بیگ دیتا
-> درود تعداد سمپل ها فایل و مکانیزم ذخیره سازی دیتابیست چیه کانفیگ رم و هاردت چیه قراره فقط واکشی دیتا داشته باشی یا مصور سازی و ریئل تایم یا آفلاین هست دیتا
-> این تیریپ چیزا که توی سایتشون زده بیشتر جنبه ی تبلیغات و یک لانچ ساده داره مثلا توی دموی اول سایت داره یک فایل csv رو از AWS لود میکنه این حرکت کلا قفله بین توسعه دهنده ی AWS یا کار با بیگ دیتا بیشتر توی کتاب های آموزشی به عنوان تسک ساده هست از طرفی کسی که داره با سیستم ذخیره سازی AWS کار میکنه دیگه فایل csv رو لود نمیکنه یا منطقی نیست مستقیم از دیتابیس های خود AWS استفاده میکنه تا فرایند مانیتورینگ و لاگ و به راحتی قابل کنترل باشه ناسا و وزارت دفاع و و کلا زیرساخت های آمریکا دیتاهاشون پایپ لاین های متفاوتی دارن معمولا از خدمات آپاچی و aws برای این کارها استفاده میکنند
-> بالای ۶۰ میلیون ردیف و حدود ۲۵ ستون فایل های csv حاصل از Sql server دیتا آفلاینه و سیستم فاقد گرافیک در واقع رو سرور های خود سازمان پردازش انجام میشه و سخت افزار کیس درگیر نیستنوعی محیط مجازی مصور سازی درکار نیست
-> درود تقریبا بالای 1 میلیارد سلول میشه با CSV نمیصرفه کلا برای انتقال چنین فایل هایی از Parquet استفاده کن البته تسک دقیقت رو نمیدونم اما اگر صرفا میخوای Query بیس بری جلو حالا میخوای یک سری دیتاها رو محسابه کنی یا سرچ کنی و در حالت عادی بهتره MSSQL روی سیستمت نصب کنی و باهاش کار کنی یعنی کلا کوئری نویسی رو روی همون انجام بدی یا بهتره مهاجرت بدی دیتاهارو به سمت PostgreSQL اگر دیتاهات Text محور هست بهتره بری سراغ Elastic ک اونجا دستت کلا بازتره یکمی رم زیادی مصرف میکنه نسخه جدیدش تا 16 گیگ رم هم استفاده میکنه در هر صورت برای این تیریپ کارت الستیک گزینه ی مناسب تری هست در کل گام اول انتخاب زبان نیست نوع ذخیره سازی دیتاهات همچنین انتخاب نوع داده و نرمال سازی ذخیره دیتاهات مهمتره اگر هم میخوای با کد کارات رو انجام بدی آداپتور یا انجینش رو نصب کن مستقیم وصل شو ب دیتابیس برای این حجم از دیتا از فایل واسط استفاده نکن
-> ممنون از راهنماییتون"
"-> استاد سلام وقت بخیر opencv کی میزارین تو سایت گفته بودین اذر
-> سلام دوره opencv هنوز خوب جا نیفتاده هنوز روش کار میکنیم از نظر ما صرف اینکه یکسری دستور opencv توضیح داده بشه ارزش بالایی نداره از طرفی خود پردازش تصویر و روشهای کلاسیک چیزی نیست که الان به وفور استفاده بشه همینطوری ساده به نظر میاد ولی خب ما هنوز به سیلابس قانعکننده نرسیدیم
-> استاد ببخشید در مورد خود دوربین و تصویر برداری هم توضیح داده میشه تو دوره یا هنوز تصمیمی گرفته نشده
-> ایشالا تا دی جمع شه البته واقعا ممنون بابت وقت و حساسیتی ک شما و تیم روی سیلابس ها ی درسی میزازین دوره حالت practical طور باشه خیلی خوبه و حالت پروژه اینا باشه مثلا بحث استفاده از مدل های پایتورچی caffeetensor و بحث real time detections tracking البته جسارت نباشه این پیشنهاد من بود
-> این چیزا بخشهای ساده کار هست حالا نهایت میشه یک ساعت مطلب
-> تشکر استاد ببخشید احیانا اگر هنوز میشه پیشنهاد داد واسه دوره میخواستم بگم کتابخونه mediapipe رو هم شاید بشه در نظر گرفت واسه تدریس من خودم سطحی باهاش کار کردم و خیلی عمیق نشدم ولی بعضی جاها مثل face detection کتابخونه opencv خیلی کند عمل میکرد در حالی که mediapipe خیلی سریع تر بود
-> ممنونم به نظرم توجیه علمی نداره که مدل رو ببریم داخل opencv قبلنا که تست گرفته بودیم مدل رو روی جیپییو نمیتونست اجرا کنه اخیرا رو چک نکردم دیتکشنش ضعیفه ولی ترکینگ میتونه تا حدی خوب باشه سگمنتیشن هم ضعیفه کلا تسک دیتکشن و سگمنتیشن معمولا مدل قوی میخواد که با opencv به نظرم نمیشه
-> برای بحث دیتکشن چه فریموورکی پیشنهاد میکنید یعنی خودتون باهاش حالا کار کردید
-> یولو 8 خیلی خوبه سبک تا سنگین خیلی هم منعطف هست سگمنتیشن هم داره
-> اوپن سی وی توی چرا توی فیس دیتکشن کند بود
-> ممنون بعد سوال دیگر الان تو صنعت opencv کجاها استفاده میشه
-> بهتر بود به جای اینکه بگم opencv کنده بگم مدیاپایپ سریع تره ما یک سری عملیات رو میخواستیم روی صورت انجام بدیم باز و بسته شدن دهن و چرخش سر و برای این کار از opencv استفاده کردیم باید روی حدودا 300 تا ویدئو این عملیات رو انحام میدادیم که با opencv دیدیم خیلی طول میکشه فکر میکنم 3 ساعت و نیم تقریبا طول میکشید ولی مدیا پایپ خیلی سریع تر ویدئو ها رو پردازش میکرد کمتر از یک ساعت دلیل اینکه سرعت opencv کمتر بود رو راستش بررسی نکردم
-> والا اگه روشها مبتنی بر دیپ باشه به نظرم opencv کم استفاده هست حتی توی بحث گرفتن فریمها هم ممکن هست از چیزای دیگه استفاده بشه هرچند توی بحث گرفتن فریم قوی هست و یک کتابخونه سی پلاس خیلی خوب داره
-> برام جالب بود ممنون البته برام مهم هست که چرا اوپنسیوی کندتر بود
-> شاید البته من کدنویسی من خوبی نکردم در مورد خود کد هم یادمه از همچین چیزی واسه دیتکتور استفاده کردیم cv2CascadeClassifierHaarcascade_frontalface_defaultxml که اون فایل xml رو باید جدا دانلود میکردیم هرچند حجمش کم بود ولی چون باید یک سری اطلاعات رو از فایل خارجی میگرفتیم سرعت رو کم میکرد در حالی که مدیا پایپ همه چیز رو built in داشت و خیلی سریع تر بود تازه ی چیز دیگه ای هم که بود این بود که واسه ی بخشی که میخواستیم لندمارک بزینم روی صورت opencv حدودا 40 تا لندمارک داشت ولی مدیا پایپ یادمه تا 200 تا لندمارک هم میزد روی صورت تازه مختصات 3 بعدی x y z و باز هم با این وضع مدیاپایپ سریعتر بود"
"-> 
-> کدهایی که نوشته توی کتاب مال اونایی هست که یا مهندس شرکت گوگل هستند یا رنکینگ بالایی توی حل چالش های کگل دارن خیلی دید میده برای مهارت کدنویسی و فاین تیون کردن و"
"-> 40 تخفیف ویژه یلدا قابل استفاده برای همه دورهها کد تخفیف YALDA4EVER مدت محدود برای تهیه دورهها به وبسایت هوسم مراجعه فرمایید در صورتی که سوالی در مورد دورهها داشتید به آیدی زیر پیام دهید
-> شامل دوره جدیدopencv ام میشه
-> اونکه تخفیف جداگانه داره همیشه اول دوره تخفیف میذارند"
"-> سلام ببخشید آموزش خوب برای tensorflow and keras میشناسید
-> یه کتاب هست ۶۰ تا رسپی برای مسابقات کگل ۶۰ تا چالش کگل رو با بهترین سولوشن ها حل کرده با تنسورفلو و کراس هست خیلی عالیه
-> ممنونم ازتون امکانش هست معرفی کنید
-> "
"-> سلام اگر بخواهیم برای تشخیص پنج کلاس مختلف از یادگیری انتقالی با شبکه گوگل نت استفاده کنیم چه تغییرات اساسی باید در شبکه گوگل نت ایجاد کرد به جز تغییر لایه اخر از ۱۰۰۰ کلاس به ۵ کلاس
-> عجب سوالی
-> سلام وقت بخیر همین تغییرات تعداد نورونای لایه اخر کافیه اینکه چندتا لایه رو فریز کنید یا نکنید که بستگی داره به میزان شباهت تسکتون به شبکه pretrained نوع شبکه هم که میزان پیچیدگی دیتاستتون بستگی داره
-> ولی استادم گفتن چند نکته خیلی اساسی داره اونا رو پیدا کنید و دقیقا هم گفتن یکی از اون نکات تغییرات نورون آخر هست و از ما خواستن جوابهای دیگه و پیدا کنیم
-> من کلیات رو گفتم وگرنه کلی ریزه کاری داره اینکه مثلا از لایه چندم تا اخر فریز کنی میتونی روی انتخاب هایپرپارامترا اثر گذار باشه و
-> متشکرم
-> فکر میکنم باید ورودیتون رو هم بر اساس ورودی شبکه گوگل نت نرمالایز و هم بعد کنید
-> بسته به مشابهت مجموعه داده تون با مجموعه داده مدل یادگیری شده و حجم داده باید تصمیم گیری کنید"
"-> سلام استاد وقتتون بخیر من یک شبکه عصبی را با استفاده از ترنسفرلرنینگ ترین میکنم و خروجی های خوبی هم میگیرم اما وقتی می خواهم آموزش را از ابتدا انجام دهم نمی تواند به دقت مورد نظر برسد آیا این احتمال هست که داده های آموزشی من پراکندگی لازم را ندارند و باید سعی کنم با الگوریتم هایی داده ها را بجای رندوم بصورت انتخاب از هر خوشه انتخاب کنم یا علت دیگری می تواند داشته باشد شبکه عصبی مورد نظر ranknet می باشد و شامل سه تا ۴ لایه فولی کانکتد می باشد
-> سلام ببینید علتی که بسیار مقالات و یا سایت ها بشدت توصیه میکنند که حتما از وزن های pretrained استفاده بشه و عملیات ترنسفر لرنینگ انجام بشه اینکه شما زودتر به همگرایی برسین در custom دیتاستی که دارین درنتیجه شما ترینینگ تایم کمتری خواهید داشت و دوما اینکه مدل شما با استفاده از وزنهای pretrained بسیار از فیچرهای ساده و حتی کامپلکس رو از قبل میشناسد به این دلیل که مثلا روی یک دیتاستی ترین شده که مثلا هم تنوع پذیری بالا داشته و هم از لحاظ کمی قابل قبول بوده لذا در فرایند بروی کاستوم دیتاست حتما ترنسفر لرنینگ مورد توجه هستش در صورتی که کاستوم دیتایی که دارین از لحاظ کمی و کیفی مناسب نباشد مدل شما در حال from scratch قطعا نمیتواند ویژگیهای مناسب رو استخراج کند و درنتیجه اون همگرایی لازم در شبکه رو بجود نمیاد حالا اگر منظورتون از پراکندگی همون تنوع پذیری دیتاست میتونه دلیل بسیار مهمی باشه برای این چالش برای همین اگر دیتای مناسبی از جهت کمی و کیفی ندارین مجبورین از ترنسفر لرنینگ استفاده کنید البته این نکته هم بسیار حائز اهمیت هستش که رفتار با شبکه درحالت from scratch با transfer learning کاملا متفاوت هستش خصوصا برای تنظیم هایپرپارامترها مثل learning rate و که حتما خودتون میدونین
-> ممنونم از توضیحاتتون من به دنبال پیدا کردن بهترین شبکه ها در یک دیتاست شامل شبکه و دقت آنها هستم پس از طرفی می خواهم با کمترین دیتا به بهترین پاسخ برسمیعنی معادل با کمترین آموزش شبکه ها و از طرفی وقتی در چند بار ترین شبکه برای جستجوی بهترین وزنها را ذخیره میکنم در اجرای بعدی و با مقادیر seed متفاوت چنانچه از وزن pretrain استفاده کنم بهترین را سریع پیدا میکند ولی در حالت from scratch نیاز به آموزش زیاد شبکه دارم و حتی در برخی موارد نمی تواند بهترین را بیابد گفتم شاید train با داده های با تنوع بیشتر بتواند پاسخ مطلوب حاصل شود
-> بطور قطعی میتوان گفت تنوع پذیری یکی از ارکان مهم برای رسیدن به دقت های بالاست ولی همونطور که خودتون گفتین انتخاب مدل تنظیم هایپرپارامترها و مسائل دیگه هم نقش اساسی در ترینینگ مدل ها دارن درواقع هدف از transfer learning همین است که با حجم داده محدود بتواند بیشترین بازدهی رو از مدل بگیرد"
"-> بخش ocr رو با یکی دو تا روش انجام میده با cnn و یک دیتاست اعداد و حروف فارسی هم انجام میده
-> بله دیدم این ویدیو رو ولی به مشکل خوردم
-> با cnn رفتین چون اون موقع که من میدیدم این ویدئو رو این ایده به ذهنم رسید که شاید rnn دو بعدی بهتر باشه واسه ocr توضیح اینکه چرا این ایده رسید به ذهنم تو چت یکم سخته باید با رسم شکل توضیح بدم ولی فکر میکنم اگه مدلی داشته باشیم که ارتباط بخش های مختلف یک عکس رو هم بتونه یاد بگیره تشخیص بهتری میده"
"-> سلام وقت بخیر استاد برای تشخیص حروف و اعداد فارسی در پلاک ماشین چه راهی پیشنهاد میکنید غیر از استفاده از پکیج هایی مثل easyocr
-> سلام بستگی به کاربردتون داره همچنین چرا easyocr نه
-> 
-> بخاطر اینکه easyocr تو تشخیص حروف و اعداد فارسی دقت خوبی نداره با استفاده از yolov8 پلاک ماشین رو تشخیص دادم الان میخوام کاراکترها رو خروجی بگیرم
-> سلام از tesseract استفاده کنین همچنین میتونین با استفاده از روش haar مدل رو روی دیتاست پلاک فارسی آموزش بدین و بعد ازش استفاده کنین
-> خیلی ممنون"
"-> سلام چطورید امیدوارم خوب باشید میخوام یک کتاب بهتون معرفی کنم هنوز نسخه نهایی کتاب منتشر نشده ولی درفت در دسترس هست که کیفیت خوبی هم داره کتاب درمورد دیپ لرنینگ هست از جهات مختلفی برای من کتاب خاصی محسوب میشه پر از تصویرسازی هست که هم فوق العاده هست و هم اینکه تا این اندازه رو در کمتر کتابی دیدم سعی کرده مقدمات رو هم بگه مثلا در فصل تابع اتلاف همون ابتدا مطالب خوبی درباره Maximum Likelihood میگه و بعد کم کم میره سر اصل مطلب نگاهی بهش بندازید شاید براتون مفید باشه خصوصا دوستانی که دوره دیپ لرنینگ 2022 هوسم رو دیدن
-> این کتاب بود اسمش رو اشتباه گفتم"
"-> سلام من برای یک پروژه سری زمانی ۲ تا فیچر دارم f که مقادیر یکی از فیچرها باینری هست و مقادیر یکی دیگه از فیچرها گسسته هست و شامل ۱۵ مقدار یونیک میشه آیا این درسته که من برای این دیتاست که شامل تقریبا ۵ هزار دیتا هستش ۲۸۰۵۰۰۰ از شبکه LSTM استفاده کنم من از شبکه LSTM استفاده کردم اما روی دقت های پایین ولیدیشنتقریبا 55 درصد مدل اورفیت شد تصورم اینه که دیتاست اطلاعات غنی نداره آیا می تونم قبل از LSTM از لایه های کانولوشن یک بعدی استفاده کنم تا فیچرهای بیشتری تولید کرده و بعد به LSTM بدم با توجه به اینکه مقادیر فیچرها باینری و گسسته ست این سوال رو می پرسم عذر میخوام من باید خودم این رو تست کنم اما چون هزینه محاسبات داره خواستم ابتدا بپرسم تا بدونم کارم از لحاظ تئوری ایراد و مشکل داره یا خیر
-> "
"-> سلام خوبی استاد استاد کتاب های خوبی که در زمینه دیپ لرنینگ و ماشین لرنینگ خودتون خوندین رو میشه به منم توصیه کنید مرسی ازتون
-> 
-> خیلی محبت کردید استاد لطف کردید"
"-> سلام چند روز پیش تصمیم گرفتیم که از سرویسهای داخلی اجاره GPU استفاده کنیم میخواستیم یک مدل Speech Recognition رو برای مدتی طولانی آموزش بدیم یک سرویس داخلی انتخاب کردیم و رفتیم برای راهاندازی پروژه و آموزش مدل این پروسه با اتفاقهای متنوعی همراه بود که در ادامه به مهمترینهاش اشاره میکنیم طولانی هست اما پیشنهاد میکنیم این گزارش رو مطالعه کنید ثبتنام در سایت ساده بود و هزینه اجاره هم بهصورت ساعتی حساب میشد ما یک سرویس بر پایه 3090 با ساعتی 20 هزار تومان انتخاب کردیم برای استفاده از سیستم باید یک ماشین مجازی ساخته میشد پروسه ساخت این ماشین مجازی کمی زمانبر هست و به گفته خودشون حدود 20 دقیقهای طول میکشه اما در عمل به نظر بیشتر از 20 دقیقه طول کشید بعد از ساخت ماشین باید استارت بزنید تا ماشین روشن بشه و بتونید کدتون رو اجرا کنید هرچی استارت زدیم روشن نشد از پشتیبانی پرسیدیم گفت ماشین خرابه و من یک ماشین دیگه کانفیگ کردم که ساخته بشه بازهم 20 30 دقیقهای منتظر موندیم که ماشین جدید رو تحویل بدن ماشین جدید روشن شد و خوشبختانه محیط ژوپیترلب بالا اومد کدهای آماده رو بدون مشکل آپلود کردیم طبیعتا باید دیتاست رو دانلود کنیم که بتونیم آموزش مدل رو شروع کنیم یک دیتاست 25 گیگ داشتیم که گذاشتیم دانلود بشه و دانلود این دیتاست 3 ساعت و 55 دقیقه طول کشید دیتاستی که لینک مستقیم داشت و اتفاقا با اینترنتی مثل همراه اول با سرعت بیش از دو مگ در کمتر از 30 دقیقه دانلود میشد فکر کنید چهار ساعت از شارژ شما برای دانلود یک دیتاست 25 گیگ میره گاهی سرعت اینترنت این سرویس به حدود 40 کیلوبایت میرسید مقایسه کنید با کولب که چنین دیتاستی رو زیر 1 دقیقه دانلود میکنه کد آماده دیتاست آماده و حالا بعد از چندساعتی درگیری میتونستیم آموزش مدل رو شروع کنیم بخش ایمپورت فریمورکها رو که اجرا کردیم کتابخونه تورچتکست رو نشناخت و داستان جدیدی شروع شد چک کردیم دیدیم که پایتورچ تورچویژن و تورچآئودیو نصب هست اما تورچتکست نصب نیست خب شاید بگید که اشکالی نداره راحت میشه نصب کرد اما نکته اینجاست که نسخه نصبی پایتورچ آخرین نسخه نبود و این دردسر نسبتا بزرگی بود دو راه داشتیم 1 نسخه مناسب تورچتکست برای پایتورچ نصبی پیدا کنیم یعنی باید سرچ میزدیم و تو آرشیوها دنبالش میگشتیم 2 از اول پایتورچ رو به آخرین نسخه آپدیت میکردیم و در کنارش تورچتکست هم نصب میکردیم این دومی دانلود زیادی نیاز داشت و ما هم میترسیدیم که بازهم ساعتها منتظر بمونیم به همین خاطر رفتیم سراغ مورد 1 احتمالا این کار برای یک دانشجو یا تازهکار کمی مشکل هست مقایسه کنید با کولب که هم فریمورکها رو زود به زود آپدیت میکنه و هم مدام پرکاربردها رو اضافه میکنه مثلا اخیرا هاگینگ فیس رو اضافه کردن دیگه همه دردسرها تموم شده بود و تونستیم آموزش مدل رو پیش ببریم حین آموزش مدل هم مشکل خاصی پیش نیومد یک مشکل دیگه شارژ مصرفی و گزارش شارژ بود شارژ باقیمونده رو باید از داشبورد میدیدیم هر بار هم که رفرش میکردیم زمان نسبتا زیادی طول میکشید که اطلاعات رو نشون بده همچنین میزان مصرف شارژ لحظهای آپدیت نمیشد بلکه بعد از یکی دو ساعتی آپدیت میشد یک بار انقدر آپدیت نشد که از 32 هزار تومان به 7 هزار تومان رسید بعدا دیدیم که ایمیل هم میزنن که شارژ شما منفی شده به نظرم این قسمت هم خوب نبود توی کولب در حد دهم واحدهای محاسباتی تغییر میکنه و این حس به شما دست میده که کاملا روی شارژت کنترل و مدیریت داری آخرین مورد هم اینکه اگه کدت رو ببندی و آموزش و استفاده از سیستم رو قطع کنی همچنان ساعتی مثلا 20 هزار تومان از شارژ کم میشه اما اگه از طریق داشبورد بعد اتمام کار ماشین رو خاموش کنی مصرف شارژ تا 90 درصد کم میشه یعنی اگه ساعتی 20 هزار تومان بدی در صورت خاموش بودن ماشین ساعتی 2 هزار تومان پرداخت میکنی یعنی فرضا اگه یک روز با سیستم کاری نداشته باشی و خاموش باشه باید حدودا 50 هزار تومان پرداخت کنی درصورتی که ماشین رو حذف کنی دیگه پرونده پروژه کد و دیتاست بسته میشه و شارژی هم پرداخت نمیکنی این مساله امکان استفاده مداوم رو مشکل میکنه خب نتیجهگیری اینکه با این شرایط بهصرفه نیست همچنین سرویس و خدمات حرفهای نیست یعنی با اینکه پشتیبانی چت آنلاین داره و زود به زود هم جواب میدن و جیپییوهای خوبی هم دارن اما بازهم چون خدمات ایرادهای ذکرشده در بالا رو داره نمیصرفه ترجیح میدیم از کولب استفاده کنیم و واقعا کولب بسیار عالی هست اسم این سرویس رو عمدا نگفتیم که داستان نشه امیدواریم که خود ارائهدهندههای سرویس ببینن اینو و بخونن و ایرادها رو اصلاح کنن
-> سلام ممنون از به اشتراک گذاری تجربیاتتون خیلی جالبه که یه شرکت به پایه های اصولی و درست ارائه خدمات به مشتری آشنا نیست و اصلا به کیفیت خدمات و تجربه مشتری هیچ اهمیتی نمیدن واقعا جای تأسف داره
-> سلام کولب پرو برای A100 زده 100 کامپیوت یونیت میده شما میدانید معادل چند ساعت استفاده میشه چون توی اینترنت نوشته هر یک ساعت برابر 13 کامپیوت یونیت میشه که میشه حدود 8 ساعت درسته این مقدار
-> سلام خواهش میکنم من برداشتم این بود که دید حرفهای به سرویسی که ارائه میدن ندارن یعنی تلاش کردن که شرایطی مناسب رو فراهم کنن ولی مدیریت و تیم قوی به نظرم نداشت مثلا اون نصب پایتورچ به سادهترین شکل ممکن انجام شده بود درحالیکه باید خیلی بیش از اینها روی فریمورکهای لازم کار کنن
-> سلام من از a100 استفاده نکردم از V100 استفاده کردم V100 حدود 55 بود همین V100 خیلی خوبه کولب پرو پلاس واحد محاسباتی بیشتری میده و اون هم عالیه کلا به نظرم برای خیلی از پروژهها شما به یک جیپییوی نرمال نیاز دارید که به مدت طولانی در دسترس باشه نه خیلی قدرتمند که محدود در دسترس باشه
-> سلام دقیقا درسته متاسفانه سرورهای پردازشی داخلی اصولی برای هوش مصنوعی نداریم یک مدتی بر اصول راه اندازی و نگهداری و خدمات این مدل سرورها تحقیق کردم پروسه سنگین و پیچیده ای داره اگه واقعا می خواستن سرور پردازشی برای هوش مصنوعی تحویل بدن باید در نظر میگرفتن که ارائه سرویس به کلاینت اصول و ماجرای خودش رو داره یا باید با سیستم مدیریت زمان بندی SLURM پیش برن یا مدیریت کانتینر KUBERNETES همراه با پلاگین NVIDIA اینساختار اصولی و تعریف شده هست
-> ممنون بابت توضیحات ارزشمند حرفهای بودن یا اصلا استاندارد بودن یک سرویس خیلی مهمه صرفا جی پی یو داشتن کافی نیست
-> حتما این رو تو گوگل بررسی کنید RunAI سرویس دهنده خدمات پردازشی در زمینه هوش مصنوعی هستش مقالاتش رو داخل سایتش بخونید متوجه اصول قضیه میشید
-> ممنون استاد خیلی مفید و جالب بود"
"-> سلام وقت همگی بخیر من سوالم در مورد svm هست کسی هست که سوالم رو براش بفرستم و راهنماییم کنه
-> سلام به نظرم این نوع سوال پرسیدن به جواب نمیرسه dontasktoaskir
-> درسته دو مدل svm دارم یک مدلش رو تایع costاش رو تغییر دادم و مدل دیگه svm معمولیه برای svmمعمولی خروجی نمودار accرو براساس بهترین پارامترهایcgamma رسم میکنه و یک rolling windowکه بهترینacc رو نشون میده برای svm ای که تابع cost رو تغییر دادمهمون hinglossولی با یک ضریبی خروجی نمودارscoreمقدارcost رو بر اساس cgamma رسم میکنه که مقدار منفی داره و نمودار دوم هم که مربوط به rollingwindowهست همون مقدار svmمعمولی رو میده میخوام که یه معیار مشترک باشه که بتونم دو مدل رو مقایسه کنمpercisionو recallوaccهمشون یک مقدار مساوی میده"
"-> سلام استاد وقت بخیر بنا به دلایلی نیاز دارم سگمنتیشن دیپ کاتالیستو ببینم خواستم ببینم بعد از دیدن cnn دوره ی یادگیری عمیق میشه پروژه سگمنتیشنو دید یا پیش نیازی داره
-> سلام بله میشه
-> سلام استاد برای شبکه های ترانسفورمر به جز دوره خودتون دوره خارجی هست معرفی کنید من در حال حاضر نمیتونم دوره دیپ لرنینگ ۲۰۲۲ رو خریداری کنم ممنونم
-> سلام دوره خاصی نمیشناسم من بیشتر کتاب میخونم کتاب d2l مثلا"
"-> سلام وقت بخیر استاد تو جلسات بخش آندرفیت و ٱورفیت درباره ظرفیت مدل هم صحبت کردین ظرفیت یک مدل رو چجوری میتونیم بدست بیاریم با چه دستوری
-> ميتوني پارامتر بزاري تو حلقه ببيني چقد درمياد
-> یکم بیشتر توضیح میدین دقیقا چیکار کنم"
"-> سلام استاد وقتتون بخیر یک مینی دوره برای کار با گیت هاب و قابلیت هاش نمیذارید
-> سلام ایشالا توی دوره پایتون میخوایم اضافه کنیم هنوز شروع نکردیم ولی تو برنامه هست"
"-> فرمت ستون تاریخ تغییر نکرده اگه مطمئنید که ضروریه تاریخ رو هم نرمالایز کنید متد extract پانداز رو اعمال کنید تا اسلش یا رو از ستون تاریخ پاک کنه
-> ممنونم از توجهتون بله اینجوری مشکل حل میشه اما نمیدونم اینکار درسته یا نه"
"-> سلام من به حوزه nlp علاقه مندم میخواستم ببینم چه دورههایی الان یا در آینده بایستی تهیه کنم دیدم سوالات گروه اکثرا در حوزه تصویر هست
-> 
-> چقد خوب و کامل توضیح دادین واقعا ممنونم
-> سلام استاد وقت بخیر برای پایان نامه امکان مشاوره هست در هوسم
-> سلام وقت بخیر درمورد retain grad میتونید توضیح بدیداین چیه اصلا
-> سلام میتونید به هوسم ایمیل بزنید و مشاوره ساعتی بگیرید موضوع مشاوره رو در ایمیل بگید howsammailcom"
"-> استاد اشرفی سلام وقتتون بخیر استاد تو دوره یادگیری عمیق بخش پردازش صوت ویدیو 288 که اولش گفتید یکی پرسیده بود اگه یادگیری ماشین خوبه پس چرا کاری روش انجام نمیشه و اون مدرس فقط درس میده شما گفتید که این تفکر ایراد داره و گفتید که تو گروه بپرسیم که شما نظرتون رو بگید میشه نظرتون رو بدونم فکر میکنم جوابش جالب باشه البته اگه قبلا جواب دادید بگید که من اون کلید واژه رو سرچ کنم تو گروه تا پیدا کنم چون خودم نمیدونستم چی باید سرچ کنم یا اگه باید تو گروه دیگهای سوال رو مطرح کنم بگید که اونجا بپرسم ممنون
-> 
-> استاد اشرفی توضیحات واقعا عالی و منطقی بود و من به شدت موافقم ممنون از وقتی که گذاشتید"
"-> سلام استاد آیا در آموزش هاتون شبکه های transformer هم توضیح دادین و اگر بله در کدام دوره هستش ممنونم
-> سلام یادگیری عمیق 2022
-> ممنونم استاد"
"-> سلام دوستان این خطای faster RCnn چرا برطرف نمیشه
-> 
-> سپاس از پاسخگویی و راهنمایی تون
-> ببخشید من باز یه سوال دارم من چندتا تصویر مختلف امتحان کردم فقط آنهایی که آبجکت شناخته شده مثل انسان یا ماشین داشتند باکس کشید اما تصاویری مثل تصویر بالا با اینکه گویا مختصات باکس پیدا میکنه اما مستطیلی در تصویر نمیکشه به خاطر اعداد score هست
-> 
-> 
-> ممکنه چون score بالای 05 نداره کد رو نگاه کنید باید توی مد مشخص باشه
-> سپاس"
"-> استاد سلام برشما در خصوص Kaggle منطقی است جهت پیش پردازش train و test دریافتی را concat کنیم تقلب نیست ظاهرا رتبه بالاتری در رقابت میدهد
-> سلام تستی که در اختیار گذاشتن ولی تارگت نداره
-> بله سمپل های test بدون تارگته
-> آره میتونید این کار رو انجام بدید فکر خوبیه"
"-> سلام وقتتون بخیر سایتتون رو معرفی میکنید و اینکه چه دوره هایی دارید
-> سلام عزیز چه جالب پس توی گروه دانشآموختگان هوسم کسایی هستن که نه دورهای از هوسم دارن و نه هوسم رو میشناسن سایت ما howsamorg آموزشهای حوزه هوش مصنوعی ارائه میشه پایتون یادگیری ماشین یادگیری عمیق و
-> سلام ممنون از شما دوره ای به نام NLP یا LLM ندارید یا دوره ای که اینا توش باشه مباحث مربوط به پردازش زبان طبیعی
-> سال دیگه قراره دورهی NLP بصورت تخصصی برگزار بشه ولی تو دورهی دیپ لرنینگ یه فصل NLP گفته شده"
"-> البته باید فایلت رو گوگل درایو البته داشته باشی
-> همین دیگه دیتاست توی گوگل درایو نیست توی درایو شخصیه آپلودش خیلی طول میکشه
-> حجمش چقدره چه دیتایی هست
-> دیتاست تصاویر ۲۵۶در۲۵۶ از حیوانات هست و حجمش تقریبا ۴۰۰ مگابایته آپلودش نزدیک به ۲ ساعت زمان می بره
-> 400 مگابایت زیاد نیست با اینترنت موبایل تست کنید شاید راحتتر آپلود بشه اگرنه میتونید کل فایل رو یکجا آپلود نکنید یک پوشه توی گوگل درایو بسازید و بعد تصاویر رو آپلود کنید نه فایل زیپ رو
-> باشه ممنونم"
"-> سلام استاد من دیتاست رو با درگ اند دراپ که وارد گوگل درایو میکنم خیلی زمان بره راهی هست که بشه دیتاست رو توی کولب مستقیما از درایو شخصی خوند
-> اره باید گوگل درایو تو mount کنی به کولب"
"-> سلام وقت شما بخیر آقای دکتر در نرم افزار neuro solution یک قابلیت تحت عنوان تحلیل حساسیت های متغیرهای ورودی در شاخص خروجی دارد sensitivity analysis کدنویسی با پایتون این مرحله در فصل های آموزشی دوره ها هست
-> سلام کارش چی هست این نرم افزار
-> سلام نمیدونم با این نرمافزار آشنایی ندارم
-> فکر کنم بشه با محاسبه گرادینهای ورودی تاثیراتشو نسبت بهخروجی به دست اورد ولی باید custom function نوشت براش البته سرچ کردم میگه کتابخانه ای مثل captum یا data explain هم هستن توی پایترچ که باید بخونیدشون که حالا مدلهای محاسبه متفاوت دارن اینا گرادیان بیس هستن و از روش Salency Maps استفاده میکنند"
"-> سلام استاد وققون بخیر روز دانشجو را هم تبریک میگم به همه استاد یه سوال داشتم الان زبان go داره خیلی بهش توجه میشه و سرعت بالایی هم داره مخصوصا توی api ها به شدت کاربردی هست و شرکت ها خصوصا تو هلند دارن سمت گو میرن میخواستم نظر خودتون را راجع به این زبان بدونم و اینکه تو هوش مصنوعی و یادگیری ماشین چقدر میتونه سودمند باشه و فایده داره وقت بزاریم سمت یادگیری اون یا نه
-> سلام برنامتون برای آینده چی هست میخواید توی چه شاخهای کار کنید چقدر بهش نزدیک هستید کجای لرنینگ کرو برای رسیدن به اهدافتون هستید اینا رو بگید تا جواب سوال شما رو بدم
-> سلام بنده دانشجو کارشناسی دانشگاه کاشان هستم سال آخرم هست و امسال کنکور دارم به شدت به هوش علاقه مند هستم و حوزه هایی علاقه دارم ماشین لرنینگ میدونم بیس هست و دارم یاد میگیرم ولی خیلی دیپ میخوام باشم طوری که به عنوان معرفی خودم ازش به عنوانmachine learning engenear یاد کنم یاد گیری تقویتی و فیس دیتکشن اگه اطلاعات دیگه ای هم نیاز بود بفرمایید تا خدمتتون عرض کنم
-> فیس دیتکشن غیرمنتظره بود امیدوارم در کنکور و بعدش موفق باشید توی این مرحلهای که شما هستید به مسائلی مثل go فکر نکنید شما فقط تلاش کنید که در همون مباحث ضروری متخصص بشید بعدش به صورت ارگانیگ خودتون میفهمید که کی باید برید سراغ go یا مباحث دیگه انرژیتون رو برای این مسائل نذارید فقط به متخصص شدن در ضروریها فکر کنید
-> میدونستم یکم جا میخورید یکم علایقم پراکندهدهست و همه چیز را دوست دارم ممنون از راهنماییتون
-> یکی از اساتید ما هم در مورد صحبت کردایشون تو زمینه شبکه عصبی و بینایی کار می کنندو گفتند پیگیر هستند که یاد بگیرند
-> شاید بدرد ایشون که استاد هستن بخوره اول باید روی اصلیها متخصص شد بعد فرعیها رو هم کم کم انتخابی یاد میگیریم"
"-> 
-> کتابخانه جدید پایترچ جهت یادگیری عمیق برای رباتها"
"-> ممنونم استاد ویدیو فوق العاده بود اگر المان جمع لاس هارو نداشته باشیم چی استاد فرض کنیم قراره عملکرد کاهش لاس رو بصورت جدا برای دوتا لاس فانکشن کاملا جدا حساب کنیم اونم داخل یک لوپ ترین انگار یکی از لاس ها mse باشه و یکی دیگه mae و اصلا هم قرار نیس ترکیب خطیشونو داشته باشیم از طرفی انتظار داریم یبار مدل با گرادیان mse اپدیت بشه و یبار با گرادیان mae چون معیارهای ارزیابی هم قراره توی فاز اینفرنس بصورت جدا مقایسه کنیم
-> نمیدونم احساس میکنم منطقی پشت این کار نیست
-> دقیقا همینطوره استاد خودمم فکر میکنم توجیهی نداره و باید مث آدم های سالم دوبار جدا ترین بکنمشون
-> منم دنبال خودت کشوندی"
"-> دوستان من کدی رو از گیت هاب در پایچارم می خوام اجرا بگیرم برای نصب reqirementtxt این خطا هست کسی میدونه مشکل چی هست
-> pip install r pathtorequirementstxt f filepathtoarchive ادرس اينجوري پر كنيد ببينيد درست ميشه يك نكته اي هست ادرس رو تغيير بده حتما
-> سلام استاد میشه راهنمایی داشته باشید"
"-> سلام وقت بخیر من برای تسک سگمنتیشن یه لوپ ترین نوشتم که داخلش دوتا لاس رو به شکل جداگونه تعریف کردم یکی ترکیب لاس دایس و کراس انتروپیه و دومی لاس باندری هرکدوم رو هم به شکل جداگونه بکوارد کردم برام سوال پیش اومده مدل من از کدوم یکی از این دو لاس فانکشن استفاده میکنه برای محاسبه اپدیت وزن ها معیارهای ارزیابیم توی بخش ولیدیشن امتیاز دایس و فاصله هاسدورفه هدفم اول استفاده از لاس باندری بود بخاطر عملکرد بهتر روی دقت مدل ها که از مقالات پایه استفاده کردم و هدف دومم مقایسه ترند کم شدن لاس بین دوتا لاسی که تعریف کردم ممنون میشم راهنمایی بکنین
-> هر لاسی که بکوارد بشه توی آپدیت پارامترها موثر هستن
-> اگر بخوایم اپتیمایزر از هر کدوم از لاس ها جدا گرادیان بگیره باید بعد از هر بکوارد جدا که برای لاس ها میزاریم یکبار step بزاریم چون فکر میکنم اگر دوبار بکوارد بگیریم و در ادامه استپ رو قرار بدیم انگار کار درستی نباشه
-> نه دیگه با هر بار بکوارد گرادیان همه پارامترها حساب میشه گرادیانها تجمیع میشه و یکبار استپ کافیه یک ویدئو میفرستم شاید مطلب رو روشن کنه"
"-> سلام وقت بخیر برای اجرای کد گیت هابی در پایچارم اجرای کدهای requeirementtext در ترمینال چگونه است ممنون میشم راهنمایی داشته باشید
-> pip3 install r requirementtxt"
"-> دوستان به نظرتون ایراد این کد چیه
-> Def رو ب اینیت چسبوندین"
"-> سلام دوستان من دو تا فولدر دارم که تو اولی تصاویر هست و در دومی ماسک مربوط به تصاویر هست و میخوام دیتاست بسازم برای تسک سگمنتیشن آیا کسی هست بتونه راهنمایی کنه یا شبیه این کار رو انجام داده باشه ممنون
-> ابراهیم یک نگاهی به پروژه سگمنت تصاویر پزشکی توی دیپ کاتالیست بنداز در کل باید یک کاستوم دیتاست بنویسی که توی متد اینیت آدرسهای تصاویر و ماسکها رو آماده کنی و توی گت آیتم هم باید اونها رو با پیل یا اوپن سی وی لود کنی آگمنت هم بخوای بزنی باید روی هر دو به شکل مشابه بزنی"
"-> برای پروژه دانشگاه تک به تک میخوام رو دیتا ست اجرا کنماگه فرمولی جمع و جور هم جایی هست بگید
-> من کتابخونهای نمیشناسم ولی پیادهسازیهای اینها رو توی دوره یادگیری ماشین یا خودمون انجام دادیم یا بچهها پیادهسازی کردن"
"-> سلام با استفاده از یادگیری تقویتی میشه کارهایی مثل بازی کردن نصب نرم افزار و با هوش مصنوعی به صورت خودکار انجام داد بازی هایی مصل کالاف دیوتی یا فوتبال
-> سلام با یادگیری تقویتی میشه روی بازیها کار کرد ولی کلا کار بسیار مشکلی هست ضمن اینکه هر بازیای رو نمیشه یکسری بازیها مثل همونهایی که اسم بردید سورسشون در دسترس نیست که بشه روشون کار کرد اما بازیهای سبکتر و اوپن سورس هست که میشه روشون کار کرد
-> نمیشه با استفاده از بینایی ماشین و کنترل کیبورد و موس بازی ها رو خودکار کرد
-> میشه ولی خب کار مشکلی هست دیگه بالاخره یک بخشی از کار هم سختافزاری میشه درموردش باید تحقیق کنید چون این کار در تخصص من نیست"
"-> سلام وقت همگی بخیر دوستان دوره ماشین لرنینگ با پایتون اموزش داده میشه یا متلب
-> و اینکه ریاضیاتش برای دوستانی که دانش ریاضی زیادی نداشتن کامل بوده از نظرتون من در بخش ریاضیش مشکل دارم یک دوره دیگه گرفتم و با اینکه خوب بود متوجه دلیل استفاده خیلی از فرمول ها نمیشم پیشاپیش ممنون بابت پاسختون
-> سلام چه دوره ای گرفتید
-> درود پایتون کاملترین دورهی ماشین لرنینگ هست و ریاضیاتش برای شروع کامله
-> خیلی عالی ممنونم
-> برای یک اکادمی پزشکی و هوش مصنوعی بود دوره خوبی بود ولی هم با متلب بود هم ریاضیاتش خیلی کوتاه بود
-> سلام دوره یادگیری ماشین با پایتون هست دیگه متلب در حوزه هوش مصنوعی فراموش شده هست دوره حدود 25 ریاضی شامل جبر خطی حساب و آمارواحتمال داره ما در حد نیاز این دوره یادگیری ماشین گفتیم پیشنیاز این هست که ریاضی رو در حد دبیرستان بدونید
-> ممنون لطف میکنید سرفصل کامل رو ارسال کنید من پیدا نکردم در سایت
-> و اینکه برای پایتون نیازی به دونستن لایبرری خاصی هست اشنایی اولیه با یکسری لابرری ها دارم ولی درکل میپرسم نیازی به پیش نیاز خاصی داره
-> 
-> 
-> ممنونم فهرست مطالب رو ندیدم
-> متشکرم از توضیح عالی و کاملتون فقط دو سوال داشتم یکی اینکه انواع classifier ها هم اموزش داده میشه داخل توضیحات چیزی ندیدم و اینکه مبحث کاهش بعد چه زمانی منتشر میشه
-> احتمالا اسم مدلهای کلاسیفایر رو نمیدونید چون از همون ابتدای صفحه تا بخش فهرست مطالب عناوینشون هست توی همون ویدئویی که فرستادم هم اسمهاشون هست رگرسیون لجستیک سافتمکس شبکه عصبی mlp ماشین بردار پشتیبان svm k نزدیکترین همسایگی knn درخت تصمیم و رهیافتهای گروهی بوستینگ استکینگ و درمورد کاهش بعد هم توی ویدئوی توضیح دادم بخشی از کاهش بعد ضبط شده و احتمالا این هفته یا هفته بعد منتشر میشه"
"-> سلام استاد وقت بخیر در پروژه image captioning در دوره دیپکاتالیست آیا دقت خروجی های تولید شده قابل قبول بوده یعنی مرتبط با تصویر تولید شده و خوانا بوده است در حدی که بتوان در یک نرم افزار استفاده کرد متن تولیدی انگلیسی بوده یا فارسی برای فارسی نیاز به تغییرات زیادی هست سپاسگزارم
-> سلام انگلیسی بله خوب بود"
"-> سلاماستاد در مورد اینکه تعداد محاسباتFLOPs رو بدست بیاریم و با کارای بقیه مقایسه کنیم راهنمایی میکنین
-> سلام یکسری کد توی گیتهاب هست که با اون میتونید فلاپس متدتون رو حساب کنید لینکی ندارم سرچ کنید"
"-> سلام وقت بخیر درسایت یودمی امکان دسترسی رایگان به دوره ها چطور هست همچنین در سایت کورسرا دسترسی رایگان به دوره ها چطور هست
-> سلام وقت بخیر توی کورسرا وقتی دوره رومیزنین پایین کوچیک میاد بعنوان آدیتور یا کلمه آدیت دقیقا یادم نیست رایگان میتونین ویدیو هارو ببینید برای تمرین هاش هم گاها هستن کسایی که توی گیت هاب بزارن تمرین هارو
-> ممنون
-> سلام اکثر دوره هایudemy در gitir به رایگان پیدا میشن
-> audit
-> ممنون
-> ممنون"
"-> سلام شبتون بخیر این تصویر با local periodic noise دچار اعوجاج شده با چه فیلتری میتونیم نویزشو کاهش بدیم
-> توی کتاب پردازش تصویر گونزالز یک مثالی از نویز زدایی تصاویر پریودیک هست فکر کنم با تبدیل فوریه این کار رو کرده شاید برای شروع اون مثال بد نباشه البته اینجا لوکال هست و احتمالا منجر به جواب خوب نشه
-> برای این تصویر چه روشی میتونه خوب باشه
-> هرچی به ذهنم رسید توی پیام قبلی گفتم"
"-> سلام استاد وقت بخیر آیا برای داده های پزشکی هم میشه data augmentation انجام داد از نظر علمی درست هست
-> مثلا توی دادههای تصویری برای سگمنت آگمنتیشن استفاده میشه فقط باید زوج ورودی و تارگت باهم مچ باشن در سگمنت باید هردو به یک شکل مشابه سگمنت بشه پروژه سوم دوره دیپ کاتالیست رو ببینید
-> خیلی ممنونم از پاسخ منظورم اینه که مثلا برای عکس های پزشکی مثل MRI آیا خروجی که بدست میاد قابل قبول هست چون ما داریم با تغییراتی که ایجاد می کنیم انگار یک تصویر جدید ایجاد می کنیم و اون لیبل اولیه مربوط به عکس MRI اوریجینال بود
-> پروژه سوم دیپ کاتالیست رو ببینید"
"-> سلام استاد وقتتون بخیر استاد این دوره دیپ چقدر طولانیه صحبتم بار منفی یا مثبت نداره صرفا طول دوره رو میگم من پارسال بهمن شروع کردم و هنوز نتونستم تمومش کنم
-> منم دقیقا مشکلم اینه اومدم جزوه بنویسم دیدم تموم نمیشه اگه ننویسم یادم نمیمونه اونجا هایی که جزوه ننوشتم زودتر جلو میرفتم و باعث شادمانی میشد از طرفی کدش رو هم باید زد ولی کلا دوره شکی نیست عالیه حالا خوب بود دوستانی که کامل تموم کردن دوره رو راهنمایی کنند که بهتره اصن جزوه بنویسیم ننویسم چطوری پیش ببریم که هم زود تموم شه هم یادبگیریم
-> اتفاقا منم برای ی سری قسمتا اون اوایل جزوه نوشتم فکر کنم دیدم نههههه اینطوری تا پایان دوره تحصیلیم باید نوت بردارم
-> حالا دوره یادگیری ماشین رو ندیدید 50 ساعت بیشتره
-> آره خوبه بچهها راهنمایی کنن کلا نت برداری کار زمانبری هست ولی ارزشش بعدا مشخص میشه چون هربار که نیاز داشته باشید در وقتتون صرفهجویی میشه پرینت گرفتن از اسلایدها و نوشتن یکسری نکتهها رو اونها میتونه خوب باشه
-> سلام استاد خسته نباشید وقتتون بخیر ببخشید مزاحمتون شدم استاد ببخشید برای جلوگیری از overfit میشه به لیبل ها نویز اضافه کرد سرکلاس امروز گفتند بهمون که اینکار باعث میشه گرادیان بیشتری تولید بشه
-> چه خوب میشد کسایی که جزوه نوشتند جزوه هاشونو به اشتراک بذارند
-> خب اینجوری خوب نیست چون کپی رایت حفظ نمیشه و دست به دست میشه
-> آره میشه
-> مممنون پس نطر خود شما اینه حتی اگه طول بکشه نت بنویسیم یا نهایت پرینت بگیریم ممنونم از راهنمایی تون انشاءالله با قدرت ادامه بدیم"
"-> سلام وقت بخیر در یک مسله تشخیص جنسیت با کمک تصاویر چهره از رندوم ریسایز کراپ استفاده کرده است در این حالت ممکن بصورت تصادفی بخشی از تصویر انتخاب شود ک حاوی الگو های مکرر برای دسته بندی نباشد بنظرتون استفاده از این فانکشن کار معقولیه چندین نوت بوک پر استناد در کگل از این روش استفاده کرده اند
-> بله آگمنت خوبه اون دستور هم خوبه اتفاقا ما هم میخواییم در ترین همیشه پرتکرارها نباشن و شبکه از سایر نواحی هم بتونه اطلاعات و الگو استخراج کنه
-> سلام استاد یه سوال داشتم شما چطوری اون برنامه translate رو نصب کردید روی هر کلمه ای که بگیرید تو ویندوزتون میتونید ترجمه کنید کلمه اش رو همونجا لازم نیست وارد سایت google translate شید
-> به نظرم خوب کار نمیکنه ولی توی گوگل بنویسید google translate chrome plugin"
"-> سلام وقت بخیر ببخشید یک سوال خارج از دوره ها داشتم برای همین اینجا فک کردم بتونم بپرسم اگر بخواهیم لپ تاپ تهیه کنیم که گرافیکشون 4GB Nvidia هست و به این صورت A2000T2000P2000 کدوم بهتره م مثلا p2000 بهتره یا t600 و میشه دیگه کلا روی کولب و کگل کد نزد آیا کگل یا کولب هم ۴ گیگ هستند
-> دوره دیپ لرنینگ یا دیپ کاتالیست رو ندیدید من در مورد جیپییوها صحبت کردم خیلی شده که من وضعیت جیپییو رو نشون دادم توی کولب 16 گیگ داریم کلا سختافزار به اهدافتون وابسته هست ولی به نظرم 4 گیگ کمه
-> دیپ کاتالیست رو شروع نکردم دیپ رو کامل ندیدم یه فصل دیدم پریدم به فصل دیگه احتمالا اینجا رو پریدم هرچند شروع کردم که از صفر ببینم و تموم کنم آهان پس باز هم مجبور به استفاده از کولب هستیم خیلی ممنونم نمی دونستم ۱۶ هست الان خود شما که هم در شرکت کار میکنید و هم دکتری رو پشت سر گداشتید با لپ تاپ شخصی هم کار میکردید یا بیشتر کولب بود با این حساب پس فرق نمی کنه که مثلا p2000 ۴ گیگ باشه یا a2000 هر کدوم ارزون تر باشه بهتره چون ۱۶ گیگ که نمیشه خرید گرونه و اصن نیست"
"-> سلام وقت بخیر درمورد پردازش صوت داخل دوره یادگیری عمیق روشهای preprocess سیگنال صوتی رو توضیح دادید و ممنونم درخصوص postprocess توضیح میخام یعنی اینکه اگر مدلی generative داشته باشیم و ورودی و خروجی سیگنال صوت هست و فعلا کاری به کاربرد نداریم سوال سیگنال تولید شده در حوزه فرکانس و فوریه هست مثلا این رو چطوری میشه برد به حوزه زمان و به صوت واقعی تبدیل کرد
-> همه تبدیلها اینورس هم دارن یعنی اگر fft داریم ifft هم داریم"
"-> استاد اینو روی دیتاستی به اندازه ۶۰۰۰ هزار تصویر برای ترین و ۳۰۰۰ تا برای ولدشن ترین گذاشتم استاد راهنمام میگه نمودار اشتباههواریانس ترین خیلی زیادهمیگه زمانی این جوری میشه که دیتای ولدشن حداقل ۲ برابر ترین باشهاز طرفی هم میگه نباید ولدیشن زیر ترین بیاد استاد نظر شما چیه
-> سلام به نظرم این رایج هست که در اوایل آموزش ولیدیشن پایینتر از ترین باشه انتظار میره رفته رفته خطای ترین کمتر از ولید بشه مثلا لرنینگ کروهای مقاله رزنت اینکه در پایان آموزش ولیدیشن کمتر از ترین باشه جای بحث داره نگفتم ایراد داره گفتم جای بحث داره نوسانی بودن ترین هم دلایل مختلفی میتونه داشته باشه از خطای کدنویسی گرفته تا لرنینگ ریت بزرگ یا عدم استفاده از لایه نرمالیزه در شبکه و البته ممکن هست مشکلی هم نداشته باشه مثلا ترین در لرنینگ کروهای رزنت نوسانی هست"
"-> سلام یه سوال داشتم برای داده ایی که مثل تصویر داشته باشیم و بخوایم سگمنت کنیم ولی ماسک نداشته باشیم فقط تصویر باشه مثل این دیتا ست هایی که همراه با ماسک هستند ماسک نداشته باشه ما چطوری میتونینم اون قسمت از تصویر رو ماسک کنیم تا بتونیم به مدل مون بدیم تا یاد بگیره یا میشه که اصلا یه مدل ران کنیم که خودش بتونه این ناحیه هایی که مد نظر ما هست رو سگمنت کنه ممنون میشم راهنمایی کنید
-> میتونید از شبکههای آماده مثل دیپ لب در تورچ ویژن یا یولو 8 استفاده کنید شاید دقت خوبی نداشته باشن که قاعدتا بسته به صورت مساله باید روش مناسبی برای بهبود عملکرد مدل پیدا کنید
-> خیلی ممنونم استاد"
"-> خیلی ممنون از پاسخ شما کاربر که نمیتونه چند دقیقه منتظر بمونه اینطوری که هزینه توسعه نرم افزارهای مبتنی بر هوش مصنوعی خیلی بالا هست الان سرورهای موجود با GPU به قیمت ما فضایی هستن قیمت هارو ببینید لطفا اگر پیشنهادی دارید بفرمایید
-> بله هزینشون بالاست اتفاقا اگر کسی بخواد پروژهای از جایی بگیره باید به مسائل سختافزاری خیلی دقت کنه ببینید برای همون سرویس آمازون هم باید حساب کتاب کنید که کدوم بهصرفهتر هست لزوما قرار نیست دنبال قویترین موردها بریم شاید با سختافزارهای ضعیفتر هم کار راه بیفته باید ببینید مساله چی هست و چه قیدهایی برای شما وجود داره
-> ممنون استاد که وقت گذاشتید"
"-> سلام استاد وقت بخیر من یک مدلی رو برای ويرايش تصاویر با شبکه های عصبی عمیق پیاده سازی کردم و خروجی خوبی میده این مدل بر اساس style gan هست حالا میخوام یک نرم افزار بر اساس مدل آموزش داده شده بسازم برام سواله که آیا سروری که میخواد چنین مدلی رو روی تصویر ورودی کاربر اجرا کنه باز هم نیاز به GPU داره میشه در این مورد توضیح بدید
-> سلام تاجایی که بلدم از gpu صرفا برای آموزش مدل استفاده میشه بنظرم راه حل خوب اینکه مدل رو فریز کنی و فقط روی سرور جدید تابع فوروارد رو صدا بزنی
-> بستگی به مدل و پروژه داره اگه کاربر صرفا یک ورودی میده و بعد هم میتونه چند دقیقه صبر کنه خب میتونید با سی پی یو هم جلو برید که شاید هزینه کمتر بشه اما اگه کار جدی باشه کاربر زیاد باشه شبکه بزرگ باشه باید به سمت دیوایس با جی پی یو برید
-> سلام نه اینطوری نیست از GPU برای ارزیابی هم استفاده میشه ما حتی بردهای کوچیک مثل جتسون انویدیا رو داریم که روشون جی پی یو دارن یا مثلا خودروهای خودران بدون جی پی یو قابل درک نیست یا خیلی از کارهای ریلتایم که باید در هر ثانیه چندین فریم پردازش بشن فریز کردن هم خیلی ارتباطی با این مساله نداره معمولا فریز رو در ترین میبینیم و با این تکنیک حجم پارامترها رو برای آموزش دیدن کم میکنیم
-> بله درسته زمانی که یک مدل روی داده های آموزشی و ارزیابی به خوبی عمل کرده در زمان دیپلوی مگر نباید از بهترین مدل در حین آموزش وزن های بهترین نتایج هر اپاک استفاده کنیم
-> بله دیگه باید چنین کاری انجام بدیم
-> جتسون tx2 حدودا ۸۰ میلیون
-> سلام مقیاس کاربردی شما چقد هستش فقط یک یوزر استفاده میکنه یا چند یوزر کلا برای پیاده سازی حالت inference مدل و ارائه سرویس بهترین راه حل استفاده از کلاستر و نرم افزارهای مدیریت کانتینر به همراه پلاگین NVIDIA هستش کلا اگر کانتینری جهت ارائه Inference به کاربر ها استفاده نکنید به مشکل مدیریت منابع می خورید اگر تک کاربر هستید کلا موارد بالا به کارتون نمیاد و کارتون راحت تر هستش
-> سلام خیلی ممنون قرار هست که کاربرهای زیادی ازش استفاده کنن"
"-> سلامدوستان کسی دیتاست foggycityscapes دانلود کردهمن میخوام این دیتاست دانلود کنم بیست گیگ حجمشههربار امتحان میکنم تا نصفه میره و کنسل میشه باید از اول بگیرمراهی برای دانلود این دیتاست های حجیم میشناسید
-> سلام با این دیتاست نه اما با دیتاست دیگهای تقریبا با همین حجم این مشکل رو داشتم لینکی که برای دانلود میداد یه لینک مدت دار بود که مثلا نهایتا ۴۵ دیقه معتبر بود آخرشب که سرعت نت بالاتر بود شعی کردم و دانلود ضد
-> دستت درد نکنه
-> سلام دو راه دارید ۱ یه vps ویندوزی بگیرید فایل رو باهاش دانلود کنید و بعد Web Server IIS رو روش فعال کنید و بعد از vps خودتون دانلود کنید ۲ راه آسون تر از سایت های لیچر و مستقیم کننده لینک ایرانی استفاده کنید
-> خیلی ممنونم از راهنماییتون"
"-> سلام وقت بخیر دوستان من یه شبکهای رو گذاشتم برای train داخل مدلم شبکههای lstm و conv دارم و loss functionم SISNR عه و optimizerم هم SGD تو epoch اول لاس شبکه خیلی خوب کاهش پیدا میکنه و از 23db به 2db میرسه اما در ادامه و در epochهای بعدی خیلی خیلی کندتر کاهش پیدا میکنه در حد چند هزارم کاهش پیدا میکنه مثلا از 2db به 1998db میرسه تو epoch بعدی مشکل کار کجاست
-> استاد نظری دارین شما
-> سلام و وقت بخير It sounds like you are experiencing a common phenomenon in deep learning called a loss plateau This is when the loss function stops decreasing or decreases very slowly during the training process There are several possible causes and solutions for this problem Some of them are The learning rate is too high or too low The learning rate is a hyperparameter that controls how much the model updates its weights in each iteration If the learning rate is too high the model may overshoot the optimal point and oscillate around it If the learning rate is too low the model may take too long to converge or get stuck in a suboptimal point A common technique to overcome this problem is to use a learning rate scheduler which adjusts the learning rate dynamically based on the progress of the training The model is overfitting or underfitting the data Overfitting means that the model learns the noise and specific patterns of the training data but fails to generalize to new and unseen data Underfitting means that the model is too simple and cannot capture the complexity and variability of the data Both cases can result in a high loss value and poor performance To prevent overfitting you can use regularization techniques such as dropout weight decay or data augmentation To prevent underfitting you can use a more complex model architecture add more layers or neurons or use a different activation function The loss function is not suitable for the task or the data The loss function is a mathematical expression that measures how well the model predicts the desired output Different tasks and data may require different loss functions For example for a classification task you may use crossentropy loss which penalizes the model for assigning low probabilities to the correct class For a regression task you may use mean squared error which penalizes the model for deviating from the true value Choosing an inappropriate loss function can lead to a high loss value and poor performance I hope this helps you understand the plateau problem and how to fix it
-> ممنونم به خاطر جواب دادنتون در رابطه با مورد اول lrهای مختلف رو چک کردم مشکل حل نشد مورد آخر هم نمیتونه باشه چون دارم مقالهای رو پیاده سازی میکنم خود اون مقاله از همین loss function استفاده کرده
-> استاد شما نظری ندارین
-> سلام نمیدونم اطلاعات کامل نیست مثلا میخوایید به چند برسه
-> میخوام به 20 برسه
-> تغییر لرنینگ بعد از چند ایپوک اول جواب نمیده لرنینگ ریت رو کوچکتر کنید
-> تو optimizerم lr رو روی 1e5 گذاشتم روی 1e3 میذارم و اعداد بالاتر اصلا جواب نمیده
-> چشم اینم تست میکنم"
"-> دوستان چطور میشه در محیط ژوپیتر لب یاداداشت متنی برای کد گذاشت جدا از هشتگ گذاشتن جلوی مطلب
-> فرمت سلول بهmarkdown تغییر بدید و میتونید از کد ها HTML5 داخلش برای اضافه کردن نوشته متن و استفاده کنید"
"-> سلام برای اینکه بفهمیم داخل یک شبکه وزنها آپدیت میشن چیکار باید بکنیم شبکمون با پایتورچ نوشته شده
-> "
"-> سلام استاد دیتاستی میشناسین که برای هر سه حوزه classification segmentation و image captioning باشه
-> شاید پاسکال وک یا کوکو اینطوری باشن
-> سلام استاد خسته نباشین من ن درسشو خوندم ن دانشگاه رفتم بعد یهویی علاقمند شدم ب این رشته الان دارم اموزشهای ماشین لرنینگو یاد میگرم هدف خاصی هم ندارم شاید بشه گفت در اینده ب درامدش فکر میکنم سوالم اینه بعد از دوره ماشین لرنینگ باید چیکار کرد باید چیز دیگه ای اموزش دید ممنون میشم ی توضیحی بدین منو از گمراهی در بیارین
-> 
-> سلام استاد خسته نباشید لطفن اگر شدش بعد از اینکه دورهی ماشین تموم شد مثل دیپ کاتالیست یه دورهی پروژه محور تو برنامتون برای ماشین بذارید که یجورایی هم ترسمون از پروژه بریزه هم اینکه خودمونو بتونیم امتحان کنیم خود دورهی دیپ کاتالیست من از دور به سرفصل هاش و جلورفتن کار نگاه ميندازم خیلی خوب بوده و سطح پروژهها هم عالی بودش و همین هدف انجام پروژه قشنگ توش نمایان هست
-> سلام محمد جان آره این برنامه رو داریم ممکن هست بلافاصله بعد یادگیری ماشین نباشه ولی تقریبا با همون فرمت مشابه با دیپ کاتالیست احتمالا دوره خواهیم داشت
-> سلام استاد و روز بخیر تخفیف black friday نداریم
-> سلام نه عزیزم"
"-> سلام استاد یک دیتاست تایم سری رو بررسی میکنم اینطوری مقادیر فیچرهاش گسسته ست توی این موارد هم میتونم از شبکه های بازگشتی استفاده کنم با توجه ب اینکه slicing window هم باید انجام بدم اگر تقریبا برای این دیتاست ۵۰۰ هزار تا سمپل داشته باشم تعداد سمپل هایی که هر اسلایس باید داشته باشه چقدر باشه بهتره
-> سلام همون بازگشتی خوبه درمورد سایزش هم خب هایپرپارامتره یک عدد بذارید و بعد با کراس ولیدیشن میتونید بهترین مقدار رو تعیین کنید"
"-> سلام وقت بخیر دوستان امکانش هست چند مقاله برای سال ۲۰۲۰ به بعد در زمینه پردازش تصویر که از روش های یادگیری استفاده نکرده باشه معرفی کنید
-> توی گوگل اسکالر سرچ کنید
-> امکانش بفرمایید چه عنوانی سرچ کنم"
"-> ممنون دوستان من مشتاقانه نظرات شما رو میخونم و مشتاق خوندن نظر سایر دوستان هم هستم پیام پایین هم نظر یکی از دوستان هست که در خصوصی گفتن سلام این یک پیشنهاد هستش برای دوره ای ک قراره ضبط شه من دوتا دوره از شما دیدم و شاید یکی از وجه تمایزات شما نسبت ب دوره های دیگر قسمت تئوری مسئله ها بوده خیلی از دوره ها این قسمت تئوری رو یجورایی بنظر کم اهمیت جلوه میدن و میرن سره کد زنی که البته این امر طبیعیه و چون مخاطب عامه شاید پر سود تر کنه کارو ولی خب تا اینجای کار مخصوصا دوره بینایی ماشین بنظرم یکی از وجه تمایزات همین قسنت تئوری محور و شاید از نظر خیلیا حوصله سر بر باشه من از اهمیت کد زنی اینا کم نمیکنم و قطعا اونا هم واجبه ولی لطفا اگر دوره ای میسازید توجه ویژه ب قسمت تئوریش کنید یا حتی جوری بنظرم میتونید ضبط کنید ک هر دو مجموعه از مخاطب مورد بررسی قرار گیرن ینی هم کسی ک میخاد بصورت جنرال یادبگیره هم کسی ک میخاد تخصصی تر و بهتر یاد بگیرهشاید مثلا اینطور ک یک قسمتایی بصورت پیشتهادی باشن تو دوره واقعیت اینه تو کارای پژوهشی اون قسمت تئوریه ماجرا خیلیی خیلی مهمه البته ک من مثل شما تجربه ندارم قطعا بازم میگم یچیز مثبتی ک دوره شما رو من بهش جذب شدم همین قسمت تئوری بود من اول دوره بینایی ماشین رو خریدم بعد دیدم چ خوب ک یه حجم زیادی از تئوری داره گفته میشه و اینا ترکیب میشه با کد زنی رفتم دوره شبکه های عمیق هم خریدم با اینکه مثلا چن تا دوره دیده بودم قبلش و صرفا گفتم ارزششو داره شاید چیز هایی باشه ک ندیدم و الان شاید ب فکر اینم حتی دوباره ماشین لرنینگ هم تهیه کنم و ببینم همچنین پیشنهاد میکنم دوره بینایی ماشین اپدیت تر هم بشه ک البته میدونم شاید در اولویت مجموعه نباشه یا حتی دوره جدیدی داده بشه با رویکرد های جدید تر ک از سال ۲۰۲۰ تا به امروز اومدن متاسفانه یا خوشبختانه پیشرفت تو این حوزه ها خیلی با سرعت داره اتفاق میوفته و اینکه دوره های به این سبک ک تخصصی ترن و حالت جنرال ندارن مثل یادگیری عمیق باید سریع تر اپدیت بشن البته ک من بازم میدونم ب هر حال کار شما یه کار تجاری اموزشی و بیزینش نیازمند افرادی هست که اون دوره رو تهیه کنن ولی خب این ک بنظرم الان اکادمی شما مقداری معروف شده شاید بخاطر همین وجه تمایزات بوده
-> سلام استاد وقت بخیر بنظر من خوبه که توی دوره به مباحثی مثل فوریه ویولت و کلا حوزه فرکانس بیشتر پرداخته بشه که کاربردی تر هستند خیلی قسمت ها مثل فیلترها و مباحث ساده ای هستند ولی کلا حوزه فرکانس و تبدیلات مختلف دوره رو کاربردی و سطح بالا و جذاب میکنه
-> سلام استاد جسارتا مطالب دوره بر اساس کتاب گونزالسه یه رفرنس دیگه ای هم استفاده میشه
-> رفرنس اصلی همون هست ولی رفرنسهای دیگه هم داریم
-> سلام آقای دکتر اگه لطف کنین ریاضیات پیش نیازش رو هم تو دوره بگین خیلی عالی میشه
-> سلام ریاضیات در حد دبیرستان مشتق ماتریس و
-> سلام راستش بیشتر منظورم بحث هایی مثل فوریه منظورم هست که تو کارهای کلاسیک کاربرد دارن
-> اونها رو به اون اندازهای که نیاز باشه میگیم ولی خب اگه دنبال دردسر میگردید از درسهایی مثل سیگنال و سیستم یا ریاضی مهندسی فوریه رو بخونید کلا برای اینجور مباحث باید آمادگی این رو داشته باشید که همون موقع که تدریس میشن درصورت نیاز بیشتر مطالعه کنید به نظرم پیش پیش نیاز نیست
-> تدریس شما یک چیز دیگه هست آقای دکتر"
"-> سلام به همه لطفا بیایید کمک همونطور که در جریانید یکی از مسائلی که این روزها در گروه مطرح میشه برگزاری دوره opencv هست میخوام قبل از اینکه اصلا دوره معرفی بشه نظر برداشت تصور و پیشنهادهای شما رو در مورد دوره opencv بدونم چه علاقهمند به شرکت در این دوره هستید چه نیستید لطفا نظر بدید قرار نیست حتما پیشنهادها رو در دوره اعمال کنیم اما طبیعتا شناخت بیشتر از تصورات شما در ارتقای کیفیت دوره موثر خواهد بود اولا لطفا در گروه بگید که بقیه هم بخونن و واکنش نشون بدن دوما اگه راحت نبودید در خصوصی در قالب یک پیام لطفا نظرتون رو بگید با احترام اشرفی
-> سلام استاد منکه قطعن شرکت میکنم بنظرم از دورهی پردازش تصویر قبلیه مفیدتره کاربرد opencv هم تو بعضی از زمینهها زیاده بنظر منم کلن دورههای شما دورههای فوق العادهای هست
-> سلام وقت بخیر اگر از پایه شروع و تا سطح حرفه ای پیش بره خیلی میتونه کمک کننده باشه
-> سلام ممنون لطفا بیشتر توضیح بدید منظورتون از سطح حرفهای چی هست میتونید از سطح حرفهای مثال بزنید
-> سلام خسته نباشيد استاد دوره ي opencv بخواي برگزار كنيد سوال من اينه فقط فريمورك opencv اموزش ميديد يا مثال پايتورچ ترين ميكني و ديتكت با opencv انجام ميديد اگه اينجوري باشه به نظرم عاليه
-> سلام استاد من قطعا شرکت میکنم باتوجه به سوابقی که از مجموعه شما دیدم بنظرم یکی از بهترین مسیرهایی که میتونه جدای از تناسب بخش کاربردی کار و تئوری کنارش که شما توی دورههاتون نشون دادین عالی از پسش برمیاین ترسیم بشه بحث جریان شناسی هم هست این که به این دید برسیم چرا یجاهایی میریم سراغ opencvو پردازش تصویر کلاسیک چرا گاهی میریم سراغ یادگیری عمیق چطور به اینجا رسیدیم چه اهمیتی داره بحث داشتن دانش کافی ازش و همچنین اگر مثل دوره بینایی کامپیوتر حرفهای یه پوشش خیلی خوبی از تسکهای main و downstream رو شامل بشه عالیه
-> منظورم اینه ما مثلا بتونیم یک مقاله کلاسیک رو از صفر پیاده سازی کنیم یا اینکه پروژه های ساخت مرسوم و به روز را به راحتی انجام بدیم مثلا من دوره پردازش تصویر قبلی رو دیدم خیلی خوب توضیح داده بودید و برای درس پردازش تصویر دانشگاه خیلی کمک کننده بود اما برای انجام کارهای فراتر از کلاس درس به نظرم نیازمند آموزش های بیشتر بودم
-> سلام استاد من با توجه به زمینه پژوهشی و آکادمیکم که بینایی رایانه هست قطعا استقبال می کنم چون opencv یکی از کتابخانه های بسیار مهم و کاربردی در این حوزه هست که امکانات و ابزار بسیار زیادی رو در این حوزه به ما می ده تا بتونیم با ترکیب با سایر رویکردها از جمله شبکه های عصبی و مدل های یادگیری عمیق کارها و پروژه های بسیار خوبی را انجام بدیم بنده به شدت استقبال می کنم و بی صبرانه منتظر شروع آن هستم و با تجربه ای که از آموزش ها و شناخت شما دارم قطعا در کلاس شرکت خواهم کرد امیدوارم کلاس همچون کلاس یادگیری عمیق رویکردی ترکیبی مبتنی بر مفاهیم پایه و تئوری و هم کد زنی و رسیدن به مباحث پیشرفته و پروژه محور داشته باشد با تشکر
-> سلام استاد وقتتون بخیر این فیلد جز الویت های من نیست و ترجیحم بر تهیه دوره های دیگه تقویتی nlp هست با تشکر
-> سلام با تشکر از زحمات شما به نظر من علاوه بر پردازش تصویر کلاسیک بحث بینائی کامپیوتر کلاسیک رو هم بگین و اگه بشه غیر از opencv کتابخونه skimage هم گفته بشه که عالی هست
-> سلام استاد منم قطعا شرکت می کنم و سعی می کنم به موازات دوره شما کتاب پردازش تصویر گونزالس رو در کنارش مطالعه کنم به نظرم بررسی تئوری های پردازش تصویر و به موازات اون دستورات معادلش و پیاده سازی مثال در اوپن سی وی می تونه خیلی کمک کننده باشه و هر چند فصل یک بار یک مینی پروژه ای داشته باشه که مباحث چند فصل به صورت عملی در یک پروژه کوچیک جمع بندی و مرور بشه مثلا به نظرم استفاده از هاف ترنسفورم الگوریتم هایی مثل SIFT و HOG و در دوره باشه به جامعیت دوره خیلی کمک زیادی می کنه حتی شاید یک فصل هم مثلا برای معرفی روندهای فعلی در حوزه های تحقیقاتی پردازش تصویر کلاسیک کمک کننده باشه و مقالات جدید و حوزه های کاری جدید صرفا معرفی بشن در حد آشنایی البته به نظرم جدا از تصویر یک فصل هم اگر برای کار عملی با ویدئو به کمک open cv باشه می تونه خیلی کمک کننده باشه و جامعیت دوره رو بیشتر کنه هر چند خیلی هم ضروری نیست داشتن یک فصل مجزا برای ویدئو و مورد بعدی این هست که اگر الگوریتم هاب موجود در open cv امکان کاستومایز کردن رو دارن یک نمونه کلی ازش ببینیم که چطور چنین کاری امکان پذیر هست ولی بازم یه نظرم در این دوره جنبه های تئوری و علمی علم پردازش تصویر برای شخص بنده نسبت به پیاده سازی یه مقدار مهم تره و ممنونم از شما و تیم خوب هوسم بابت زحمتی که می کشین و کارهای با کیفیت تولید می کنین
-> سلام بنده به عنوان کسی که دوره پردازش تصویر قبلیتون را تهیه کرده و دوره را به اتمام رسونده اظهار نظر میکنم دوره قبلی واقعا عالی بود هم به تئوری هم به کد زنی پرداخته شد فقط یک سری ایرادات داشت که انشاءالله در سری جدید پردازش تصویر امیدوارم رفع بشه مثل اینکه ۱صدا خیلی نویز داشت و رو اعصاب بود ۲اینکه تو کلاس زمان گذاشته میشد برای پرسش و پاسخ حوصله سر بر بود اگه افلاین ضبط بشه بعد هر کس سوال داشت در گروه مربوط بپرسه به نظرم خیلی بهتره و یه چندتا خواهش کوچولو لطفا دوره را برای کسایی که دوره قبلی را تهیه کرده بودند باز بزارید و مورد دوم از کتابخانه sypy پایتون هم بگید لطفا باتشکر
-> سلام آقای دکتر دوره اگر حول فیلترها و لبه یابی و تبدیل فوریه و باشه بسیار کسل کننده خواهد بود حداقل برای من به نظر من خیلی بهتره که مثل دیپ به سه تسک طبقه بندی و آبجکت دتکشن و سگمنت کردن تقسیم بشه و روی این ها مباحث آموزش داده بشه جدا از این تازگیا قدرت اپن سی وی رو درک کردم انگار خیلی کم بهش پرداخت شده موفق باشید
-> سلام به نطر من هم پروزه های زیادی رو میشه به جای دیپ لرنینگ با opencv انجام داد همچنین وقتی با رزبری پای ارتباط پیدا میکنه خیلی کاربردی و جذاب میشه البته نمیگم رزبری پای بگید منظورم اینه چنین پروزه هایی که بعدا خودمون با رزبری پای اجرا کنیم
-> محض یادآوری لطفا پیام ریپلای شده رو بخونید و نظرتون رو بگید
-> سلام بر استاد اشرفی برای دوره ی ML ظاهرا خودتون تصمیم گرفتید که این شد نتیجه اش در خصوص سایر دوره ها هم احتمالا باز شما تصمیم بگیرید به صلاح اکثریته
-> سلام استاد من کاملا از برگزاری این دوره استقبال می کنم و اگه سرفصل ها خیلی کاربردی و پروژه محور باشه و موارد غیر مهم و کم اهمیت تر هم کمرنگ باشن بهتره
-> سلام ممنون لطف دارید راستش صحبتهای دوستان روی دورهها مستقیم یا غیرمستقیم اثر میذاره دوستان کامنتهای خوبی دادن
-> استاد سلام وقت بخیر الان توی این قسمت دکامپوزیشن این دوتا الگوریتم unsupervised که دیروز گفتم بهتون شامل مثلا مباحث ماشین نمیشه چون اینا تو بحث داده های متنی هست خواستم ببینم تو داده های tabular استفاده نمیکنند ممنون
-> اولی رو نمیدونم ولی دومی فکر کنم استفاده میشه
-> ممنونم
-> سلام خوبید میشه سیلابسش ها رو بفرمایین که بتونیم نظر بدیم قطعا که خوبه ولی ایا پیشنیاز ماشین ویژن رو قبلش میخواد یا همونجا میخواد در قالب ماشین ویژه باشه
-> سلام میخواستیم نظر دوستان رو به صورت کلی بدونیم و نمیخواستیم بایاس به سیلابس ایجاد بشه
-> سلام استاد وقت بخیر توی ویدیوهای ماشین لرنینگ چند باری اشاره کردید ماشین لرنینگ پیشرفته و مقدماتی من سرچ کردم یه مقداری ولی ماشین لرنینگ پیشرفته همه جا همون الگوریتمای عادی بودن دیپ لرنینگ میشه بگید منظورتون از ماشین لرنینگ پیشرفته چی هست
-> سلام کجا گفتم میتونید آدرس یکیشو بدید چک کنم
-> اخرین بار اگر اشتباه نکنم ویدیوی اشنایی با آمار بود ولی مطمئنم توی ویدیوهای قبلی هم گفتید چون یک سری جاها که بحث دیپ شدن روی الگوریتم ها بود گفتید فعلا توی مرحله مقدماتی نیازی نیست و با اونها توی ماشین لرنینگ پیشرفته مواجه میشید
-> نه منظورم این بود که فعلا در سطح این دوره و شروع کار همین اندازه آمار و احتمال براتون کافیه دو سه سالی که کار کردید و تجربه بدست آوردید میتونید برای یاد گرفتن بیشتر آمار و احتمال اقدام کنید یا الگوریتمها و متدهای بیشتری یاد بگیرید و
-> بصورت کلی که کار خیلی خوبیه من خودم اگر میتونستم به تنهایی نظر دهنده باشم دوست داشتم این کلا در قالب یه دوره پردازش تصاویر از ابتدا و روشهای سنتی مهندسی مخابرات تا روشهای نوین یادگیری عمیق و ماشین ویژن باشه
-> سلام وقت بخیر ضمن تشکر از دورههای خیلی عالی شما که واقعا توی این حوزه بهترین هستین دوتا مورد برام جای سوال هست اول اینکه مشابه با opencv کتابخونههای دیگه هم هستن که خیلی از کارهای این حوزه رو راه میندازن ینی رقیب زیاد داره و دلیل اینکه همیشه گزینهی اول هست مگر اینکه خلافش ثابت بشه رو دوست دارم بدونم مورد دوم اینکه آیا بیشتر مطالب پردازش تصویری هست و بر اساس کتاب گنزالس پیش خواد رفت
-> مخابرات چرا
-> ممنون عرفان جان مثلا چه کتابخونههای دیگهای هست
-> PIL ScilitImage
-> 
-> اینا هم خوبن خصوصا سایکیت ایمیج جالبه من ازش خوشم میاد اما همین سایکیت متمرکز بر پردازش تصویر هست نه بینایی کامپیوتر ولی اوپن سی وی اوپن سورس کامپیوتر ویژن هست در بخش ویدئو هم اوپن سی وی خوبه مثلا خوندن ویدئو یا دوربین داره ترکینگ داره و نسخه سی پلاس پلاس عالی داره که فکر نکنم بقیه داشته باشن کامیونیتی بزرگی داره بیشتر از بقیه در کارهای عملی استفاده میشه
-> سلام وقت بخیر تو دوره ماشین لرنینگ ایا از لحاظ اماری هم وارد مباحث میشیمیچیزی شبیه چیزی ک تو درس پترن هستش
-> کلا این دوره خیلی از آمار تاثیر گرفته فکر کنم حدود 13 ساعت فقط در مورد آمار و احتمال به عنوان پیش نیاز صحبت شده تلاشمون این بوده که به اون نگاه آماری اهمیت داده بشه
-> بیصبرانه منتظرم
-> گونزالز یکی از رفرنسهامون هست ولی اینطور نیست که با سرفصل اون جلو بریم و همون رو آموزش بدیم سرفصلها متنوعتر هست
-> من شخصا پردازش ویدیو با استفاده از opencv رو خیلی براش مشتاق هستم به عنوان یک درخواست هم بهنظرم اگر کار با دیوایس و دیپلوی کردن برنامههای مربوطه رو اگر در دستور کار قرار بدید خیلی عالی میشه
-> سلام استاد این دوره جذاب کی شروع میشه تاجایی که یادم میاد گفته بودید توی آذر میاد بیشتر از خود دوره خیلی کنجکاوم سر فصلاش اعلام بشه
-> آره نزدیکه فکر کنم فهرست مطالبش خوب باشه به نظرم جذاب فقط دیپ کاتالیست
-> من علاقمند به این دوره هستم و حتما شرکت میکنم در مورد پیشنهاد اگه بشه یه سری دیتاها از تصاویر پزشکی خصوصا MRI و fMRI باشه خیلی عالی میشه من دوره بینایی کامپیوتر رو هم شرکت کردم اونجام یه پیشنهاد داشتم یا برای یکی از پروژه های دیپ لرنینگ کاتالیست اونم تحلیل تصاویر fMRI با کانولوشن سه بعدی
-> سلام وقت بخیر fmri اصولا تصاویرش رزولوشن پایینی دارن و کارانچنانی نمیشه کرد باهاش بیشتر از سیگنالش استفاده میکنن که اونم یکم دردسره کلا کلا معمولا پیش پردازش داده هاش و عملیات متفاوتش باپایتون سرعتش خیلی کمه و محدود بیشتر از afni fsl و استفاده میشه برای پردازشش چه تصاویر و چه سیگنال و یچیزی مثل afni روی لینوکس فقط قابل اجراس و بشدت سخت و کشنده توی دیپ بیشتر از داده هایی که پیش پردازش شده و امادس استفاده میشه
-> من همه نرم افزارهای fsl و afni و spm و رو بلدم سخت و کشنده هم نیستن حقیقتا خیلی هم راحتن منتها بدی این نرم افزارها اینه که شما باید از آنالیزهایی استفاده کنید که دکمه ش رو اونجا گذاشتن یعنی از آنالیزهایی که جدید هستند و توی مقالات پیشرفته معرفی میشن خبری نیست یا اگه خودتون ایده ایی داشتین نمیتونین پیاده سازی کنین در مورد پایتون هم من انجام دادم و اصلا سرعتش پایین نبوده و خیلی بهتر از نرم افزارهایی که معرفی کردین هست توی یوتیوب سرچ کنین اکثر دانشگاههای معتبر دنیا برای اینکه آنالیزهای جدید و ایده های خودشون رو پیاده سازی کنن با پایتون آنالیز میکنن و اتفاقا روی گوگل کولب خیلی هم راحت دست میشه در مورد اینکه رزلوشن پایینی دارن هم خب کارای پیش پردازشی زیادی میشه انجام داد ولی در عوض ما دینامیک مغز رو در طول زمان داریم و باید با این چالش دست و پنجه نرم کنیم مهم اینه بفهمیم توی اون زمانی که از مغز در حین انجام یه تسک یا در حالت rest یه فیلم گرفتیم چیا بهش گذشته و خب این دینامیک بودنه خیلی جذابش میکنه
-> شما با mevisvlab هم کار کردید
-> سلام شب تون بخیر نه اسم ش رو هم نشنیده بودم الان سرچ کردم سایتشو دیدم یه فریمورک برای پردازش تصاویر پزشکی هست"
"-> ممنون میشم واقعا هر نظری دارید به اشتراک بزارید که بدونم کدوم سمتی باید برم و چی رو باید یاد بگیرم که اجرا کنم
-> 
-> خیلی ممنون از راهنمایی تون صحبتی که از مدل شده بود فقط یه ایده بودواقعیتش اینه که من دوره ی یادگیری عمیق رو چند ماه پیش دیدم و عملا چیز زیادی ازش یادم نیست و کد نویسی پایتورچ م هم خوب نیست متاسفانهمضافا اینکه اصلا رشته م یه چیز دیگه ستگفتم بپرسم شاید یه مدل آماده ای یا چیزی باشه من بگم مثلا 100 عکس دارم و بهش بگم این 50 تا عکس دسته ی اول هست و این 50 تا عکس دسته ی دوم حالا بهم بگو عکس 101 م جزو کدوم دسته هستبرای این کاربرد روش راحتی یا مدل آماده ای یا ابزاری هست که زیاد درگیر جزئیات نشم یا باید حتما یه مدل آموزش بدم
-> خب اگه پروژه دانشگاهی نیست یا بدید یک نفر براتون انجام بده یا اینکه به چت جی پی تی خوب توضیح بدید که کدش رو بنویسه یک نرم افزاری سال قبل اومده بود بنام تورچ استودیو که به صورت نرم افزاری میتونستید تنظیماتی انجام بدید و مدل ترین کنید شایدم کتابخونه ای مثل کراس وجود داشته باشه که مثلا با دو سه خط کد بتونید کارتون رو انجام بدید که من نمیشناسم"
"-> زندگی صابر راستیکردار کوتاه اما کارهای او بسیار با ارزش و ماندگار بود متاسفانه صابر خالق فونتهای زیبای وزیر متن و گندم و شبنم و کلی فونت دیگر دیروز آسمانی شد صد حیف و صد افسوس با آرزوی صبر برای خانواده صابر عزیز روحش شاد
-> "
"-> سلام وقت بخیر منابع اموزشی دوره بینایی کامپیوتر به زبان انگلیسی مانند object detection pose estimation رو از چه سایتی میشه دریافت کرد ممنون میشم راهنمایی داشته باشید
-> یادگیری عمیق در بینایی کامپیوتر جاستین جانسون در میشیگان بسیار عالی و سطح بالاست البته به نظرم استراتژیتون درست نیست
-> ممنونمبحث pose estimation در اموزش اقای جاستین جانسون وجود نداره اموزش دیگه ای رو معرفی بفرمایید استراتژی درست رو هم راهنمایی بفرمایید
-> نمیشناسم
-> ممنون مبحث pose estimation در دوره بینایی کامپیوتر شما هست به خاطر همین پرسیدم
-> بله هست متوجه منظورتون نشدم به خاطر چی پرسیدید
-> من چون دیدم در مطالب تدریس هست پرسیدم راهنمایی داشته باشید در مورد اموزش های این مبحث
-> آهان دورهای معرفی کنم که مثل جاستین جانسون ناقص نباشه و تخمین پوز رو هم داشته باشه در واقع اون دوره به خاطر اینکه این قسمت رو نداره به کارتون نمیاد درست متوجه شدم
-> بله منظورم همین هست که دوره شامل pose estimation هم باشه استاد ببخشید دوره جاستین جانسون اموزش هاش کد نویسی نداره اگه ممکن هست دوره ای معرفی بفرمایید مربوط به مباحث بینایی ماشین که همراه با کد نویسی هم باشه ممنون
-> خب استراتژی اشتباه یعنی همین چیزا دیگه یک دوره عالی رو کنار میذارید چون فقط یک مبحث رو نداره چرا چون یک جایی یکی دیگه این رو در دوره گفته یک دورهای دیدید و ارزیابیتون مثبت هست ولی دنبال کپی خارجی همون هستید و این باعث میشه یکی پس از دیگری منابع رو بزنید کنار من که جز این مطالب راهنمایی خاصی ندارم
-> نخیر سوء تفاهم نباشه منظورم این نیست که دوره کنار گذاشته بشه من منظورم این بود که دوره ای هم باشه که همراه با کد نویسی همون مطلب و همچنین در موردpose estimation هم اموزشی داشته باشه ممنون
-> خب با این پیامتون روی مورد دوم از آخرین پیامم صحه گذاشتید همون اول گفتم دیگه نمیشناسم حداقل من اینجوری کسی رو راهنمایی نمیکنم
-> ممنون اگه از دوستان اموزشی در مورد بینایی کامپیوتر همراه با کدنویسی سراغ داشتند ممنون میشم راهنمایی داشته باشندممنون
-> استاد سلام وقت بخیر فرق بین probability vs likelihood میگید ممنون
-> Probability همون احتمال است مثلا سکه ای پرتاب می کنیم احتمال اینکه تو یه سکه سالم خط یا شیر بیاد ۵۰ درصد است Likelihood تابع درستمایی است مثلا یک سکه ای را ۱۰۰ بار پرتاب کرده ایم میخواهیم ببینیم آیا سکه سالم است برای این کار از تابع درستمایی استفاده میکنیم در واقع تابع درستمایی تابعی از پارامترها و مقادیر مشاهده شده است اگه جوابم اشتباه بود استاد تصحیح کنید
-> ممنون"
"-> سلام وقت بخیر سوالی داشتم از خدمتتون اینکه امکانش هست نتایج شبکه ای با پایتورچ بسیار متفاوت باشه با همون شبکه تو کراس من برای از cnn برای طبقه بندی تو هر دو استفاده کردم تمام تبدیل ها و مثل هم هستند ولی نتایج تو پایتورچ ۱۰ درصد بدتر از کراس شده
-> این اختلاف خیلی زیاده برای شبکههای معروف حوزه دستهبندی تصویر مثل رزنت و غیره وقتی تنسورفلو و پایتورچ رو مقایسه میکردن زیر 1 درصد اختلاف داشتن جزئیات زیاد داره و خیلی مولفهها رو باید چک کنید
-> استاد سلام ی موقع هست ما پروژه ای انجام دادیم و کلی کتابخونه مثلا 4050 و ب بالا ایمپورت کردیم فایل پروژه برای کسی میفرستیم اینجا چند تا نکته پیش میاد این ک خوب طرف ممکنه ورژن کنابخونه ها ش با ما یکی نباشه و هی باید upgrade downgrade کنه ورژن هاشو و این که از یه طرف هم ممکنه خوب اون ریپازیتوری هایی ک ماژول pip اش داره با ما فرق داشته باشه لینک دانلود پکیج هاش تو این حالت بهترین کار چیه ممنون
-> 
-> ممنونم"
"-> سلام استاد وقت بخیر مبحث به اسم lete fusion و intermediate fusion و early fusion به گوشتون خورده معادل فارسیش تو مباحث ما بوده ممنون میشم راهنمایی کنید اگه امکان پذیره
-> سلام این مباحث در بحث دیتاهای مالتی مدال یا مالتی اینپوت کاربرد دارن
-> ممنونم"
"-> سلام وقت بخیر برای طراحی مدل اگر یک مدل سری را داخل مدل سری دیگری استفاده شود با توجه به اینکه این مدل با سایر لایه های مدل سری تعریف کردیم در عمل لایه های دو مدل باهم چه حالتی دارند سری یا موازی یا اصلا چیز دیگری هست برای مثال دو مدل زیر مثله هم هستند در مدل دوم مدل hiden با لایه های بعدی چه حالتی داره model1nnSequentialnnLinearnum_inputhl1 nnReLU nnLinearhl1hl2 nnReLU nnLinearhl2num_class hidennnSequentialnnLinearnum_inputhl1 nnReLU model2nnSequentialhiden nnLinearhl1hl2 nnReLU nnLinearhl2num_class
-> سلام خب دو تا سری سری میشه دیگه نمیشه
-> پس میفرمایید دو مدل یکی هستند درسته منم با همین منطق داشتم تحلیل میکردم ولی وقتی تو عمل برای یک دیتا و کد کاملا یکسان دو مدل رو امتحان کردم نتایج کاملا متفاوتی رو بهم برگردوند نه خطای ولیدیشن و نه حتی learning curve اونها شبیه هم نبود دلیل این عدم یکسان بودن نتایج رو متوجه نمیشم
-> دیتا و کدی هم که داشتم استفاده میکردم همون کد و دیتای کلاس بندی موبایل هست که با MLP انجام دادید
-> نمیدونم نمیشه اینجوری وارد جزئیات شد"
"-> سلام استاد وقتتون بخیر استاد ببخشید یه سوال داشتم برای تشخیص سطح صاف ایا میشه در جمع اوری دیتاست از تصاویر معمولی استفاده کرد یعنی منظورم این هست که تصویر عمق نداشته باشیم و مورد بعدی اینکه ایا میشه تصویر ورودی به شبکه یک تصویر RGB معمولی باشه چون من دیدم اکثر مقالات از مجموعه داده هایی که شامل تصویر عمق هست استفاده کرده بودند خیلی ممنونم
-> سلام نمیدونم موضوع چی هست تشخیص سطح صاف یعنی چی تصویر معمولی و غیرمعمولی یعنی چی و
-> استاد یک محیطی هست که پستی و بلندی زیادی دارد و شیب یا به عبارتی جایی مناسب که قابل ایستادن باشه کم داره و تصویر معمولی منظورم تصویر بدون عمق هست یعنی depth map نداره یا به عبارتی با یک دوربین معمولی گرفته شده برای مسیر یابی ربات میخواستم انجام بدهم ربات جایی که میخواد بره باید سطح مناسبی داشت باشه تا لیز نخوره و بتونه ساکن باشه
-> استاد منظور را کامل رسوندم
-> از یک تصویر هم نقشه عمق استخراج میکنن ولی من تو این حوزهها کار نکردم و نمیدونم در چه حدی هستن
-> استاد پس میشه دیتاستی که جمع آوری کردیم فقط از تصاویر معمولی تشکیل شده باشه و تصویر عمق نداشته باشه
-> من یک کلیدواژه دادم ازش نتیجهگیری نکنید تحقیق کنید شاید اصلا مناسب کار شما نباشه و"
"-> به نظرم ایراد کار اینجاست که بعد لود کردن میخواید سریع به دیتا فریم تبدیل کنید و خروجی توی ذهنتون رو بهتون بده فکر نکنم اینطوری بشه شما باید از دل mat_data متغیرهای مختلف رو بکشید بیرون و در یک دیتا فریم یا دیکشنری بچینید یکم کدنویسی میخواد
-> فکر کنم متوجه منظورتون شدم خیلی ممنونم"
"-> سلام دوستان ببخشید در مورد اجرای این کد کسی میتونه کمک کنه GitHub nyukatGMIC An interpretable classifier for highresolution breast cancer screening images utilizing weakly supervised localization
-> سلام دستورالعمل داده کجاش مشکل دارید"
"-> استاد سلام میشه لطفا یه دوره NLP خوب و ترجیحا خلاصه معرفی بفرمایید
-> 
-> ممنون استاد ولی این دوره برای ثبت نامش اطلاعات پرداخت رو میخواد که برای ما امکان پذیر نیست
-> از فاند باید استفاده کنید
-> میشه بیشتر راهنمایی کنید
-> همون بالای هر دوره یه گزینه به اسم financial aid وجود داره پرمیکنید و دوره رو میده میشه مثل لینک پایین دانلود هم کرد
-> ممنون
-> عالیه ممنون
-> اگر سرتیفیکیت برات مهم نیست این دوره رو میتونی رایگان دانلود کنی"
"-> سلام دکتر وقتتون بخیر دکتر میخواستم بدونم شما راجع به MLops اطلاعی دارید راستش میخواستم به عنوان پروژه خودم در کارشناسی موضوعی مرتبط با آن بردارم و یادش بگیرم میخواستم ببینم شما پیش زمینه ای دارید از این موضوع تا بتونید بنده را راهنمایی بفرمایید
-> سلام نه تخصص ندارم"
"-> سلام یک سوالی داشتم و از اینکه وقت می گذارید و جواب می دهید بسیار سپاسگزارم توالی را با روش one hot تبدیل به بردار کردمابعاد torchSize46710 4 28 را دارم 4 تعداد فیچر و 28 طول توالی دادم به یک اینکدر و دیکدر و فیچر استخراج کردم خروجی اینکدر شده torchSize46710 40 6 الان این 40 تعداد فیچر هست درسته با فرض درست بودن من این فیچرها را که ابعاد آن torchSize46710 40 6 هست را با تعدادی داده دیگه که ابعاد آنها torchSize46710 4 20 می خواهم کانتکت کنم برای اینکه اینها را با هم بتونم ادغام کنم ابعاد فیچر را تبدیل کردم به torchSize46710 240 و ابعاد داده جدید که می خواهم با این فیچرها کانتکت کنم را تبدیل کردم به torchSize46710 80 و انها را با هم کانتکت کردم torchSize46710 320 حالا این را می خواهم به یک مدل CNN بدهم این کارم از نظر منطقی درست هست
-> سلام آره منطقیه
-> ممنونم یک سوال دیگه وقتی من دارمtorchSize46710 320 را به مدل CNN می دهم می تونم یک بعد به داده اضافه کنم بشه torchSize46710 3201"
"-> سلام دوستان کسی شبکه سیامی کار کرده
-> "
"-> سلام وقت بخیر من یه دیتاست از سیگنالهای مخابراتی با مدولاسیونهای مختلف دارم اما تعداد سیگنالی که از هر مدولاسیون دارم متفاوته و برام مهمه که تعدادشون برابر بشه درواقع دیتاستم imbalanceعه چجوری میتونم این مشکل رو حل کنم
-> سلام من سوال شما رو متوجه نشدم دلیلش رو میگم شاید براتون مفید باشه مدولاسیون یعنی چی من مخابرات نخوندم و بنابراین با اصطلاحات کار شما آشنا نیستم درحالیکه متن شما برای افراد مشابه با خودتون نوشته شده یعنی کسانی که هم مخابرات باشن و یادگیری ماشین بلد باشن لازمه درمورد دادهتون در عین خلاصه بودن شفاف توضیح بدید تا حتی کسانی که همرشتهای شما نیستن بتونن مساله رو بفهمن و کمکتون کنن
-> تو مخابرات یه سیگنال حامل داریم و یک سیگنال پیامکه توالی از صفر و یکهاست هدف اینکه سیگنال پیام رو ارسال کنیم اما امکان ارسال سیگنال پیام به همون شکلی که هست وجود نداره میایم سیگنال پیام رو سوار سیگنال حامل میکنیم مثل این میمونه که یه آدمی میخواد از کویر عبور کنه خودش به تنهایی توانایی عبور از کویر رو نداره اما اگر سوار شتر بشه اون شتر میتونه عبورش بده از کویر حالا چجوری سیگنال پیام رو سوار سیگنال حامل میکنیم میایم یکی از پارامترهای سیگنال حامل رودامنه فرکانس یا فاز رو با توجه به سیگنال پیام تغییر میدیم به این کار میگن مدولاسیون"
"-> سلام ممنون میشم اگر یک مسیر برای ورود به حوزه NlP بیان کنید و همچنین در این حوزه نیاز به چه مهارت هایی است و ایده هایی که نیاز دارن به نواوری در این حوزه چیا هستند نظر شما در خصوص social bot هارو هم ممنون میشم بیان کنید ایا تو زمینه زبان انگلیسی بهتره کار شه یا زبان فارسی
-> 
-> سلام خیلی ممنونم ازتون"
"-> 
-> لینک کانال و فیلم های ضبط شده و اطلاعیه ها"
"-> 
-> لینک گروه یادگیری تقویتی"
"-> سلام وقت شما هم به خیر فکر نمی کنم کار درستی باشه این جا لینک ها رو بفرستم هر کدوم از دوستان که مایل بودن یک پیام توی پیوی بفرستین برای بنده لینک گروه و کانال رو خدمت تون ارسال می کنم کاملا رایگان هم هست و هزینه خاصی نداره
-> سلام امیررضا جان مشکلی نیست لینک بذارید اگر هم نیاز به همراهی از جانب هوسم هست ما در خدمت شما و سایر دوستان هستیم که شرایط دریافت آموزش براشون تسهیل بشه
-> خیلی ممنونم از لطف شما استاد الان لینک ها رو ارسال می کنم همین جا از لطف شما و مجموعه هوسم بسیار سپاسگزارم"
"-> سلام استاد وقت بخیر آیا دورهای برای یادگیری تقویتی Reinforcement Learning دارید یا در موردش در دوره خاصی آموزش دادید تشکر
-> سلام قرار هست در ادامه کورس های دیپ لرنینگ و همین طور ماشین لرنینگ بنده در حدود چندین ساعت یک مینی دوره برای یادگیری تقویتی ضبط کنم از اواسط آذر ضبط ویدئو ها رو شروع می کنم"
"-> سلام استاد وقت بخیر شما قبلا مسیر وارد شدن به کامپیوتر ویژنو گفته بودید که ماشین لرنینگ دیپ و کامپیوتر ویژن به ترتیب بودن من خودم کارم فقط با دیپ و تصویره اما واسم جالب بود برای این مسیر ویژن چه مطالبی از ماشین لرنینگ نیازه ماشالا دوره ماشین لرنینگ خیلی پرو پیمونه و واقعا وقت نمیشه همشو دید ممنون میشم اگر مطالبی که از دوره ماشین برای ویژن نیازه بگید بصورت تاپیکی مطالعه کنم
-> "
"-> سلام دکتر شما کورس مناسب دیپلرنینگ برای کسی که یادگیری ماشین و دیپ لرنیگ را از دوره های رایگان هوسم فراگرفته و قصد داره بینایی ماشین حرفهای شروع کنه چی پیشنهاد میکنید کورسی که از اندرو در سایت هوسم هست حس میکنم یکم قدیمی هست و آپدیت نیست همین طور برای بینایی کامیپورتر پردازش تصویر هوسم را دیدم و قصد دارم بینایی ماشین حرفهای هوسم را هم ببینم منتها دانشچو هستم و از لحاظ اقتصادی نمیتونم دیگه دوره یادگیری شبکه های عمیق هوسم را تهیه کنم خواستم اگه براتون مقدور هست یه کورس مناسب رایگان معرفی کنید تا یادگیری عمیق را یاد بگیرم
-> کتاب d2lai یادگیری عمیق در بینایی کامپیوتر جاستین جانسون در میشیگان هر دو عالی هستن البته با وجود منابع بالا بازهم من توصیه میکنم همون دورههای سایت رو تهیه کنید شما مجبورید چند تا منبع با سرعت کمتر بخونید تا بتونید مطالب دوره سایت رو پوشش بدید ولی دوره سایت جمع و جوره تئوری داره کدنویسی داره مینی پروژه داره و هزینه هم که زیاد نیست تو تخفیفها میشه با هزینه خیلی کمی تهیه کرد تعریف و تبلیغ از خود میشه ولی بازم میگم دوره یادگیری ماشین بیش از 120 ساعت آموزش داره من که نظرم این هست حتی اونهایی که یادگیری ماشین رو قبلا گذروندن و به مباحث آشنا هستن هم این دوره رو تهیه کنن و چند بار ببینن
-> یه سوال دیگه اگه مثلا الان من دوره یادگیری عمیق ۲۰۲۲ را تهیه کنم و بعدا آپدیتی بیاد مثلا ۲۰۲۳ به آپدیت میتونم دسترسی داشته باشم یا برای اون هم باید چداگانه هزینه ای بپردازم
-> اگه فصلی و محتوایی به دوره اضافه بشه همه بدون هزینه دسترسی دارن
-> استاد در دوره جدید دیپ لرنینگ بحث gcn اضافه شده ولی ما دوره قدیمی را شرکت کردیم بهش دسترسی نداریم البته اگر در هر دوره جدید مباحث تازه ای اصافه شد ممنون میشیم بصورت جداگانه قابل خریداری بشه انجام داد ممنون میشیم زحمات شما در دوره هایی که برگزار میکنید واقعا جای تشکر داره ازتون واقعا ممنونم
-> سلام من اینجا دلیلش رو با جزئیات گفتم چون سوال پرتکراری هست عذر میخوام طولانی شد
-> توضیحاتتون مثل همیشه جامع و کامل بودتشکر میکنم از خرید دوره های شما همیشه چیزهای جدید یادگرفتیم حتی اگر مجدد هم دیپ لرنینگ را بخرم مطمئنا برایم سود خواهد داشت"
"-> class CustomCAEnnModule def initself superCustomCAE selfinit selfencoder nnSequential nnLinear3318 1536 nnBatchNorm1d1536 nnReLU nnLinear1536 256 nnBatchNorm1d256 nnReLU selfdecoder nnSequential nnBatchNorm1d256 nnLinear256 1536 nnBatchNorm1d1536 nnLinear1536 3318 def forwardself x x selfencoderx x selfdecoderx return xsqueeze def extract_featuresself x return selfencoderx model CustomCAE
-> عکس هم نیست
-> چطور اینطوری فرستادید
-> 
-> آقا دم شما گرم"
"-> سلام وقت همگی بخیر من دیتاستی دارم که خیلی از ویژگیهای دیتاست categorical هستند میخوام ببینم اینا رو باید قبل از اینکه مثلا طبقهبندی کنیم dummy کنیم یا نه اگر همه رو دامی کنم دیتاست من خیلی بزرگ میشه و مشکل ساز میشه
-> سلام باید تک تک متغیرهای کتگوریکال رو بررسی کنید شاید بعضی موارد ترتیبی باشن هرکدوم که ترتیبی باشه تبدیل به یک ستون میشه
-> ممنونم"
"-> class CustomCAEnnModule def __init__self superCustomCAE self__init__ selfencoder nnSequential nnLinear3318 1536 nnBatchNorm1d1536 nnReLU nnLinear1536 256 nnBatchNorm1d256 nnReLU selfdecoder nnSequential nnBatchNorm1d256 nnLinear256 1536 nnBatchNorm1d1536 nnLinear1536 3318 def forwardself x x selfencoderx x selfdecoderx return xsqueeze def extract_featuresself x return selfencoderx model CustomCAE در مدل بالا من دارم فیچر استخراج می کنم
-> جهت خوانا تر شدن کد ها در تلگرام"
"-> مقدار داده تست را افزایش دادمداده تست و ترین را هم بصورت رندوم جدا کردم پارامترها را هم چندیدن بار چک کردم نمی فهمم چرا خطا تست کمتر از خطا ترین داره میشه آیا این حالت برای مدلهایCAE عادی هست
-> 
-> 
-> ممکنه مدل را خوب طراحی نکرده باشم البته تابع leaky relu را هم امتحان کردم همین طور شده
-> "
"-> سلام استاد وقت بخیر من وقتی یه cnn رو زدم ترین از ۴ گیگ وی رم gpu انقدشو مصرف کرد در حال ترین این که مقدار کمتر از یک سوم مصرف شده معنی میده که بچ سایز رو میتونم بزرگتر بزارم
-> بله
-> به نظرم میشه تعداد پارامترها رو هم بیشتر کرد البته بستگی به دیتا و مسئله داره"
"-> کلا کار روی شبکه های سبک وزن هست ساختار شبکه ها تغییر می کنند و هدف این هست با پارامتر کمتر عملکرد افزایش یابد
-> 
-> سپاس"
"-> شبکه سوپر رزولوشن هست و پایان نامه دو بخش را جدا در نطر گرفتم که نتایج اثر هر بخش را بدست بیاورم
-> یعنی ایدهتون این بوده که شبکههای متفاوت در نظر بگیرید"
"-> من دوتا ساختار مختلف تست کردم نتایج اصلا خوب نشد
-> بستگی به هدف داره دیگه باید دید هدفتون چی بوده چرا از یک جنس در نظر نگرفتید ضمن اینکه در پیام قبلی هم گفتم الزامی وجود نداره یکی باشن ولی معنیش این نیست که به جواب هم برسه"
"-> سلام استاد وقت خوش ببخشید در رابطه با GAN در generator و Discriminator ساختارها لازم هست یکسان باشد یا از دو نمونه شبکه مختلف میشه استفاده کرد
-> سلام الزامی وجود نداره ولی معمولا از یک جنس هستن"
"-> سلام علت تفاوت relative error با R2 چیه کدومشوم معیار بهتری برای دقت پبشبینی شبکه هستند
-> سلام جواب GPT4 علت تفاوت بین خطای نسبی Relative Error و R2 این است که هر کدام از این معیارها به شکل متفاوتی عملکرد یک مدل پیشبینی را اندازهگیری میکنند خطای نسبی Relative Error این معیار نشان میدهد که چقدر پیشبینی مدل نسبت به مقادیر واقعی دادهها انحراف دارد مقدار کمتر خطای نسبی به معنای دقت بیشتر مدل در پیشبینی است معیار R2 ضریب تشخیصاین معیار نشان میدهد که چه میزان تغییرات متغیر وابسته هدف توسط متغیرهای مستقل ویژگیها توضیح داده شده است مقدار R2 بین 0 و 1 قرار میگیرد و مقدار بالاتر به معنای عملکرد بهتر مدل است برای انتخاب بهترین معیار برای دقت پیشبینی شبکه بستگی به مسئلهای که قصد حل آن را دارید دارد اگر هدف شما کاهش خطای پیشبینی است استفاده از خطای نسبی میتواند مناسب باشد در صورتی که میخواهید بفهمید چقدر مدل شما توانایی توضیح تغییرات متغیر هدف را دارد استفاده از R2 مناسب است
-> ممنون ولی به زبان ساده تر میشه گفت r2 چیو نشون میده ایا r2 بالاتر به معنای عملکرد بهتر شبکه در پیشبینهاست
-> بله R2 بالاتر به این معنیه که مدل توانایی بهتری در تشخیص ارتباط بین متغیرها داره و پیش بینی دقیق تری میده"
"-> سلام استاد وقت بخیر شما گفتید این مثال ها واقعیه و خودتون رسم کردید واسم خیلی جالب بود که با چه دستوراتی اینارو رسم کردید اگر امکانش هست کدشو بگید یا راهنمایی کنید ممنون میشم خیلی واسم جالبه
-> با متپلات لیب کار سختی نیست و دستور پیچیده ای هم نداره مثلا اینجا از دستور کانتور و پلات استفاده شده الان توی دوره یادگیری ماشین عمق و دیپ کاتالیست دوستانی هستن که از این نوع خروجیها تولید میکنن بعد اینکه نمودار رو کشیدید باید برای زیبا کردنش با رنگ و استایل و غیره وقت بذارید اولین بار خیلی وقت گیره و بعدش دیگه از اون تنظیمات قبلی میتونید استفاده کنید چت جی پی تی هم که هست و میتونه خیلی کمک کننده باشه"
"-> سلام استاد با عرض خسته نباشید یه سوال داشتم از خدمتتون و راهنمایی میخواستم تجسم کنید یه سری داده سه بعدی داریم که خاصیت سری زمانی یا ویدیویی دارن و میخوایم وارد unet کنیم اینا رو تا خروجی مورد نظر رو segment یا حتی reconstruct کنیم خب target ما t2 یا سگمنت تصویر نهایی و ورودی های ما t2 و t1 و t و t1 هستن اما خب unet فقط دو ورودی میگیره و قصد دارم خاصیت temporal بدم بهش توی مقالات خیلی گنگ گفتن اینو که بشه با rnn ترکیبش کرد به نظرتون چه راه هایی پیش روم هست منظورم اینه که چه ترکیبی باید شکل بگیره ممنون میشم حتی در حد ایده راهنمایی کنید اگه آشنایی یا مقاله ای دیده باشید ممنون
-> 
-> خیلی خیلی خیلی ممنونم واقعا عالی فقط یک سوال استاد به نظرتون استفاده از rnn ها و ترکیبش با unet امکان پذیره همینطور بنده قصد دارم روی داده سه بعدی این کار رو بکنم و باهاشون کار کنم واسه همین توی بحث اضافه کردن کانال به عبارتی باید یه تصویر 3d رو بچسبونم و فقط یه کانال نیست که کار رو پیچیده تر میکنه
-> 
-> استاد واقعا نمیدونم چطور ازتون تشکر کنم هر سری خیلی چیزای خوب و کاربردی ازتون یاد میگیرم حتما با همین فرمون پیش میرم که فرمودید ممنون واقعا
-> خواهش میکنم انشالله به زودی وبیناری با موضوع استراتژی انجام پایان نامه یا پروژه برگزار میکنیم"
"-> سلام دوستان میخواستم بدونم کورس از متوسط تا پیشرفته و تخصصی یادگیری تقویتی میشناسید به بنده معرفی کنید ترجیحا به روز باشد و پرکتیکال همچنین بعد از کورس بینایی ماشین دکتر اشرفی که در سایت هست چه مسیر و دوره یا کتابی را برای ادامه راه و مطالعه در این حوزه معرفی میکنید
-> سلام
-> ببخشید اشتباه نوشته بودم منظورم یادگیری تقویتی بود به صورت حرفه ای
-> سلام و عرض ادب ما یک گروهی داشتیم که کتاب Grokking Deep Reinforcement learning رو به صورت کامل خوندیم و فیلم هاش ضبط شده به جز دو فصل اول که فصل های مقدماتی بودن می تونید فعلا از همون فیلم ها استفاده کنید برای یادگیری در ادامه قرار هست هر هفته ارایه داشته باشیم در خصوص مقالات جدید RL و به صورت نه چندان پیوسته ارائه مقالات هم انجام می شه و در خصوص آشنایی با Unity و کد نویسی هم افراد مطالبی رو ارائه می کنن ولی از اون جایی که افراد گرفتار هستن معمولا به صورت منظم نیست ارائه ها ولی هم چنان جلو میره اگر علاقمند بودید بفرمایید تا لینک کانال و گروه رو ارسال کنم خدمت شما
-> سلاموقت بخیر چطور میشه به محتوای اون گروه یا فیلم ها دسترسی داشت
-> سلام وقتتون بخیر چه طور میتونیم به محتوای اموزش گروه و فیلم هایی که فرمودین دسترسی داشته باشیم"
"-> سلام استاد وقت بخیر پروژه کارشناسی ارشد من سمنتیک سگمنتیشن تصاویر mri هست و از شبکه های گن باید استفاده کنم با کانولوشن 3D استادم یه gpu 4070 12GB گرفته ولی مطمئن نیستم که جوابگو شبکه های مدرن باشه اگه با کمبود مواجه بشم باید حتما کولب پرو پلاس تهیه کنم یا راه دیگه ای هست
-> توی این ویس یکسری نکات ریز برای مدیریت پروژه با جیپییو گفتم شاید بدرد سایر دوستان هم بخوره
-> ممنون بابت توضیحات کامل بله دوره بینایی هستم ولی فعلا دیپ تموم نشده چکش نکردم من حدود ۴۰۰ تا سابجکت دارم برای هر سابجکت ۴ تا تصویر224224 150 دارم که اگه اینا stack کنم یه حجم وحشتناکی میشه"
"-> سلام دوستان یە سوال داشتم اگر یک شبکە عصبی رو برای یک دیتای یکسان چندین بار اجرا کنیم ممکنە از RUN های قبلی اطلاعات رو ذخیرە کردە باشە اگر بلە راەحل چیست
-> سلام بله دیگ هر سری شما ران میگیری وزنای تصادفیه اولیه فرق دارن اگر رندوم سید ست نکرده باشید باهم در نتیجه خروجیات متفاوت میشه تا حدودی میتونی وزنای مدل رو سیو کنی بعد از تموم شدن هر ترین ک همشو داشته باشی حالا بعدش هرکاری میخای کنیید باهاشون
-> منظورم اینکە ممکنە وزن ها جدا از سید نگرفتن تحت تاثیر ران های قبلی تغیر کنەبهینە بشن"
"-> سلام کسی تجربه کار در حوزه deep optical flow رو داره
-> سلام من یه مقدار با RAFT کار کردم RAFT Recurrent AllPairs Field Transforms for Optical Flow"
"-> سلام دوستان کسی تا به حال دیفیوژن مدل ها رو روی یه دیتاست با تصاویر زیاد و پیچیده ترین گذاشته من یه سرچ کردم یه چیزی حدود ۸۰ ساعت برای mnist ترین گذاشته بودن انگار یعنی انقدر مکانیزم سنگینی دارن
-> دیفیوژن مدلها ساختار خیلی سمیای دارن توی این تصویر پروسهای وجود داره که از نویز مرحله به مرحله به اون s رسیدیم برای تک تک این مرحلهها یک شبکه مثلا یونت باید اجرا بشه تا بتونه کم کم اون s ساخته بشه مثلا صد بار شبکه یونت باید برای یک نمونه تصویر اجرا بشه اینجوریه که 80 ساعت طول میکشه
-> پس درست متوجه شدم من فعلا قسمت ادینگ نویزش رو پیاده سازی کردم از روی مقاله اصلی تا همینجا هم کار هم جالب بود هم دشوار به نظرم شخصا علاقه زیادی به این مدل ها دارم ولی خب با این که یادگیریش خارج از لطف نیست ولی با توجه به این غول آسا بودن ریسورسی که نیازه پیشنهاد میکنید ادامه بدمش
-> خانم lilian weng با دانش خفن هم این رو گفتن It is very slow to generate a sample from DDPM by following the Markov chain of the reverse diffusion process as can be up to one or a few thousand steps One data point from Song et al 2020 For example it takes around 20 hours to sample 50k images of size 32 32 from a DDPM but less than a minute to do so from a GAN on an Nvidia 2080 Ti GPU این سورس عالی ولی سنگینه آقای جی هم که کارش ویژوالایز کردن هست
-> مثل همیشه بسیار عالی ممنون آقای دکتر
-> یونتهای متعدد
-> سلامبا دیفیوژن تست کرده بودم یادم نیست چه دیتا ستی بود اما اینقد طول نکشیده بود"
"-> سلام وقت بخیر آخرین و بروز ترین مقالات در مورد موضوعات هوش مصنوعی به جز سایت paperswith code در کدام سایت منتشر میشه همچنین موضوع vqa موضوع بروز و جدیدی هست وجای کار داره برای نوشتن مقاله
-> سلام سایتهایی مثل IEEE science Direct و کنفرانسها مقالات بروز رو میذارن سایت زیر هم شاید خوب باشه ولی به بزرگی paperswithcodecom نیست در مورد VQA اطلاعی ندارم اگه دنبال موضوعی برای نوشتن مقاله هستید پس در اولین قدم باید خودتون با تحقیق ببینید که زمینه مورد علاقتون جای کار داره یا نه تحقیق از لحظه انتخاب موضوع شروع میشه
-> ممنونممنون میشم اگه راهنمایی داشته باشید که چه موضوعاتی در حال حاضر به روز و قابلیت نوشتن مقاله هم در زمینه هوش مصنوعی وجود داره
-> 
-> این ویدئوها رو ببینید
-> دوستان اگه درگیر انتخاب موضوع هستید این دو تا ویدئو تا حدی میتونه کمکتون کنه اگه سوالی در راستای ویدئوها داشتید بپرسید"
"-> سلام وقت بخیر ببخشید دوستان در مسائل چند کلاسه و طبقه بندی به کمک CNN توی پایتورچ از چه تابع اتلافی استفاده میشه در یک کد نمونه که براساس کراس بود از categorical_cross_entropy استفاده کرده ولی معادل اون توی پایتورچ چی هست ممنون
-> سلام torchnnCrossEntropyLoss
-> خیلی ممنونم"
"-> سلام اگر مدلی داشته باشیم که با دادههای آموزشی بازه خاصی مثلا 10 تا 20 آموزش دیده باشه منطقیه پیشبینی اون در بازه ای غیر بازه آموزش دیده باشه
-> 
-> ممنونم آقای دکتر از جواب کاملتون"
"-> اینم الان دیدم دقیق مثل هم هستن اما روش اول از نظر حافظه مصرفی ظاهرا مناسب تره
-> یه نکته جالبی که اینجا استفاده کرده ولی اسمشو نیاورده همون اصل سوپر پوزیشن هست که توی سیگنال سیستم استفاده میکردیم بجای اینکه بیایم یه ورودی پیچیده بدیم به سیستم پراسس کنه خوردش میکنیم به اجزای کوچیکتر و ساده تر و بعد هرکدومو تحلیل میکنیم و در نهایت خروجی هارو جمع میکنیم برای همین گفته شده روش دوم همون لینک بهتره چون احتمالا علاوه بر اینکه مموری کمتری استفاده میکنه شاید یکمم سریعتر باشه مسئله خورد شده
-> من اصلا نشنیده بودم این رو یعنی اصلا درس سیگنال نداشتم ولی پاسختون خیلی دقیق بود مشتق هر ترکیب خطی از توابع برابره با همون ترکیب خطی از مشتقات توابع"
"-> دوستان سلام من یه مدلی دارم که تابع اتلافش شامل دو تا ترم متفاوته و لاس مدل درواقع حاصل جمع این دو تا ترم هستش حالا سوالم اینه که توی پایتورچ اگر من برای هرکدوم جداگانه backward کنم فرقی داره با اینکه این دوتا رو با هم جمع کنم و بعد روی کلش backward بزنم ممنون میشم راهنمایی کنید
-> سلام از نظر ریاضیاتی که فرقی نداره روی کاغذ چون جمع و تفریق اینا عملیات خطی هستند و کار خاصی نمیکنند یه تابع دو بخشی بنویسید روی کاغذ مشتق بگیرید یبارم جداشون کنید مشتق بگیرید جوابا یکی میشه
-> چه مسلط لذت بردم
-> خیلی ممنون"
"-> سلام اقای اشرفی یک سوال از دوره بخش بازگشتی دارم x_trainshape y_trainshape torchSize7352 128 9 torchSize7352 در این قسمت ما 7352 جمله داریم که 128 تا کلمه داره و 9 تا فیچر داره من چطوری می تونم جمله های من بصورت زیر هستند همه جمله ها منحصر به فرد هستند و طول ثابت دارند CGAGGTCTCAAACGCGCTGGAAGAA هر کرکتر بعنوان یک کلمه باید در نظر بگیریم پس 25 تا کلمه میشه چطوری من 9 تا فیچر ازش استخراج کنم به نظر خودم باید بیام از توکنایزر استفاده کنم و هر کرکتر به بردار تبدیل کنم که یک بردار 28 تایی میشه و برای 9 تا فیچر بیام بازههای مختلف در نظر بگیرم ولی نمی دونم چطوری باید این را پیاده سازی کنم میشه راهنمایی ام کنید
-> من با حوزه کاری شما آشنا نیستم و با این دادهها کار نکردم حدسم این هست که شما برای هر کلمه یک بردار ویژگی شبیه امبدینگ بسازید یعنی هر بار که A بود یک بردار ویژگی به طول n بهش تخصیص داده بشه
-> من توالی را به توکنایزر dnabert دادم یک بردار 768 تایی گرفتم تعداد توالی های من58421هست و تعداد ویژگی های استخراج شده از توکنایزر 768 برای هر توالی یک مقدارعددی بعنوان لیبل داریم حالا با این تعییرات روی توالی من یک دیتاست دارم 58421768 که یک لیبل دارند قبل ار دیدن دوره شما من این داده ها را دادم به مدل mlp مقدار لاس من خیلی کم بود زیر صفر بود ولی معیاری که مقاله بیس معرفی کرده بود معیار اسپیرمن و پیرسون بود من نمی تونم این دو معیار را بهبود بدهم به نظر شما چرا نویسنده مقاله این دو معیار را برای ارزیابی مدل انتخاب کرد الان دوره را کامل دیدم دوباره مدلهایم را با اطلاعات دوره شما ادیت می کنم ولی هنوز برایم این سوال هست چرا معیار پیرسون و اسپیرمن را بعنوان معیار ارزیابی قرار داده امکانش هست در این مورد راهنمایی ام کنید
-> نمیدونم
-> خود مقاله توالی را one hot کرده و داده به مدل کد مدل را نگذاشته ولی مدل ترین شده را گذاشته من چطوری خودم معیارهای دیگه را بدست بیارم و مدل خودم را با اون معیارها مقایسه کنم از ابتدا خودم مدل با توجه به معماری که گفته پیاده سازی کنم
-> دوست عزیز دایره کاربرد یادگیری ماشین به قدری وسیع شده که گاهی آپدیت بودن توی یک شاخه خاص هم برای ادما سخت شده روز به روز مدل ها و معماری های مختلف ارایه میشه کار پژوهشی اینجور نیست که شما ایده و مدل خودتون رو بسازی و حالا بعدش دنبال بیسلاین براش بگردی اتفاقا برعکس ما ابتدا روی یک مدل خاص متمرکز میشیم و خوب مطالعه ش میکنیم روی پیاده سازیش عمیق میشیم و بعدش ایده های خودمونو برای بهبودش اعمال میکنیم حالا شما با مدلی مواجه هستی که کدش رو نداری اونهم در زمینه بیوانفورماتیک توصیه من این هست که اگر تازه کارید مدلی رو انتخاب کنید که کد خوانا و تمیزی داشته باشه ما دوست داریم کمک کنیم منتها در مورد مساله و داده هاش شناختی نداریم
-> متاسفانه اصلا نمی تونم مقاله را عوض کنم مقاله ام معتبر هست و داده ها هم درست هستند به اجبار باید راه حلی پیدا کنم خیلی زمان گذاشتم ولی تا حالا موفق نبودم
-> این سوال ها رو از چت جی پی تی هم پرسیدید اون خیلی جواب های غیرمنتظره و مفیدی میده
-> اگر مقاله معتبر باشه معمولا کدهاشو تو گیت هاب به اشتراک میذاره
-> مقاله نیچر هست و یک کار آزمایشگاهی بسیار زیادی انجام شده و حدود 200 هزارتا توالی را ازمایش کردند و این تعداد داده در حوزه بیوانفورماتیک بسیار زیاد و ارزشمند هست نظر چت چی بی تی این هست من روی معیار اسپیرمن و پیرسون کار کنم و با این معیار مقایسه کنم ولی چون داده های من زیستی هستند و باید طوری با اون برخورد بشه که منطق زیستی را هم در نظر گرفته بشه نمی تونه این بخش را درست بهم راهنمایی کنه و معمولا باگ داره تعداد توالی زیاد هست به طول 25 وقتی من بردار می گیرم میشه 7682558421 این حجم داده من دچار مشکل رم میشم کد را تغییر دادم بردار 76858421 گرفتم و به GRU را دادم ولی نتونستم نتیجه خوبی بگیرم الان که دارم ویدیو ها می بینم احتمال میدم انشالله در کدها من اشتباه کردم و این بخش را درست کنم شاید درست بشه
-> چرا باید 58 هزار داده رو به یکباره به مدل بدید مینی بچ نمیشه ساخت
-> من بچ کردم دادم به مدل ولی وقتی دارم بردار می گیرم دچار مشکل رم می شدم اومدم تیکه تیکه بردار گرفتم ولی باز این حجم خیلی زیاد بود 7682558421 برای همین این طور بردار گرفتن را بیخیال شدم با کمک چت چی بی تی همون 76858421 را به مدل GRU دادم الان که دارم ویدئوها را می بینم احتمالا من اشتباه کردم"
"-> سلام من دارم روی fall detection کار میکنم دیتاستم مجموعه ای از ویدیوهاست که لیبل ها به صورت 11331205345 که عدد اول شماره فریم عدم دوم شماره کلاس و بقیه مختصات bounding box عه استاد ترین کردن این دیتاست جزو چه دسته ای حساب میشه object detection یا activity recognition چون علاوه بر شماره کلاس شماره فریم هم داره چون دیتا ویدیوعه
-> سلام اگه قراره لحظه شروع اون حالت رو دیتکت کنید فکر کنم باید برید سراغ action Detection"
"-> سلام استاد وقتتون بخیر میشه چندتا سورس خوب برای مالتی تسک لرنینگ معرفی کنید الگوریتم های مختلف که استفاده میکنن و ممنونم
-> سلام به نظرم شما باید از یک نفر برای کارتون مشاوره بگیرید
-> کسی رو سراغ دارین اتفاقا خودم خیلی مایلم اما متاسفانه کسی که دانش کافی داشته باشه نیست
-> ما در هوسم مشاوره داریم میتونید موضوع رو با توضیح مناسب ایمیل کنید تا بررسی بشه پیام قبلی من در مورد مشاوره تبلیغاتی نبود برداشتم این بود که شما مثل خیلی افراد دیگه نیاز به مشاورههای ساعتی دارید کلا بیشتر از مشاوره ساعتی با افراد باتجربه و بروز استفاده کنید1 حرف مشاورها وحی منزل نیست ولی میتونه خیلی بهتون دید بده و ذهنتون رو باز کنه
-> بله ممنون چقدر خوب حتما فقط امکان مشاوره با خودتون نیست استاد
-> این موارد رو ایمیلی پیگیری کنید
-> ممنونم"
"-> سلام فقط خواستم تشکر کنم بابت پکیج های آموزشی هوسم هیچ پکیجی به کاملی هوسم توی حوزه بینایی کامپیوتر به زبان فارسی من پیدا نکردم خیلی تو انجام پروژه دکترای من مفید بوده تاحالا فقط خواستم تشکر کنم و پیشنهاد کنم به کسانی که تو این حوزه کار میکنن حتما خریداری کنن
-> سلام خیلی خوشحال شدم و ممنونم موفق و سلامت باشید"
"-> با پایتورچش رو میدونید
-> فک کنم یه سرچ کوچک کنید نمونه کد پیدا بشه الان تو ذهنم نیست باز استاد اگه میتونن راهنمایی کنن
-> شاید این کد کمکتون کنه"
"-> سلام دوستان کسی توی زمینه شبکه عصبی عمیق کار کرده
-> اینجا همه عمیق کارن"
"-> سلام استاد آیا شما دوره دیپ لرنینگ برگزار میکنید
-> سلام دوره دیپ 2022 توی سایت هست فعلا برگزار نمیکنیم چطور
-> کلاس آنلاین ندارین
-> خیر به احتمال خیلی زیاد کلا دیگه کلاس آنلاین برگزار نمیکنیم دورههای اخیر رو آفلاین جلو میبریم و در ابعاد مختلف آموزش با کیفیت بهتری حاصل میشه
-> سلام خسته نباشید بعد از دوره دیپ کاتالیست چه دوره ای در نظر گرفتین و چه زمانی برگزار خواهد شد"
"-> صبح بخیر استاد من چهارتا کلاس دارم که کلاس اول تعدادش ۶۰۰۰۰ هزارتاست و کلاس اخر ۲۰۰۰ تا چجوری میتونم بالانسش کنم و مطمئن هم باشم دیتاstratified هست برای ترین لازمه دیتام بالانس باشه و برای تست لازم نیست
-> Smote"
"-> سلام وقت بخیر آیا سایت یا نرم افزاری برای ساخت دیتای اینستنس سگمنتیشن میشناسین
-> Cvat
-> و اینکه قبلا یه فایل برای ماسک ها تولید میشد یه فایل json هم برای باندینگ باکس ها ولی مثل اینکه جدیدا فقط فایل جیسون میدن بیرون درسته
-> نمیدونم جدیدا کار نکردم"
"-> سلام دوستان وقتتون بخیر یک سوال داشتم برای مصاحبه دکتری آیا داشتن مقاله کنفرانسی داخلی شامل نمره میشه من یک مقاله کنفرانسی بین المللی داخلی فرستادم ولی پشیمونم چون میگن ۰۵ نمره بیشتر نداره
-> سلام من نمیدونم ولی میدونم که داشتن مقاله حتی کنفرانس داخلی خوبه"
"-> 
-> 
-> "
"-> سلام دوستان یک سوال داشتم از کجا میشه مسائل به روز کامپیوتر ویژن رو پیدا کرد برای انتخاب موضوع مقاله منظورم هست یعنی نمیدونم چه حوزه و موضوعی رو برای کار کردن انتخاب کنم که اشباع نشده باشه و کاربردی هم باشه
-> 
-> ممنونم از توضیحات تون خیلی ارزشمند بود خیلی لطف کردید حتما سعی می کنم به مواردی که فرمودید توجه کنم
-> سلام واقعا کاربردی بود"
"-> اخیرا گوگل کولب پرو برای دوره دیپ کاتالیست تهیه کرده بودیم یکسری تجربه از کار با این سرویس پولی کسب کردیم که در اینستا گذاشتم و اینجا هم میذارم 1 سه نوع gpu در پرو با نامهای V100 T4 و A100 وجود داره ضعیفترین T4 و قویترین A100 هست یک بار درخواست A100 دادم اما نداد بهنظر میرسه که این gpu رو سختتر میده 2 هر سرویس پرو شامل 100 واحد محاسباتی شارژ هست با هر بار اتصال به یک ماشین مجازی این شارژ مصرف میشه 3 هرکدوم از دیوایسها یک نرخ تقریبا ثابت مصرف شارژواحدمحاسباتی دارن مثلا T4 به ازای هرساعت 2 واحد محاسباتی مصرف میکنه V100 قدرتمند هم حدود 5 واحد محاسباتی مصرف میکنه من به A100 وصل نشدم اما فکر کنم میزان مصرف بین 7 الی 8 باشه نتیجه اینکه با T4 حداکثر 50 ساعت کولب پرو و با V100 حداکتر 20 ساعت واحد محاسباتی دارید 4 هزینه کولب پرو هم 10 دلار هست فکر کنم برای خارج کشور 10 دلار برای 50 ساعت gpu نسبتا به صرفه باشه کولب پرو رو میتونید از ایرانیکارت تهیه کنید اگه شما هم تجربهای در استفاده از کولب پرو کسب کردید لطفا با ما و سایر دوستان به اشتراک بذارید
-> سلام یک تجربه ای دارم خودم با حالت لوکال کسب میکنم یک gpu سری 3060ti دارم میخوام با رابط رایزر یکی دیگه 3060ti روی سیستم بالا بیارم فکر کنم با این کار کارایی تقریبا دو برابر بشه حداقلش حافظه gpu دو برابر میشه و برای بچ سایزهای بزرگ میشه هم از قابلیت دو gpu استفاده کرد و هم از حافظه بیشتر
-> دو جی پی یوی 3060ti خوبه"
"-> خود بینگ چت جوابش اینه
-> بینگ مگه به اینترنت وصل نیست یادم قبلا سه حالت داشت
-> برای من هم سواله ک چرا داره خودش ضعیفتر عمل میکنه یکی از دلایلش توی بکند میتوته دیتای کش شده و دیتابیس مجزا برای این کار باشه که بروز نشده و توی بینگ به نظر میرسه مدل از قبل دیتاهاش خزش و گرداوری میشه و بعدش ترین میشه و دیپلوی میشه ولی openai از مدل های میانی یا میدل مدل داره استفاده میکنه کلا کاملا یک مدل مجزایی هست اول ریکوئست میزنه ب کرنل بینگ و از توی نتایج انلاین پردازش میکنه در حقیقت این مدل روی کلید واژه ها و کلمات کلیدی داره موفق عمل میکنه نتایج گوگل هم به صورت انگلیسی هم بهترین نتایج رو بهت پیشنهاد میده ولی توی فارسی ضعف داره"
"-> openai
-> امروز 3 اکتبر هست دیروز نسخه جدید پایتون ریلیز شده"
"-> ببخشید کسی اکانت gpt4 خریده قیمتش در اروپا و آمریکا متفاوت است می خواستم بدانم اگر از اکانت را با IP آمریکا بخریم و از vpn با IP آمریکا استفاده کنیم آیا اکانتمان غیرفعال میشود
-> درود برای IP موردی نداره مگه قیمت امریکا در پرداخت نهایی چقدر میشه
-> قیمت آمریکا 20 دلار و قیمت اروپا 24 دلار است
-> ببخشید شما تجربه مشابه دارید
-> من خودم نسخه 4 کار میکنم خرید اروپا ایرانم بچه های دیگه هم خرید از اسپانیا داشته رفته امریکا اومد ایران اوکی بوده و موارد دگه توی ای پی مشکلی نداره
-> دقت جواب gpt4 ایا تغییر زیادی نسبت به 35 داره نسخه رایگان فعلیش جواب اشتباه زیاد میده
-> اینکه جواب اشتباه زیاد میده به نوع و نحوه ی سوال هم بستگی داره نسخه رایگان که خب یکسری چیزها محدود هستید چه توی ابزار چه توی API در نسخه 4 حالت محتوای محیطی و سرفیس بروز شده و از کرنل بینگ برای جستجو استفاده میکنه و از خود بینگ چت هم قویتره جالبست در بخش تحلیل دیتاست و تصویر رو میدی بهش و تحلیل میکنی گام ب گام باهاش میری جلو اما در حالت عادی اگر کاری مربوط و طراحی سرویس و API یا دید بیزنسی ندارید و فقط میخاید برای یکسری سوالات ازش بپرسید و همچنین نسخه 4 رو داشته باشید سوالاتتون رو از Bing بپرسید
-> اره بینگ حداقل رفرنس میده و تا حدودی مطمینی جواب درسته
-> الان اوپن ای دقیقتر شده با کرنل بینگ
-> عکس هارو فرستادم بالا چک کن
-> Bing به شدت ضعیفه من دو الی سه تا سوال ازش کردم یکیش مثلا این بود جدید ترین الگوریتم تو مسیر یابی که به صورت مثلا یه موردی بود نمی تونست جواب بده اولین الگوریتم را آورد و جالبیش اینجاست وقتی بگی اشتباه گفتی میگه بهتره این بحث را همینجا خاتمه بدید
-> قدرت حل مسئله اش توی مباحث طراحی الگوریتم کلا توی دیواره من چند تا چالش تئوری باهاش جلو رفتم بدون کد تهش دوباره رفتم سراغ openai فقط اگر با کد باهاش جلو بری کمی کمک میکنه بازم نصف راه میره جاده خاکی اما اگر مسئله اسون باشه کارت راه میوفته
-> برای کارهای عمومی الان بینگ نسبت ب قبل بهتر شده کار راه اندازه
-> دقیقا همینطور هست ولی بارد گوگل تو مسائل تئوری الگوریتم خوبه ولی تو برنامه نویسی یکم مشکل دارد امتحان بکنید
-> بارد رو هم امتحان کردم ولی بیشتر کارم چند تا زبان باهم هست برای دیپلوی فقط openai کارم راه میوفته یعنی سرعت میده ب کارم و ایده پردازی های خفنی ارائه میده بعدش میشینه باهات فکر میکنه مودبانه پیشنهاد میده
-> همفکریش خیلی خوبه"
"-> سلام دوستان من دارم رو یه موضوع شبکه عصبی و بینایی ماشین با پایتون کار میکنم کسی هست تو این زمینه بتونم ازش راهنمایی بگیرم
-> سلام لطف میکنید در مورد موضوع تون بیشتر توضیح بدید"
"-> فقط جسارتا در رگرسیون غیرخطی چند جمله ای با 6 فیچر میشه از polynominal درجه یک استفاده کرد من درجه ۲ رو هم تست کردم اختلاف دقت چندان زیادی نداشتن با هم
-> سلام بله میشه"
"-> سلام وقت شما بخیر عذرخواهی می کنم در مقاله ای به این اطلاعات برخوردم گفته شده از mlp استفاده شده آیا با mlp مثل ردیف آخر این جدول میشه برای x های ورودی میانگین وزنی گرفت موضوع دوم اینکه می توان با random forest مثل رگرسیون خطی وزن های x های ورودی رو شناسایی کرد با تشکر فراوان
-> سلام من سوالتون رو خوب متوجه نشدم همه اون معیارها رو میتونیم برای mlp حساب کنیم رندوم فارست وزن نداره یکسری درخت داره که از تجمیع اونها تصمیمگیرب انجام میشه
-> ممنونم از شما من در mlp دنبال وزن های x های ورودی هستم ولی در mlp به دلیل اینکه از لایه ها استفاده میشه عملا در طول پردازش وزن های مختلفي رو پیدا می کنند و چون هدف تعیین وزن x های ورودی هست در این خصوص دچار چالش شدم مگر اینکه راهی وجود داشته باشه که میانگین وزنی برای x های ورودی گرفته بشه
-> من متوجه منظورتون نمیشم وزنهای ورودی نمیدونم یعنی چی ویس بفرستید
-> آقای دکتر فیلم های آموزشی رو مجدد چک کردم مشکل در حال برطرف شدن هست"
"-> همين كارو با دستور torchnnunfold هم ميتونم انجام بدم
-> داداش پیاما رو یکی کن این شکلی خرابکاری داره میشه"
"-> يعني بعد 2 و 1 رو جابجا كنم خروجي كه بعد انفولد بهم ميده دقيقا خروجي موردنظرمه منتهي بعد fold همون تنسور اول رو بهم نميده
-> انفولد میاد یه بعد دیگه در راستای توالی و قدمی که میخای ایجاد میکنه یعنی اگه داده هات ۱۰۰ تا باشه و بیای توالی ۵ رو بدی با قدم ۵ میاد یه تنسور با شیپ ۲۰ و ۵ بهت میده
-> من الان مثالم رو ساده تر كردم ارسالش ميكنم اگر ممكنه روي اون تصوير بهم توضيح بدين"
"-> به جاي دستور torchTensorunfold از دستور torchnnunfold استفاده كنم
-> Permute کن توی انفولد نیازه که اینکارو بکنی"
"-> من یه تنسور با ابعاد 4096 دارم میخوام این تنسور رو به chunkهایی با طول 200 با Overlapعه مثلا 50درصد تبدیل کنم و بعد تنسور حاصل رو به همون حالت قبل برگردونم با چه دستورهایی توی pytorch میتونم این کارو انجام بدم
-> 
-> سلام وقت بخير من از دستور torchTensorunfold استفاده كردم قسمت اول رو به خوبي انجام ميداد اما دستوري براي fold كردن نداشت و مجبور شدم از torchnnfunctionalfold استفاده كنم كه همون تنسور اوليرو بهم نداد"
"-> سلام من یه مشکل توی دیتا و فرمتشون دارم حجمش زیاده وقتی میدم به language model و امبد میکنم نمیتونم csv ذخیره کنم چون مشکل تو تبدیل string پیش میاد و pkl هم حجمش زیاده بعدم میخوام این دیتارو مدل کنم و ensemble و ممنون میشم راهنمایی کنید چه فرمتی ذخیره کنم بعد از امبد شدن
-> سلام حجمش چقدره
-> ۱۰ گیگ هست که بعدا توی استکینگ مثلا میشه ۵ تا ۱۰ گیگ نمیدونم ذخیره به نامپای کاره درستیه بعد از اونور باز مشکل تبدیل به رشته و استرینگ و اینارو دارم
-> من توی این زمینه تخصص و تجربه ندارم صرفا جهت همفکری و یاد گرفتنم سوال میپرسم نمیشه در فایلهای تکهتکه ده فایل 1 گیگ ذخیره کرد ده گیگ باید توی رم کامیپوتر مدیریت بشه که فکر کنم حجم زیادی هست
-> ممنونم استاد دارم همین کارو میکنمموقع استک یکم اذیت میکنه ببینم چجوریه بازم ممنونم ازتون
-> بعد استاد من کامل فایل آموزش رو ندیدم مالتی تسکینگ رو هم داریم تو دوره ماشین یا دیپ ممنونم ببخشید اگه سوالم ممکنه اینجا جاش نباشه
-> مولتی تسک لرنینگ رو توی فصل 9 یادگیری عمیق گفتیم
-> ممنونم
-> سلام استاد وقتتون بخیر لاس فانکشن خاصی باید برای مالتی تسکینگ استفاده کنیم هنوز فرصت نکردم ویدیوهارو ببینم
-> 
-> ممنونم ازتون"
"-> سلاموقت بخیر کسی میدونه مشکل از کجاست
-> درود امیر جان فکر کنم تهش باید برسی به pip uninstall matplotlib pip install matplotlib
-> ممنون از راهنماییتون تست کردم ولی متاسفانه اوکی نشد
-> روی همین env روی base انجام دادی و نشد
-> اره روی همین env"
"-> سلام وقت بخیر یکی از داور های مقالم گفته باید تو مقاله اثبات کنی شبکه عصبی حتما همگرا میشه توی متن مقاله نتایج نهایی و نتایج شبیه سازی آورده شده چه باید بکنم
-> 
-> خیلی ممنونم بابت توضیحات من هم مشابه شما فکر میکنم مقاله من درباره هدایت یو ای وی هاس که گین های یک الگوریتم کلاسیک رو ابتدا با الگوریتم ژنتیک پیدا میکنیم بعد بدلیل عدم قطعیت هاچون نمونه اولیه ازش ساخته شده با جی دی بصورت بک پرو اپدیت میشن توی متن مقاله پایداری سیستم با لیاپانوف اثبات شده توی بخش دوم یک ام ال پی داریم که کار سنسور فیوژن رو انجام میده شبیه سازی ها که اثبات میکنن سیستم خیلی خوب کار میکنه
-> حالا برای پارت اول میگه اثبات ریاضیاتی کن که شبکه عصبی همگرا میشه
-> راستش فقط خط اول رو فهمیدم به هر صورت موفق باشید و لطفا پیامهاتون رو در یک یا دو پیام بفرستید
-> سلام شاید این قضیه تقریب جهانی منسوب به سایبنکو ۱۹۸۹ جواب این سوال همگرایی باشه سایبنکو اثبات کرده است که یک شبکه پیشخورد چندلایه با یک لایه پنهان که متشکل از تعداد مناسبی نورون باشد با تابع فعالساز سیگموئید میتواند هر نوع تابع پیوسته را تخمین بزند البته ۲ سال بعدترش هم ثابت شد برای سایر توابع فعالساز هم این قضیه برقراره
-> خیلی ممنونم رفرنس هاشو میشه لطفا برام بفرستید"
"-> سلام دوستان وقتتون بخیر باشه من یه سوال داشتم ممنون میشم راهنماییم بفرمایید ما همیشه در مدل های یادگیری عمیق دنبال این بودیم که لاس رو بیاریم پایین برای اینکه به حالت بهینه برسیم اما این موضوع در شبکه های گن به دلیل رقابت این دو شبکه متفاوت هستش کلا بهینه ترین حالت برای نمودار لاس برای شبکه های گن چه طوری هستش من جاهای مختلف موارد مختلف که خوندم و الان کمی گیج شدم که لاس شبکه مولد و تفکیک دهنده باید جفتشون پایین بیاد تا بگیم شبکه بهتر عمل کرده یا اینکه عکس هم عمل کنند مثلا اگه مولد لاس بیشتری داشته باشه و تفکیک دهنده لاس کمتری داشته باشه بهتر بودن رو میرسونه
-> "
"-> سلام دوستان من توی نصب کتابخانه hazm مشکل دارم کسی کتابخانه بهتری برای tokenize کردن کلمات فارسی سراغ داره
-> سلام چه مشکلی دارید هضم خوب و سرراسته"
"-> سلام تفاوت argmax و multinomial چی هست خیلی جاها میبینم از multinomal استفاده میکنن و جواب خوبی میگیرن اما من وقتی میزنم خیلی ضعیف تر از argmax جواب میده و اینکه softmax برای استفاده از multinomial نیازه
-> "
"-> سلام یه سوال دارم تو شبکه های بازگشتی چون انتظارشون batchlf هست زمانی که ما توی تسک nlp بخوایم embedding استفاده کنیم یه بعد دیگه اضافه میشه و داده ی ما ۴ بعدی میشه اون موقع باید چیکار کنیم میشه reshape کرد به حالتی که b lf embedding راه حلی اگه کسی داره بگه ممنون
-> با توجه به صورت مساله شما باید دیده بشه بله راه داره اما ما الان در دوره دیپ کاتالیست هوسم به بهترین نحو درگیر این موضوع و حل چالش های اون بودیم و توصیه میکنم شرکت کنید پروژه یک NLP بود
-> شرکت کردم گذاشتم تموم شه یهویی ببینم ولی مشکل اینجاست اگه بخام فلت کنم پدینگ ها مشکل درست نمیکنن و یا تگ sos و eos مثال هر sequence چنتا فیچر داره و خیلیاش پدینگ هست بخام فلت کنم مثلا یه بردار ایجاد میشه که قسمت پدش تو ایندکس های مختلف متفاوته خب
-> سلام توی دوره دیپ فصل nlp اینها رو گفتیم فکر کنم هم امبدینگ داریم و هم با پد سیکوئنس سمپلها رو یکسان کردیم اگه از امبدینگ استفاده کنید بعد چهارم اضافه نمیشه امبدینگ همون f میشه آقا شما وب و نرم افزار بلدید زودتر شروع میکردید چند تا کار خلاقانه توی دیپ کاتالیست میزدید اتفاقا جای امثال شما خالیه"
"-> سلام استاد امکان شیر کردن فایل پاورپوینت یکی از فصل ها هست عکس خام یکی از اسلاید ها رو لازم دارم
-> "
"-> ۱۲ ثانیه تو لپتاپ ۷ ثانیه توی سیستم
-> تازه ديدم لپتاپ زياد جالب نيس برا ترين كردن
-> کلا لپتاپ برای ترین خوب نیست گرمای جیپییو خیلی زیاده و لپتاپ فضای مناسب براش رو نداره از لپتاپ برای کد زدن و اجراها و ترینهای کوچیک در حد تست و بررسی میشه استفاده کرد قدرت یک جیپییو روی لپتاپ و پیسی با مدل مشابه هم یکی نیست پیسی قویتر هست
-> حتي براش فن بزاريم جوابگو نيست
-> ترجیحا این کار رو نکنید GPU شما ممکنه لحیم سردی پیدا کنه و ریبال کردن اون دردسر داره و ممکنه جواب نده
-> کلا gpu های لپ تاپی از هم مدل دسکتاپی ضعیف تر هستند
-> اینم چک کن که خود لپتاپ یه سری مواقع برا مصرف برق کمتر میاد و قدرت کمتری بهت میده مثلا لپتاپ Asus که من دارم خودش سه نوع مصرف برق برای گرافیکش داره و هر کدوم مقدار خاصی قدرت داره
-> کلا یک مدل gpu برای دسکتاپ قدرت بالاتری داره در ضمن انتقال حرارت و cooling لپ تاپ ضعیف تره هیچ کس train با لپ تاپ رو پیشنهاد نمیده
-> سلام وقت بخیر من یه لپاتپ جدید asus tuf f 15 گرفتمبرای ماشین لرنینگ فنcpu به محض وبگردی و تماشای آنلاین ویدئو هر ۵ دقیقه onوoffمیشه به صورت سیکلیک ۵ دقیقه بی صدا و دو دقیقه افزایش دور فن با صدای بلند بردم نمایندگی گارانتی محل سکونتم محل خرید ویندوز رو عوض کردن ایندفعه برعکس شده ۵ دقیقه دور فن زیاده با صدای بلند ۲ دقیقه آف و سایلنت وقتی نت خاموشه تماشای ویدئو بدون صدا هست و فن بیصداست این رفتار فن طبیعیه چون اصلاtask آورلودی نیست تماشای آنلاین ویدیو دمایcpu 40هست بهتره چند تا مدل ران بگیرم که فنgpu هم چک بشه یا بهتره ارجاع بوم گارانتی خودشون بررسی کنند نمدونم مشکلش چیه و در overload task عملکرد فن وgpuچطور خواهد بود ممنون میشم راهنمایی کنید اگه فردی لپتاپ مشابه گیمینگ asusداره
-> ترین و تست کوچک منظور حجم دیتای چقدر هست با چه گرافیکی
-> من asus zenbook دارم دقیقا همینطوریه
-> احیانا موقع کار روی فرش یا میز میگذارید راه خروجی هوا رو باز بگذارید حتما
-> سلام مشکل از لپ تاپتون هست من همین سری رو دارم و چنین مشکلی ندارم
-> نه یه استند چوبی شیبدار از روز اول گرفتم فاصله داره از سطح زمین و کلا مشبکه
-> به نظرتون بگم بفرستن نمایندگی تهران الان یا فردا خودم یه کم چک کنم و چند تا مدل ران بگیرم ببینم فنcpu gpu و کلا عملکرد gpu چطوره بعد ارسال کنم جهت بررسی
-> هیچکدام رو سیستم تون Armoury Crate رو باز کنید از اونجا کاملا به سرعت فن و مود های پردازنده و گرافیک دسترسی دارید با اون بررسی کنید ی بار برای من مشکل سیستم عاملی داشت وقتی پلاگین میکردم رو مود توربو گیر میکرد و بعد جدا کردن به نرمال بر نمیگشت با آپدیت ویندوز اوکی شد
-> باید با fn و دکمه تنظیم فن ک بالا هست تنظیمش کنی بزارش روی performance یا silent دیگه فنه خیلی بی صدا کار میکنه
-> میشه مدل کامل لپ تاپ رو بدین
-> Asus Tuf f15 gaming fx517zr و تجربه من برا لپتاپ ماشین لرنینگ هزینه نکنید اصلا همون گوگل کلاب و IBM بهتره
-> لپتاپ خوب برای یادگیری ماشین که حتما خوبه و برای یادگیری عمیق در فاز ریسرچ و پیادهسازی میتونه خوب باشه نهایتا برای ترین کردن مدل بهتر هست سرویس آنلاینی اجاره بشه
-> اینکه خوب بدیهیه بشرطی اینترنت دچار مشکل حاد نشه که حسابی نیست"
"-> سلام پایتورچ با کودا ۱۱۸ نصب کردم یه اتفاق جالبی نظرم رو جلب کرده وقتی مدل آموزش میبینه فقط از ۴۰ درصد gpu استفاده میشه حالا سوال اینه چرا بیشتر از ۴۰ درصد gpu زیر بار نمیره دلیلش میتونه سایز بچ باشه در ضمن سیستم گلوگاه نداره ممنون
-> مدل تکست هست تصویر یا صوت
-> تصویر
-> سلام 40 درصد رو از کجا دیدید
-> سلام نرم افزارهای آنالیز کارت گرافیک در اصل فقط ۴۰ درصد کودا کور ها زیر بار میره
-> در کل بخوام بگم مثال فصل cnn با دیتاست cifar10 رو آنالیز کردم از بچ سایز ۳۲ تا ۲۵۶ تست کردم زمان هر ایپاک از ۴۱ ثانیه برای بچ سایز ۳۲ بود تا زمان ۳۱ ثانیه برای بچ سایز ۲۵۶ اما در کل درصد استفاده از هسته های کودا بیشتر از ۳۸ الی ۴۰ درصد نمیشد
-> این مساله غیرطبیعی نیست یکسری فاکتور ممکن هست دخیل باشه مثلا بچ سایز که آزمایش کردید پیچیدگی مدل ممکن هست مدل بزرگ نباشه بخش داده و لود کردنش کلا اینطور فرض کنید که مدل و بچ شما یکسری محاسبات نیاز داره که جیپییو با یک حد مناسبی از ریسورس اون محاسبات رو پوشش میده و انجام میشه مثلا با دستورهای پایتورچ زمان اجرای یک بچ وقتی به مدل داده میشه رو حساب کنید یا مثلا شبکه رو بزرگتر کنید و نتیجه رو ببینید
-> ممنون یک نتیجه دیگری که جالب بود اینه که هر وقت سایز بچ رو بزرگتر میکردم زمان اجرای ایپاک کمتر و درصد استفاده از کودا به حدود ۲۵ میرسید پس طبق گفته شما ظرفیت مدل در حدی نیست که کودا کورها بخوان تماما درگیر بشن
-> بله من چند روز پیش برای دوره دیپ کاتالیست یک ترین با مدلی به شکل لایه امبدینگ lstmفولی کانکتد گذاشتم 99 درصد از کودا کورها درگیر بود
-> ممنون لطف کردید پس اگر gpu قوی تری برای همین مدل تستی من گذاشته بشه قرارنیست که زمان هر ایپاک با همان بچ سایز کمتر بشه
-> گرافیکت چیه
-> این رو مطمئن نیستم شاید با سختافزار قویتر همین مدل سبک هم زمانش کمتر بشه فاکتورهای زیادی دخیل هستن مثلا فرکانس کودا کورها
-> 3060ti
-> یه تجربهای داشتم که درست شد این بود که یه پکیجی داریم به اسم Protobuf وظیفهاش اینه که دیتاها رو سریالایز میکنه برای ورود به شبکه اگر ورژنش پایین باشه عمدتا سرعت کم میشه و جی پی یو هم کامل درگیر نمیشه ورژن این پکیج رو چک کن ببین قدیمی نباشه من یک بار این مشکل رو داشتم حل شد با ارتقا پکیج"
"-> سلام استاد عزیز دوره nlp درحال برگزاری است
-> سلام نه شروع نشده دوره دیپ کاتالیست رو تازه شروع کردیم
-> میتونیو تخمین بزنید که کی دوره nlp برگزار میشه
-> تخمینی حدودی فکر کنم اول زمستون"
"-> شما راه حلی براش دارید مقاله اولم رو هم تابستون ۹۹ نوشتم بهمن ۱۴۰۰ سابمیت کرد و مقاله سر مسخره بازی هاش اردیبهشت امسال چاپ شد
-> سلام چرا میگه سابمیت کردم درحالیکه انجام نداده میدونید دلیلش چیه اغلب دانشجوها خودشون کار سابمیت مقاله رو انجام میدن
-> خب دلیلش همین که بیشتر از یک سال دستش و فقط میخواد از شر من خلاص بشه تا حداقل یع مدت بهش زنگ نزنم نمیذاره خودم انجام بدم یبار انجام دادم وسطا گفت برو ریمو کن
-> آدم بشدت بی مسیولیتی یبار پایان نامه مارو نخوند مقاله هارو حتی یبار از رو نگاه نکرده تا الان دلیلشم این که خودم بعدا مرور کردم دیدم چقدر غلط های تایپی ناجور و تابلو وجود داشته
-> الان من در تعجم سابمیت مقاله کار سختی که نیست چند تا مرحله هست که یک نصفه روز وقت میبره چرا میخواید با ایشون بازهم مقاله بدید شما که دفاع کردید میتونید با کسی دیگه همراه بشید
-> اتفاقا خیلی خیلی مایلم با کس دیگه بنویسم از خیر ریکام ایشونم گذشتم اما چون نتایج آزمایشگاهی داره هیچ کس حاضر نیست بیاد تست های منو تایید کنه
-> درسته من شرایط شما رو نمیدونم استاد شما رو نمیشناسم و خیلی ملاحظات دیگه رو نمیدونم به همین خاطر نمیتونم پیشنهاد خوبی بدم بهتر هست به همون روش قبل به ایشون فشار بیارید بلکه بفرستن حتی اگه ژورنال کمی پایینتر از سطح مقاله باشه و تا حدودی خیال شما از پذیرفته شدن راحت باشه هم بد نیست چون کمتر نگرانی ریجکت شدن قدیمی شدن و داستان دوباره با استاد براتون پیش میاد اگه با استادتون صرفا تلفنی ایمیلی یا راه دور در ارتباط هستید پیشنهاد میکنم حضوری برید توی ایران اثر حضوری خیلی بیشتر از راه دور هست
-> درست خیلی ممنونم درست میفرمایید اما هزینه سفر واقعا سنگین میشه برام و زور هم داره واقعا نمیشه با شکایت به دانشکده یا دانشگاه پیش برد یا اصلا کلا دور استاد رو خط بکشم خودم یه تنهایی سابمیت کنم
-> به نظر من که کلا دور شکایت از استاد رو لااقل برای امور علمی کامل خط بکشید برای سوال دوم هم من جواب افرادی که گفتید رو میدم از کجا معلوم داور نگه که این نتایج آزمایشگاهی هست استاد راهنمات کجاست کجا این آزمایشها رو انجام دادی من نمیگم حتما این رو میپرسن ولی احتمال بدید که با این سوال مواجه بشید جوابی قانع کننده براش دارید به اونها نمیتونید بگید که استاد راهنمام منفعل بود و
-> ببخشید این وقت شب منم وقت گیر اوردم ازتون خیلی ممنونم که زمان گذاشتید"
"-> با سلام استاد من يك تسك سگمنتيشن با مدل يونت رو ديتاست hc18 زدم با بهينه ساز sgd و تابع اتلاف mse حس مي كنم كه نتيجه مطلوب نيست درسته و يك موردي كه بود من ٣٠ ايپوك گذاشتم و هر دفعه خطا كم مي شد و جا براي ايپوك هاي بيشتر بود ولي به نظرم كار درستي نيامد كه تعداد ايپوك از ٣٠ بيشتر باشد و مدل اشكال دارد لطفا راهنمايي بفرماييد ارادتمندم
-> 
-> بي نهايت سپاسگزارم
-> استاد اگر داخل ماسك ها را با رنگ سفيد پر كنم و به عنوان تارگت ازشون در فرايند اموزش شبكه استفاده كنم به نظرتون ايده خوب و اثر بخشيه
-> نمیدونم
-> سلام استاد وقت بخیر من بعد از دفاعم سال پیش خرداد ماه پایان نامم رو تبدیل به مقاله کردم بعد از فشار زیاد و یک ماه موندن تو تهران بالاخره استاد آبان ماه سابمیتش کرد که بلافاصله ریجکت شد چون موضوعش با ژورنال همخوانی نداشت از اونوقت تا الان هم هرچقدر فشار میارم میگه سابمیت کردم"
"-> سلام وقتتون بخیردر فیلم های اموزشی تون یک سایت معرفی کردین که میشد کارکرد اضافه کردن نورون و لایه و رو به صورت بصری انجام داد میشه ادرس سایت رو بفرمایید
-> Tensorflow playground
-> ممنون"
"-> سلام برای ادامه مسیر توی NLP کسی اموزشی میشناسه که تخصصی رو nlpکار کرده باشه زبانش زیاد مهم نیس
-> کورسراDeepLearning و udacity دورههای خوبی دارند تو NLP اما بنظرم یکم صبر کنید تا هوسم دورهی جدیدش رو بده طبق صحبت های قبلی احتمالن اواخر آذر ارائه میشه
-> سلام کورسرا هنوز هم financial aid به صورت فول می ده ینی کورس رایگان بشه چون یک ماه پیش من درخواست دادم اما تخفیف داد فقط
-> من چند ماه پیش امتحان کرده بودم داد
-> مرسی"
"-> دوستان سلام شبتون بخیر من یه مدل پایتورچ رو بعد آموزش به این صورت torchsavemodelstate_dict argsout_dir model_ argsdataset ckpt ذخیره میکنم حالا وقتی مدل رو لود کنم این یه آبجکت از همون مدل ترین شده با همون مقادیر برای پارامترها در زمان ذخیره س و اینکه من به راحتی میتونم متدهای اون مدل رو برای تسک پیشبینی صدا بزنم بدون اینکه مجدد نیازی به آموزش داشته باشم
-> 
-> خیلی از پاسختون و وقتی که گذاشتین ممنونم استاد"
"-> بعد از کلی کلنجار این دو نمودار رو بدست اوردم اما باز نمیدونم خوبن یا نه و واسه دفاع خوبه یا نه
-> به نظر من بذارید بیشتر آموزش ببینه الان به محض افت لاس نمودار تموم شده و نمیشه زیاد بهش اتکا کرد تعداد epoch ها رو بالا ببرید بذارید بیشتر ترین بشه تا بفهمید این روند کاهشی کجا تموم میشه و روند ثابت میشه و یا اینکه لاس آموزش و ولییدیشن در ادامه چجوری تغییر میکنن"
"-> این نمودار مشکلی داره میشه باش دفاع کرد برای پایان نامه
-> این ممکنه اندر فیت باشه بذار بیشتر ترین شه بهتر میشه احتمالا اگه رف ب سمت اورفیتی ته فیت کردنش همینجاس با این شبکه اینو میگم چون قبلا نمودار دادی اونجا اور فیت شده بود
-> عملکرد روش پیشنهادی و مقایسه اون با کارهای دیگران هست که در دفاع تعیینکننده هست
-> سلام استاد مورد عجیب اینکه در train دائم نوسان داره و در valid خیلی عملکرد خوبی حالا یا نام گذاری های نمودار اشتباه یا من به این مشکوک شدم که مشت نمونه خروار نیست در valid نظر شما در این مورد چیه
-> سلام بله این مهم هست که دلیلش رو بفهمیم ما اطلاعاتی از پروژه مدل و داده نداریم به عنوان یک نمونه مشکوک باید ببینیم سایز دیتاست چقدره الان با پرینتهای توی تصویر گویا تعداد نمونههای ارزیابی به 100 تا هم نمیرسه تعداد سمپلها که کم باشه میزان نوسانی و پلهای معمولا زیاد میشه ولی ممکن هست اشتباهی که فرمودید هم رخ داده بشه
-> تعداد کلش۳۶۰ تاس"
"-> به نظرتون این نمودار دسته مشکلی نداره
-> لاستون زياده
-> اوورفیت شده تقریبا از ایپوک ۲۰۰ ببعد بنظر میاد ۲۰۰ تا کافی باشه و بیشتر نباید ترین کنید"
"-> سلام کسی مقالهای میتونه معرفی کنه که مدل ترکیبی CNNGRU رو توضیح داده باشه
-> سلام تو بحث Human Action Recognition معمولا زیاد هست مثل Human Action Recognition Research Based on Fusion TSCNN and LSTM Networks
-> مدل CNNGRU هم توضیحاتش پیدا میشه
-> اره هست دیدم مثالاشو
-> اگه دنبال توضیح درمورد اتصال ساده این دو شبکه هستید بهنظرم مقاله خوب نیست مباحث در مقاله سطح بالاتر از این مبحث هست بهتر هست آموزشهای ویدیویی یا وبلاگها رو دنبال کنید اگه دوره دیپ خودمون رو دارید درموردش صحبت شده و در پایتورچ یک شبکه بازگشتی و کانولوشنی رو به هم متصل کردیم و ترین گذاشتیم اگه ندارید احتمالا هم در یوتوب و هم مدیوم درموردش صحبت شده
-> برای ارائه نیاز دارم که توضیحاتی داده بشه که این توضیحات طبق رفرنس مثل مقاله باشه بخاطر همین دنبال مقاله هستم"
"-> سلام خسته نباشید ببخشید یک سوال داشتم یک مجموعه داده دارم و موقعی که روی آموزش میگذارم بدلیل نویز های موجود شبکه گمراه میشه برای این کار الگوریتم رفع نویز را ایجاد کردم ولی باید موقعیت نویز مشخص بشه آیا بهترین راه استفاده از شبکه دیکدر و انکودر هست برای مشخص شدن موقعیت نویز ممنون میشم راهنماییم کنید
-> سلام من تخصصی در این حوزه ندارم"
"-> سلام وقت بخیر من یک کد بر روی کولب دارم اجرا می کنم چنین خطای میدهد کسی با این خطا مواجه شده این خطا فقط بروی این تابع و خط نیست برکه بروی خطوط دیگر هم بوده
-> سلام کسی هنگام اجرای کد بروی کولب با این خطا مواجه شده"
"-> سلام خدمت دوستان سوالی داشتم حداقل مدل GPU سازگار با پایتورچ چه مدلی هست ممنون
-> 
-> ممنون فقط مسئله ای وجود داره روی گرافیک quadro k2000 کودا رو نصب کردم اما cudnn نصب نشد و خطای ورژن compute کارت گرافیک رو گرفت دقیقا سوال در همین مورد برام پیش اومده بود مثل اینکه cudnn روی ورژن ۶ به بعد نصب میشه در صورتیکه این کارت گرافیک ورژن compute اش ۳ هستش
-> بله درسته"
"-> سلام استاد یه سوالی داشتم بعد دیپ کاتالیست چه دورههایی رو قراره برگزار کنید
-> سلام برنامه مون اینهاست ولی نظم خاصی ندارن سلسله وبینارهای کاربردی برای دانشجوهای ارشد و دکترا مثل مقاله سمینار پروپوزال و دوره پردازش تصویر و بینایی مبتنی بر opencv با تمرکز بر تکنیکهای کلاسیک تئوری کم و کدنویسی زیاد این دوره جایگزین دوره بینایی کامپیوتر حرفه ای نیست دوره جایگزین دوره پردازش تصویر با پایتون ماست که دیگه بازنشسته شده پردازش زبان طبیعی ماشین پلاس ماشین کاتالیست ولی همشون توی امسال نمیشه دیگه تقویتی هم هست ولی احتمالا بعد این بالایی هاست جز nlp و rl بقیشون دوره های سنگین و بزرگ نیستن این دو تا احتمالا از لحاظ حجم محتوا در سطح یادگیری ماشین و عمیق و بینایی باشن
-> خیلی ممنون استاد بسیار عالی خیلی برنامههای خوبیه و امیدوارم سریعتر ارائش بدید پس شاید بشه در کنار دورهی NLP هم ارائه بشه
-> ممنونم از شماها که مدام تشویق میکنید که بهتر بشیم
-> سلام Kavehtguserid135007083 چند تا دوره مطرحه ولی فکر کنم opencv رو برگزار کنیم
-> استاد ببخشید یک سوال اگر بخوایم هم عکس آگمنت شده رو بدیم به مدل هم عکس معمولی رو باید چیکار کنیم منظورم اینه مثلا ما ی سری آگمنت روی عکس اعمال میکنیم فرضا horizontal flip الان روی عکس ورودی این اگمنت اعمال میشه و میره تو مدل من اگر بخوام علاوه بر اگمنت شده عکس خود عکس هم بره تو مدل باید چیکار کنم یعنی انگار تعداد عکسامون چند برابر بشه و به مدل بره البته ببخشید شایدم سوالم خیلی از مفاهیم پایه ای باشه ولی ممنون میشم اگر ی توضیحی بدین
-> 
-> 
-> با پایتورچه
-> نه
-> 
-> سلام استاد وقتتون بخیر یه سوالی داشتم دورهی NLP و opencv برای آذر قطعیه
-> سلام هنوز قطعی نشده شاید یکی دو هفته دیگه بتونیم دقیق مشخص کنیم
-> امیدوارم سریعتر برگزار کنید
-> استاد یک سری وبینار هم قرار بود همین مهر و آبان برای پروپوزال نویسی و مقاله نویسی برگزار بشه اونها کی برگزار میشن
-> روش فکر میکنم کم و بیش محتوا هم تولید میکنم در جریانه ولی هنوز شکل نگرفته البته در مورد نگارش مقاله نمیتونم ولی در مورد اینکه چه استراتژیای باید برای مقاله دادن و انجام پروژه باید داشت برنامه دارم
-> سلام وقتتون بخیر استاد میشه لطفا یه دیدی نسبت به هوش مصنوعی خوندن تو ارشد تو دانشگاه های خوب بدین میشه همزمان با تحصیل کار کرد و اینکه کسی که کارشناسی برق خونده با توجه به بیس ریاضیش میتونه تو هوش موفق باشه
-> سلام منظورتون از دید رو متوجه نشدم اگه میخواید اپلای دانشگاهی بعد ارشد داشته باشید بهتر هست تمرکزتون روی ارشد باشه و کیفیت تحصیل رو فدای کار نکنید هندل کردن کار و ارشد کار بسیار بسیار مشکلی هست تجربه بارها به من نشون داده که بین کار و ارشد ارشد به حاشیه میره از برق وارد هوش مصنوعی شدن بسیار زیاد رخ میده درسهایی مثل سیگنال و سیستم ریاضیات و حتی کنترل خطی میتونن در درک بهتر حوزه هوش موثر باشن ولی ایراد بچههایی که از یک رشته دیگه میان به هوش این هست که خودشون رو وسط دو رشته قرار میدن بهشون میگی رشتت چیه میگه من برقم ولی هوش کار میکنم بهش میگی خب چرا برنامهنویسیت ضعیفه و کار با ابزارها رو مسلط نیستی میگه آخه من برقی هستم اگه کسی از یک رشته دیگه وارد هوش میشه باید بپذیره که دیگه هوش کار میکنه و باید مثل بقیه باشه
-> استاد ببخشید یه سوال ازتون داشتم در زمینهی ارشد شما مشاوره میدید راستش تو سایت دیدم که برای پایان نامه مشاوره میدادید اما خوب درمورد ورود به ارشدم میخواستم بدونم اینکارو میکنید
-> منظورم اینه فضاش خیلی آکادمیک هست یا که جوریه که بشه بعد از ارشد تو صنعت مستقیما کار کرد و اینکه برای کسی که میخواد استارتاپ رو شروع کنه خوبه یا اینکه بره mba بخونه و در کنارش بره صنعت تو زمینه هوش کار کنه
-> مشاوره ورود ارشد یعنی چی
-> بیشتر درمورد انتخاب رشته هست ولی درمورد اپلای و کنکور ارشد هم چند تا سوال داشتم خواستم اگر میشه یه مشاوره بگیرم ازتون
-> استارتاپ و MBA رو نمیدونم فضای دانشگاه آکادمیک هست و به نظرم برای ورود به کار شاید از نظر علمی از طریق دانشگاه آماده کار باشید ولی از نظر عملی و فنی تقریبا هیچی ولی ارشد توی حوزه هوش در یک دانشگاه خیلی خوب و سختگیر خیلی ارزشمنده صرفا بحث درسها نیست اتفاقا درسها نقطه قوت این دوره تحصیلی نیست فاز تحقیقاتی و انجام پروژه خیلی ارزشمنده و میتونه آینده ساز باشه
-> میخواید سوالاتتون رو ایمیل کنید برای هوسم ببینم چی هست بعد در مورد مشاوره تصمیم گرفته میشه
-> خیلی ممنون از شما استاد
-> سلام استاد وقتتون بخیر برنامهی NLP و یادگیری تقویتی اوکی شدش گفته بودید تو این هفته اعلام میکنید
-> سلام قرار بود درمورد opencv و nlp بگم برنامه احتمالی ما این هست که امسال opencv و سال بعد اوایل سال nlp رو برگزار میکنیم
-> مرسی استاد بازم عالیه بنظرم احتمالن این تصمیم باعث بشه یه سری از دورهها که آپدیت میخوان رو آپدیت کنید یا شاید ادامهی ماشین پلاس و کاتالیست که حجمش کمتره رو بتونید برگزار کنید ولی خواهشن NLP رو واقعن در اولویت سال آینده قرار بدید
-> بله انشاالله توی اولویت اول سال بعد هست ممنون
-> سلام وقت بخیر من میتونم در خصوص NLP بهتون پیام بدم و یک مشورتی باهاتون داشته باشم
-> سلام روز بخیر آموزش opencv حدودا کی شروع میشه
-> آذر ایشالا شروع میکنیم نزدیک بشیم دقیقترش رو زودتر اینجا میگم"
"-> سلام وقتتون بخیر ببخشید یه سوال داشتم برای تریس یک آبجکت فقط بهترین راه از بین segmentation object detection آیا object detection بهتر هست و این برتری بسته به سایز شی دارهالبته که هر کدام کابرد خودش را دارد من فقط میخوام پریدیک کنه حرکت بعدی شی مورد نظر را که تابع این حدس زدن حرکت شی را طراحی کردم ولی فقط سرعت تشخیص مهمه به عبارتی FPS ممنون میشم یه راهنمایی بکنید
-> 
-> خیلی خیلی ممنونم استاد از راهنماییتون"
"-> سلام وقت بخیر بی زحمت میشه یه توضیح کوتاه درباره داده کاوی بدید اینکه چی هست و چه فرقی با دوره های یادگیری ماشین و یادگیری عمیق داره و اینکه میشه از دوره های هوسم برای داده کاوی استفاده کرد
-> سلام کوتاهش اینکه دوره یادگیری ماشین و پایتون برای هوش مصنوعی میتونه به درد داده کاوی هم بخوره اما فرصت کنم یک ویس میفرستم از تفاوتها و"
"-> استاد در مورد ارائه پایان نامه داشتید که از دوره هایتان حذف کرده اید میشه لطف کنید و آن دوره را رایگان بکنید
-> امیرعباس اینو همه جا گفتی اگه صلاح بود حتما این کار رو میکردیم"
"-> چه طور معمولی بنویسم فصل ها باید به چه صورت باشدترتیب فصل ها باید چگونه باشد
-> 
-> سلام استاد امکانش هست دوره یا وبیناری در این مورد برگزار کنید
-> سلام ممنون بابت پیشنهاد اتفاقا اخیرا یک برنامه براش ریختیم دوره برای اینها مناسب نیست اما فکر کنم در یک وبینار حداکثر سه ساعته بتونیم درمورد این مسائل صحبت کنیم حدود 7 8 مورد وبینار مرتبط با کارهای آکادمیک طراحی کردیم
-> چه عالی زمان برگزاری امسال هست حدودا یا میفته برای سال های بعد
-> نه امساله میخوام سعی کنم انشالله از مهر شروع کنیم که کمکی به دوستان تازهوارد ارشد بشه"
"-> سلام برای نوشتن مقاله از چه قالبی باید استفاده کنم ممنون میشم اگه کسی من رو راهنمایی کنه
-> بستگی به این داره که برای کجا مقاله رو میفرستید بعضی جاها خودشون قالب اختصاصی دارن و بعضیها محدودیتی نذاشتن باید صفحات دستورالعمل مقالات رو بخونید میتونید مقاله رو در ورد یا لتکس به صورت معمولی بنویسید بعد بسته به اون جایی که میفرستید متن رو به شکل مناسب در بیارید"
"-> سلام استاد وقت شما بخیر من یک مدل Neural Image Retrieval روی دیتاست Flickr پیاده سازی کردم ولی هر ایپاک تقریبا ۴۵ دقیقه طول میکشه می خواستم بپرسم راهی هست که من بتونم GPU قوی تری داشته باشم
-> سلام چند عامل در زمانبر بودن موثر هست حجم دیتاست ترین گویا فلیکر حدود 30 هزار نمونه داره شبکه سایز ورودی با فرض اینکه کد مشکل نداره دو تا کار میتونید انجام بدید اول حجم دیتاست رو برای گرفتن نتایج اولیه کاهش بدید کارهای دیگه مثل کاهش سایز ورودی و مدل هم میتونید انجام بدید دوم بعد اینکه با دیتاست اولیه به نتیجه امیدوارکننده رسیدید روی کل دیتاست ترین بذارید برای این کار میتونید از سرویسهای داخلی و خارجی استفاده کنید یک نفر به من گفت که سرورهای فردوسی مشهد خوب بودن و از قیمتش هم راضی بود من تجربه کار با سرویسهای داخلی رو نداشتم بازهم سرچ کنید حتما سرویسهای دیگهای هم پیدا میکنید ولی حتما مطمئن بشید که کد مشکل نداره از بخشهای مختلف کد تایم بگیرید تا ببینید واقعا بهصورت منطقی 45 دقیقه طول میکشه یادم هست توی دوره بینایی تمرینی دادیم که یکی از بچهها هر ایپوکش 2 ساعت طول میکشید بعدا متوجه شد که کدش تصاویر رو از گوگل درایو لود میکنه و توی کولب کپی نکرده بود
-> سلام من یه ویژن ترنسفرمری رو ترین میکردم ک هر ایپک حوالی چهار ساعت بودش با imagenet با سی تی اسکن ایپک کمتر از دو ساعت من ندیدم
-> از سیمرغ امیرکبیر سه تا a100 گرفته بودم و موازیشون کرده بودم این تایمی ک گفتی چیز معقولیه
-> راستی سیمرغ تخفیف ۴۵ درصدی برا امیرکبیریا میده از دوستات کسی باشن میتونی از اکانت اونا استفاده کنی تخفیف لبزنت رو هم چک کن
-> ممنون حسین جان اطلاعات ارزشمندی بود"
"-> سلام استاد وقتتون بخیر بنده این ترم میخواهم درس بینایی کامپیوتر که در راستای تزم ردیابی ویدیویی طیف چندگانه هست بردارم جسارتا میخواستم بدونم دوره بینایی کامیپوتر شما چند درصد از سیلابس تدرسی را کاور میکند و تو راستای تزم کمکم میکنه
-> سلام با موضوع پایاننامه شما آشنایی ندارم دوره بینایی کامپیوتر هوسم متفاوت از این درس شماست در درس شما بینایی کامپیوتر کلاسیک آموزش داده میشه و در فصل آخرش از شبکه cnn صحبت میشه اما در دوره هوسم کل دوره بر پایه دیپ لرنینگ هست معمولا اینطور هست که برای شروع درس شما خوبه اما معمولا برای انجام پایاننامه خوب نیست اما دوره هوسم برای فردی که میخواد تازه شروع کنه و قبلش چیزی از پردازش تصویر و دیپ نمیدونه مشکل هست اما برای پایاننامه خوب هست توضیحاتم کلی بود و مختص پایاننامه شما نیست
-> سپاس گزارم از لطفتون"
"-> سلام دوستان من موقع کار با گوگل کولب فیلتر شکنم روشن بود الان حدود یک هفته ست هرکاری میکنم موقع ترین بدون دلیل اجرا رو قطع میکنه اکانت دیگه م رو تست کردم انگار تمام اکانتام رو بلاک کرده ایا راه حل برای رفع این مشکل وجود داره
-> منم موقع استفاده از فیلتر شکن استفاده میکنم بعضی وقتا هم نه و تا حالا مشکلی نبوده روی local host هم قطع میکنه
-> دقیقا نمیدونم مشکلش چیه به جز حلقه ترین تمام کد ها رو اجرا میکنه وقتی به حلقه ترین میرسه runtime قطع میشه چند روز پیش که از فیلتر شکن استفاده کرده بودم برام یه ایمیل اومد که اکانت گوگل درایوت رو به یه اکانت ناشناس سپردی از اون موقع کلا کار اصلی رو انجام نمیده
-> وقتی آی پی عوض میشه معمولا همچین پیغامی بهت داده میشه اما اگر هنوز دسترس داری به درایو نباید مشکلی باشه اینو در نظر بگیر که کولب همیشه پکیج هاشو اپدیت میکنه و بعضی وقتا دلیل ران نشدن تغییر ورژنه ی نکته دیگه هم اینکه قوانین استفاده از کولب رو ببین قبلا روی اکانت های رایگان فقط gpu بود که محدودیت داشت شاید برای cpu هم اخیرا گذاشته باشن
-> توی نت که سرچ کردم میگه یکی از راه ها اینکه نوت بوکت باید روی local runtime باشه این مشکل رو چطور میشه برطرف کرد
-> local runtime یعنی به سیستم خودت وصل بشی و توان محاسباتی رو از سیستم خودت بگیری معادل استفاده از جوپیتره فقط ide رو از گوگل گرفتی"
"-> سلام برای دوستانی که میخوان مقاله رزنت رو بررسی کنن
-> سلام تند تند خوندمش خوب بود موفق باشید
-> باعث خوشحالیه آقای دکتر خیلی ممنونم"
"-> سلام دوستان من مقالات جدید سگمنتشن آشنا نیستم در حال حاضر کدوم الگوریتم برای سگمنتشن برترین هست از لحاظ سرعت و دقت
-> سلام از سایت paperswithcode اطلاعات جامعی درمورد سوالتون میتونید بدست بیارید
-> ببخشید برای پردازش تصاویر thermal camera چه روشی هایی رو پیشنهاد میکنید من نتونستم موضوع زیادی پیدا کنم
-> متشکرم
-> من توی این حوزه تخصصی ندارم"
"-> سلام و ادب خدمت استاد گرامی استاد در دوره فصل ۱۳ یک مثال که در سطح نود هست رو ارائه شده که فقط مثال رو دیدم با دیتاست mutag بود استاد این فصل چقدر میتونه برای کار لینک پردیکشن به من کمک کنه
-> سلام به نظر میرسه شما با مقدمات آشنا هستید فکر نکنم دانش جدیدی به شما اضافه کنه"
"-> سلام وقت بخیر من یه کدی رو از گیتاب گرفتم و روی کولب اموزش دادم اما باید روی سیستم خودم هم که gpu داره امتحانش کنم تا حالا این کار رو نکردم میتونید کمکم کنید که چه مراحلی رو باید طی کنم
-> سلام چه کمکی منظورتون از مراحل چی هست
-> اینکه در محیط اناکوندا اجراش کنم یا یک ide مثل پایچارم مثلا بخوام در محیط پایچارم اجرا کنم به چه صورت باید عمل کنم Requirement ها رو چجوری نصب کنم که ارور کتابخونه ها نباشه بعد چطور gpu رو در دسترس قرار بدم یعنی باید پایتورچ کودا نصب کنم کولب همه کتابخونه هارو داره و راحت جی پی یو استفاده میکنه ولی با سیستم به این راحتی نیست مثل اینکه چون بلد نیستم بهتر از این نتونستم توضیح بدم عذرخواهی میکنم
-> متوجه شدم در خصوصی بهتون میگم
-> یک دنیا ممنون"
"-> سلام استاد وقت بخیر درمورد سمانتیک سگمنتیشن سوال داشتم فرض کنید ما دو تا کلاس برای سگمنت کردن داریم و میایم هر کلاس رو جداگانه با یک مدل سگمنت میکنیم فرض کنید با دو تا یونت دو تا کلاس رو سگمنت میکنیم بعد در آخر دوتا تصویر سگمنت شده خروجی رو ترکیب میکنیم دقت کار در این حالت در مقایسه با حالتی که هر دو کلاس رو با یک مدل سگمنت کنیم چگونه خواهد بود
-> 
-> ممنون که وقت گذاشتین"
"-> سلام آقای دکتر وقت تون بخیر اگر داده مون تصویر باشه توی کلاس دیتاست نوشتن بهتره که تو init کل داده رو بخونیم یا اینکه توی فوروارد همون داده ای که ایندکس ش داده میشه رو بریم پیدا کنیم و توی فوروارد بخونیم
-> 
-> سلام خیلی ممنونم آقای دکتر"
"-> سلام استاد میخواستم بدونم اگر دوتا دیتاست داشته باشیم که shape های متفاوتی داشته باشن امکان کانکت کردن شون هست یا اینکه حتما باید shape هاشون رو یکسان کنیم بعد کانکت بشن
-> سلام نمیشه یک راه کلی ارائه داد بسته به تسک و دیتاستها باید تصمیم گرفته بشه شاید کتاب hands on data preprocessing کمکتون کنه کمی درمورد این مسائل توضیح داده"
"-> سلام استاد دوره جدیدی در حال تهیه است من توی سایت که دوره جدیدی ندیدم
-> بله دورهی دیپ کاتالیست هست
-> سلام این دوره هست"
"-> سلام آقای دکتر من میخواستم الان دوره دیپ کاتالیست رو تهیه کنم ولی سایت خطا میده
-> سلام به پشتیبانی میگم پیگیری کنه
-> ممنونم"
"-> سام استاد وقتتون بخیر استاد این امکان وجود داره که فقط بخش هایی که مربوط به پروژه های تصویر هست رو از دوره جدیدتون تهیه کنیم
-> 
-> خیلی ممنون از توضیحاتتون استاد"
"-> 
-> بله منبع همین بود"
"-> قبلا با فیلتر میانه پیش پردازشش کرده بودم اما نتیجه اونقدر تغییری نکرد
-> سورس پروژه و ماجراهایش
-> مطابق کارهایی که خودش کرده پیش برم
-> اره همه چیو اماده گذاشته"
"-> دیتاستی دارم اینطور فلش خورده قسمت تومورش برای یادگیری عمیق این دیتا مشکل داره
-> سلام وقت بخیر بنظر من که داره چون ماشین داره طبق داده یاد میگیره خودآگاهی نداره که بفهمه اون قسمت جز تصویر نیست اگه اونوقت یه دیتای بدون فلش بدید ممکنه اشتباه تصمیم گیری کنه یادتون باشه مدل براساس دیتاست یاد میگیره برای حذفش شاید بشه از تکنیکای پردازش تصویری به نحوی استفاده کرد ولی مشکل اینجاست که دیتا ست پزشکیه حساسه بلفرض فلش رو حذف کنیم چه مقادیری رو داخل پیکسل هایی ک فلش بوده بزایم تصویر معمولی بود درون یابی میکردیم اکی بشه ولی تصویر پزشکی اصلا
-> درود نه این خوبه تکونش نمیده زیاد در حد تیر مشقیه اون عدد های پر رنگ و زرد فقط نویزش زیاده ک خارج از تارگت هست پس اصل داستان اوکیه دیتاست داریم کلا مثل جاده هراز مه گرفته اصلا یه وضعی این تصویر خیلی خوبه حداقل افسر بیاد چهار تا کروکی میکشه نکاتی که این تصویر داره البته من فقط این تصویر رو دارم میبینم کل دیتاست رو نمیدونم دقیقا چقدره و تارگت گذاری بقیه چطوریه اما صرفا از روی این تصویر میگم اگر اون نقطه هایی ک خط تیره هست به صورت خط ممتد و صاف بود تازه میشه گفت این یک نویز جدی هست البته یک روش دیگه هم داریم ک ب صورت یک فلش هست یعنی طول یا عرض فقط مشخص شده ک کار سخت تره اما توی این تصویر شما نویز تقریبا ضعیفه میزان رنگ هایی ک به صورت نقطه نقطه در قسمت تارگت وجو داره خیلی کم هست و اصلا حساب نمیشه چون اگر زوم کنید با توجه ب تارگت سایه گذاری یا shadow داره هر نقطه ی زرد رنگ ک باعث میشه به شما در زمان ترمیم اگر خواستید دیگ خیلی حرفه ای برید جلو هم کمک کنه و هم به شبکه کمک میکنه و تراز میان رنگ های سیاه و زرد و سفید در اون ناحیه نرمال باشه وقتی لیبل گذاری بشه برتری و اولویت با ناحیه سیاه هست اون معماری و طراحی شبکه ی شما تعیین کننده هست که قراره با چ تکنیکی رد کنه این داستان رو
-> سلام این دیتا میتونه برای تسک ویکلی سوپروایز مفید باشه توی ویکلی سوپروایز یا کلا لیبل نداریم یا لیبلش همچین پر نیست مثل فولی سوپروایز تسک جالبیه من شخصا بهش علاقه مندم توی ویکلی سوپروایز لیبل میتونه حتی یه نقطه باشهچهارتا پیکسل یا خط باشه یا خط خطی باشه نگاهی به مقالاتش بندازید
-> این دیتاست خامی بود که از مقاله برداشتم در نظر دارم باش کار classification انجام بدم البته annotation هم داره جدا گونه
-> اینجا هم مشابه به همون نقطه میشه چون از سنتر مجدد به طرفین مقیاس گرفته میشه
-> فایل annot تکست هست یا json
-> شاید بشه پیکسل هارو به هم متصل کرد و دوتا خط متقاطع ایجاد کرد اینجوری انوتیشن اطلاعات بیشتری میده بهش
-> تا اونجا که بلدم به نظر مشکلی نباشه
-> ببخشید من اینطور میگم ولی تازه کارم و کم کم دارم یاد میگیرم این پایان نامه هست Annotation این دیتاست تصاویر مشخص شده تومور بود
-> اگر دیتاست عمومی هست لینک بدید
-> سلام پیام دوستان رو خوندم ممنون بابت مشارکت من وقتی تصویر رو باز کردم بلافاصله نگاهم به سمت بخش علامت خورده رفت و طبیعتا فهمیدم که تارگت توی اون ناحیه قرار داره این سوال برام ایجاد شد که این علامتها که بخشی از هر تصویر شدن شاید شبیه اطلاعات اضافه و تقلبی برای شبکه باشن چرا شبکه هم همیشه دنبال بخش علامت خورده در تصویر نباشه یک جایی امیرحسین گفت که ممکن هست تصویر بدون چنین خطهایی رو به شبکه آموزش دیده با این تصاویر بدیم عملکرد مدل ضعیف باشه فکر میکنم این نکته مهم باید مورد آزمایش و بررسی قرار بگیره اما در عین حال یک جایی هم از تکنیکهای حذفش صحبت کرد که زود بود به نظرم ما وقتی احتمال میدیم چنین مشکلی وجود داره باید از طریق آزمایش و بررسی به یقین برسیم ممکن هست اصلا این حدس اولیه ما درست نباشه و این خطها مشکلی ایجاد نکنن
-> سلامسونوگرافیست اینجوری با ست آپ خود دستگاه سونوگرافی اومده ناحیه تومورال رو نشون داده این فلش ها و این علامت ها مرتبط با همینه
-> سلام من دیروز یادم رفت بگم اگه واسه تسک سگمنتیشن بخواید دیتارو استفاده کنید فک کنم مشکلی نداشته باشید چون نواحی تومور رو مجبورید پیکسل به پیکسل لیبل بزنید و نواحی فلش دار میفته توی نواحی لیبل زده شده و مشکل انچنانی نداره"
"-> سلام استاد وقت بخیر این دوره ای که چهار شهریور شروع میشه مثل بقیه دوره ها ریکورد میشه و در اسپات پلیر قرار میگیره که بعدا استفاده کنیم
-> سلام امیرحسن جان بله ویدئوها با کیفیت خوبی مثل دوره یادگیری ماشین و یادگیری عمیق ضبط و تدوین میشن و در اسپات پلیر قرار میگیرن"
"-> سلام استاد وقت بخیر پکیج یا راه حلی دارید برای اینکه توی ویدئو بتونه افرادی که در حال راه رفتن یا دویدن هستند رو تشخیص بده
-> سلام خیر اطلاعاتی ندارم"
"-> برای نصب EfficientNetB7 در محیط Google Colab و با استفاده از PyTorch مراحل زیر را دنبال کنید 1 نصب پکیج های مورد نیاز ابتدا باید PyTorch را نصب کنید برای نصب آخرین نسخه PyTorch از دستور زیر استفاده کنید python pip install torch torchvision سپس پکیج efficientnet_pytorch را نصب کنید برای نصب این پکیج از دستور زیر استفاده کنید python pip install efficientnet_pytorch 2 وارد کردن کتابخانه ها و ماژول های مورد نیاز python import torch from efficientnet_pytorch import EfficientNet 3 ساختن مدل EfficientNetB7 python model EfficientNetfrom_pretrainedefficientnetb7 4 استفاده از مدل برای پیشبینی python input_tensor torchrandn1 3 224 224 تنسور ورودی به ابعاد batch_size channels width height output modelinput_tensor توجه برای استفاده از مدل EfficientNetB7 باید دستور from_pretrained را با ذکر نام مدل efficientnetb7 استفاده کنید همچنین قبل از استفاده از مدل باید پکیج efficientnet_pytorch را نصب کرده باشید داخل پایتورچ اینجاست from torchvisionmodels import efficientnet_b7
-> اینو خودتون نوشتید پایتورچ مدل افیشنت نت B7 نداشت لینکی که بالا فرستادم مربوط به B7 در کتابخونه پایتورچ هست
-> خودم با کتاب خانه ای برای efficient net b0 استفاده میکردم b7رو زدم ولی خطا میداد کتاب خانه نصب نیست این رو از یکی از هوش مصنوعی ها پرسیدم در کدم اعمال کردم درست شد
-> بههرصورت خوشبختانه مشکل حل شده اما نیازی به استفاده از لایبرری جانبی برای افیشنت نت نیست دوست عزیزی که راهکار رو برای شما فرستادن در آخرش به این اشاره کردن که B7 در پایتورچ موجود هست"
"-> کتاب خونه efficient net b7 رو با چه دستوری در گوگل کولب نصب کنم
-> افیشنت نت B7 مگه کتابخونه داره توی پایتورچ شبکههای افیشنت نت آماده وجود داره
-> برای b7 نداشت چک کرده بودم"
"-> سلام دوستان توی سایت پایتورچ توی دستور نصب اگر از کندا استفاده کنین کودا رو هم نصب میکنه ولی با pip نه دلیلی داره مگه وقتی با pip نصب میشه نیازی به کودا برای استفاده از کارت گرافیک نداره
-> 
-> سلام خیلی ممنونم آقای دکتر از توضیح جامع تون و وقتی که گذاشتین
-> سلام خسته نباشید خواستم بپرسم داخل دوره هاتون درباره گراف غیر قطعی هم آموزشی دارید
-> سلام خیر
-> ممنون از توضیحاتتون من هفته پیش کلی با زحمت کودا و پایتورچ به دلیل حدف و نصب ویندوز نصب کردم آیا می تونستم با conda پایتورچ نصب کنم و بقیه پکیج هارو با pip نصب کنم یا فقط با یکی از این پکیج منیجرها باید کار کرد
-> سلام میتونید"
"-> دیتاستی دارم که شامل تصاویر سونوگرافی تخمدان مبتلایان به سرطان تخمدان هست چطور میتونم از اون برای تشخیص سرطان تخمدان استفاده کنم دیتاست من فقط شامل تصاویر کلاس بیمار هست و تصاویر سونوگرافی تخمدان سالم رو ندارم
-> میتونید خروجی لایه اخر رو یه سیگموید بزنید که اگه شبیه به بیماری نبود و احتمالش کم بود بگه بیمار نیسشاید بشه به دوقسمت جداش کردخروجی چند لایهدو کلاسهبگه بیمار هست یا نه و اگر بیمار بود وارد چند لایه FC بشه واسه این که نوع بیماری رو تشخیص بده من باشم میام از بین همین دیتا ست چند تا عکس رو نویزی میکنم که ببینم خروجی سیگموید در بدترین حالت از چند درصد پایین بود بیماری نیسکه به اشتباه عکس بیماری رو سالم تشخیص ندمیه کانفیوژن ماتریس برا سیگموید تشکیل بدی خیلی کمکت میکنه"
"-> سلام وقت بخیر جسارتا برای شروع مهر ماه روی دوره ها تخفیف نمیزارید
-> سلام عرفان جان مشخص نیست"
"-> سلام استاد وقت بخیر برای object detection به دیتاستی نیاز داریم که باکس ها برای اشیا درون تصویر رسم شده باشه در صورتی که رسم این باندینگ باکس ها با توجه به تعداد زیاد تصاویر موردنیاز می تونه خییلی وقت گیر باشه ۱ سوالم اینه که ایا روش نیمه اتوماتیک یا اتوماتیکی وجود داره که این باکس ها رو بکشه چه روشی ۲ سوال دیگه ام اینه که اگر فرضا یه روش کاملا اتوماتیک داشته باشیم که باکس ها رو بکشه در واقع داره اشیا رو مکان یابی می کنه بنابراین فقط نیاز داریم که اشیا هر تصویر رو طبقه بندی کنیم درسته یعنی کار خییلی ساده میشه ۳ ایا برای ابجکت دیتکشن روش های بدون نظارت هم وجود داره ۴ ایا roboflow می تونه بهم کمک کنه چه کاربردی داره ببخشید سوالات زیاد شد لطفا راهنماییم کنید ممنون و دعاگوی شما هستم
-> سایت CVAT رو امتحان کن
-> ممنون از همکاری تون برنامه ها و سایت هایی هستن که عکس رو که بهشون میدی می تونی یه باکسی بکشی و اون برنامه مختصات باکس رو بهت میده منظورم این نبود برنامه ای که ما مجبور نباشیم دونه دونه باکس بکشیم خودش بکشه
-> سایت cvat خودش هم میکشه و بعد اون رو میتونید اصلاح کنید"
"-> سلام استاد گرامی وقت بخیر در مورد ترین شبکه unet سوال داشتم من از این شبکه برای سگمنت تصاویر رادیوگرافی پزشکی استفاده میکنم دیتا کاستوم هست و خودم با کمک سایت roboflow درست کردم ابعاد تصاویر بعد از ریسایز ۵۱۲ هس و برای loss هم از dicebce استفاده میکنم از albumentations هم استفاده کردم مشکلی که دارم اینه که loss با سرعت کمی کاهش پیدا میکنه مثلا یک هزارم در هر ایپاک ولی اورفیت هم نمیشه با این حال بعد از صد و بیست ایپاک از مثلا ۱۵ تا همون نزدیک ۱ کاهش پیدا میکنه لرنینگ ریت رو هم عوض کردم تغییر چندانی نکرد به نظرتون مشکل از کجاس
-> سلام شبکه یونت پریترین نداره و میتونه این مساله کمی اثرگذار باشه میتونید از مدلهایی که توی لایبرری پایتورچ سگمنتیشن مدلز هست استفاده کنید اینها انکدرشون پری ترین داره بهینهسازها هم تغییر کنه یا باهاشون کار باشه خوبه مثلا sgd با مومنتوم سریعتر میشه یا آدام و آدام w سرعت بالاتری نسبت به sgd دارن در عین حال سعی کنید قلق شبکه رو بفهمید مثلا با کم کردن دادهها سعی کنید مدل اورفیت بشه آیا این اتفاق میفته یا نه با چه هایپرپارامترهایی این اورفیت سریعتر رخ میده این هایپرپارامترها میتونن توی کل دادهها هم استفاده بشن اینها به ذهنم رسید
-> ممنون از راهنماییتون
-> استاد سلام وقت بخیر در مورد اهمیت مسائل بهینه سازی محدب و غیر محدب بودن و کاربرد هاشون توضیح میدید"
"-> سلام وقت بخیر استاد یه سوالی داشتم ممنون میشم اگر امکانش هست راهنماییم کنید من یه مدل گن pix2pix ساختم روی تصاویر پزشکی و چون روی کولب ترین میشه به ارور GPU limit میخورم حین ترین اما مدل رو سیو میکنم و وقتی دوباره دسترسی جی پی یو پیدا کردم دوباره مدل رو لود و ران میکنم اما اولین ایپاک بعد از لود معمولا چند ساعت طولانی حدود ۲ الی ۳ ساعت طول میکشه و از بعد اون ایپاک ها حدود ده دقیقه ای ران میشن میخواستم بدونم این طبیعی هست و اینجوری هست اصلا یا اینکه جایی رو دارم اشتباه میکنم به کل یا مشکل از دیتاهای من هست خیلی ممنون میشم اگر ممکنه راهنماییم کنید
-> سلام طبیعی نیست دلیل مشکل رو نمیدونم باید کدها دقیق بررسی بشه"
"-> کسی به این دیتاست دسترسی داره
-> پولیه درسته من ندارم ولی چند راه به ذهنم میرسه شاید بدردتون بخوره اگه دانشجو هستید شاید ایمیل بزنید و بگید من دانشجو هستم و شرایط مالی مناسب ندارم بهتون رایگان دیتاست رو بدن اگه هم با هزینش مشکلی ندارید ولی در پرداخت مشکل دارید میتونید از طریق سایتهای واسط مثل ایرانی کارت اقدام کنید اگر برای کارهای دانشجویی میخواید شاید بتونید با استادتون هماهنگ کنید که بعد پرداخت هزینه رو دانشگاه به شما بده
-> سپاس بی کران از راهنمایی شما"
"-> سلام وقت بخیر استاد گرانقدر برای اجرای کدهای گیت هاب و دستورات مربوط به پیش پردازش دیتاست چه اموزشی رو پیشنهاد می فرمایید
-> 
-> صدا خوب نشده فکر کنم
-> ممنون از توضیحات بله من فیلمهای بینایی ماشین رو دیدم ولی read me هر کد گیت هاب فرق داره الان مشکل من در دانلود دیتاست و پیش پردازش دیتاست هست که دیتاست در قسمت one drive هست ولی هیچ لینکی برای دانلود گذاشته نشده همچنین نحوه چیدمان دیتاست و ادرس دهی آن هست که مثلا قسمت train در چند پوشه گذاشته شده و من اطلاعاتی در این مورد ندارم اگه راهنمایی بیشتری داشته باشید ممنون میشم
-> مثلا این قسمت که گفته چینش دیتاست به این صورت باشه اگه اموزش یا توضیحی در این مورد وجود داره ممنون میشم راهنمایی بفرمایید
-> اینها رو نمیشه در آموزش گفت مشکل برمیگرده به همون چند فاکتوری که در ویس گفتم یا باید با سعی و خطا مشکل رو خودتون حل کنید و یاد بگیرید یا اینکه ناچارا یک معلم خصوصی پیدا کنید که اینها رو به صورت اختصاصی به شما آموزش بده
-> بله ریپوهای گیتهاب متفاوت هستن به همین خاطر بود که گفتم اینها در قالب آموزش نمیگنجه و شما باید فاکتورهای دیگه رو تقویت کنید تا این مشکلتون حل بشه
-> بله ممنون چه فاکتورهایی باید تقویت بشه که این مطالب و مشکل من هم حل بشه
-> تو ویس گفتم و چند بار تاکید کردم
-> ممنون از راهنماییها
-> سلام استاد گرانقدر در مورد سوال مربوط به چینش دیتاست مطابق شکل دقیقا چه کلید واژه ای باید سرچ بشه مربوط به دستورات لینوکسی هست یا پایتون من فیلم پایتون و گیت هاب و پایتورچ رو دیدم ولی متاسفانه چنین گزینه ای رو پیدا نکردم اگه راهنمایی بفرمایید ممنون میشم
-> متوجه منظورتون نشدم دستور قرار هست چیکار براتون انجام بده
-> گفته read me کد هست که گفته دیتاست رو به این شکل گفته شده چیده شده باشه قسمت set up لینک پایین هست اگه راهنمایی داشته باشید من واقعا ممنون میشم خیلی گشتم ولی چیزی پیدا نکردم
-> به نظر من که اینها چند تا دیتاست هستن که باید دانلود بشن و با همین ساختاری که گفته کنار هم قرار بگیرن شاید حتی کد هم نخواد چند تا کپی از این پوشه به اون پوشه این کارها آموزش و فیلم براشون پیدا نمیشه وقتتون رو برای سرچ آموزش درباره اینها تلف نکنید بیشتر نیازمند کمی مهارت در کار با فایل و دیتاست و اینجور مسائل هست اگه احساس میکنید مشکل دارید یک نفر رو پیدا کنید که آنلاین براتون انجام بده بهتون توضیح بده راهنمایی بیشتری به ذهنم نمیاد
-> بله ممنون"
"-> سلام دوستان الگوریتم های ماشین لرنینگ کلاسیک هنوز توی nlp استفاده میشن و کاربرد دارن یا اینکه فقط از دیپ لرنینگ توی nlp استفاده میشه دیگه ممنون میشم راهنمایی م کنین
-> بستگی به تسک داره اما الان که همه داریم از ترنسفرمرها استفاده میکنیم فقط یه نکته ای که دیتای زیادی لازمه حتی برای فاین تیون مدلها الان مدلهای پری ترین چندزبانه هستن و اگر دیتا کم باشه توی اینفرس مدل یهو به جای فارسی مثلا هندی یا عربی خروجی میده"
"-> سلام و وقت بخیر خدمت استاد گرانقدر استاد من یک سوال داشتم ما در شبکه های کانولوشنی مثل vgg و ResNet از modelchildren برای دسترسی به لایه های داخلی استفاده می کنیم آیا در معماری yolo v5 راهی هست که بتونیم به لایه و بلوک های داخلی دسترسی پیدا کنیم و مدل رو طبق نیازمون ببریم لازم به ذکره که مدلی از یولو رو ک با torchload لود می کنیم تایپش autoshape هست
-> سلام من با یولو 5 کار نکردم معمولا مدلهای پایتورچی رو میشه با همون دستور آنالیز کرد ولی گویا مدلی که اشاره کردید از پایه در پایتورچ نیست و شاید از یک فریمورک دیگه به پایتورچ تبدیل شده چنین شرایطی رو شک دارم بشه با دستور بالا کاری کرد
-> بله درسته از فریم ورک Darknet Research استفاده کردن ممنونم بابت پاسختون"
"-> سلام وقت بخیر برای تشخیص بیماری از روی تصاویر پزشکی تصاویر سی تی اسکن برای تشخیص سرطان با استفاده از شبکه عصبی کانولوشن دوره یادگیری عمیق کفایت میکنه یا باید دوره بینایی کامپیوتر هم تهیه بشه
-> سلام به نظر من دوره بینایی کامپیوتر برای این پروژه خیلی بهتره
-> سلام اگه به یادگیری عمیق مسلط نیستید حتما با یادگیری عمیق شروع کنید چنانچه خواستید روی شبکه عصبی کانولوشنی تسلط بیشتری پیدا کنید فاز اول بینایی کامپیوتر رو ببینید"
"-> دوستان کسی از colab pro استفاده کرده تا حالا چجوری میشه خریداری کرد و اینکه از نظر سرعت تفاوت چشمگیری داره
-> من داشتم ی مدت سرعتش فرق نمیکرد
-> 
-> خیلی ممنون از توضیحات کاملتون به هرحال من الان برای کارم دارم از کولب رایگان استفاده میکنم و تا حد خوبی جواب میده و من میخوام سرعت کارمو یه کم بالا ببرم"
"-> سلام دوستان من می خوام موضوع پایان نامه انتخاب کنم که ترکیب یک شاخه پزشکی و یادگیری ماشین یا یادگیری عمیق هستش کار کردن در زمینه یادگیری ماشین میتونه چالش ها و سختی های بیشتری داشته باشه یا یادگیری عمیق خیلی نگرانی این رو دارم که از پس چالش ها برنیام چون در حال حاضر دانش خیلی زیادی درمورد ماشین لرنینگ و دیپ لرنینگ ندارم و همزمان میخوام دانشم رو هم زیاد کنم ممنون میشم دوستانی که اطلاع دارن یا این مسیر رو طی کردن راهنماییم کنن
-> سلام یادگیری عمیق یکی از زیرشاخه های ماشین لرنینگ هست و زمانی که بخواید دیپ لرنینگ کار کنید باید حداقل با مفاهیم کلی ماشین لرنینگ آشنا باشید از نظر چالش ها هم بستگی به تسکی داره که میخواید انجام بدین ولی در کل دیپ لرنینگ موضوع ترند این روزهای ماشین لرنینگ هست و راه حل های خیلی قوی تری برای مسائل نسبت به سایر روش های ماشین لرنینگ داره پس بهتره با مفاهیم ماشین لرنینگ شروع کنید و بعد سراغ دیپ برید
-> خیلی ممنونم بله با مفاهیم ماشین لرنینگ آشنا هستم قوی تر بودن دیپ لرنینگ فرضا برای کارکردن روی شناسایی یک بیماری کار رو آسون تر میکنه یا برعکس پیچیدگی و چالش ها بیشتره
-> خواهش می کنم بازم میگم بستگی به نوع مسئله داره اگه داده هاتون جوری باشه که متد های دیگه ی ماشین لرنینگ بتونن خیلی سرراست حلش کنن که خب نیازی به دیپ نیست از طرفی به نظر من زیاد نگران پیچیدگی نباشید برای تسک های معمولی اونقدر کد و مثال و پیپرو هست که جای نگرانی نیست پیچیدگی مدل زمانی مورد توجه باید باشه که تسک خیلی خاصی دارید که با شبکه های موجود به راحتی و سرراست حل نشه
-> 
-> خیلی ممنون که وقت گذاشتید استاد متوجه شدم
-> 
-> خیلی متشکرم از توضیحاتتون استاد واقعیش با سرچ به نتیجه نرسیدم و با توضیحات شما و دوستان دید بهتری بدست آوردم ولی درست میفرمایین بیان مسئله ام قوی نیست چون خیلی درگیر کلیات هستم هنوز و مطالعاتم رو عمیق نکردم و همونطور که گفتین میشه گفت مسئله ام رو نشناختم"
"-> سلام استاد چه راهی رو برای ترسیم معماری شبکه برای ارائه دادن یا برای مقاله پیشنهاد می دید
-> سلام من با ویزیو میکشم دوستان خارجی که با لتکس کار میکنن drawio
-> ببخشید در مورد دیزیو و لتکس از کجا می تونم پیگیری کنم یه نمونه اولیه ببینم
-> نمیدونم معمولا برای اینجور کارها شخصا دنبال نمونه اولیه نمیگردم نرم افزار رو باز میکنی و نیم ساعتی باهاش کار میکنی و همه چی دستت میاد به هر صورت شاید یوتوب برای اینها آموزشی داشته باشه
-> سلام من خودم این لینکو چندتا قسمتشو خوندم رفتم یکم با drawio کار کردم برای یه مقاله که داشتم معماری Xception با یسری تغییرات که توی لایهاش دادم کشیدم"
"-> Web Stable Diffusion
-> هوش مصنوعی اگه خوب بود باید برای اسب هم لباس و دستگاهی درنظر میگرفت هنوز باهوش نیست
-> مثل چت جی پی تی بعضی وقتا برای یکسری موارد فکت های دقیقی میاره که آدم میگه درست میگه"
"-> سلام آقای دکتر امکانش هست پیپرهایی که این روزا کاربردی شده مثل Segment Anything یا پیپرهایی که داخل paper with code ارجاع بالایی دارن را توضیح بدین و داخل فروشگاه بذارین
-> سلام به نظر منم اگه آقای دکتر زحمت این کار رو بکشن خیلی ایده خوبیه
-> سلام ممنون کاوه جان راستش قرار بود چنین کاری برای مقالات پرارجاع انجام بدیم ولی هیچ وقت توی اولویت نبود در اینکه مورد استقبال هم قرار بگیره شک داشتیم
-> ممنون فکر میکنم از سمت دانشجوهای دکتری مورد استقبال قرار بگیره به نظرم میشه یک یا دومقاله پرارجاع را که شامل بخش تیوری و کدنویسی میشه را انجام بدین و میزان استقبال را بسنجید و برای ادامه تصمیم بگیرید
-> خودم به این کار علاقه دارم الان نمیتونم ولی بیشتر به این قضیه فکر میکنم البته کار زمانبری هست"
"-> دوستان سلام من با DataLoader بچ هامو تنظیم میکنم که هر بچ شامل داده هایی از دو کلاس مثبت و منفیه راهی به نظرتون میرسه که با همین DataLoader توی هر بچ الزاما به تعداد مساوی نمونه از دو کلاس انتخاب بشه
-> به نظرم یکی از راه ها نوشتن کاستوم دیتاست هست
-> یه شرط ساده پیمانه به دو رو داخل دیتالودر بزاری اوکی میشه"
"-> سلام و احترام به همه بزرگواران و اقاي دكتر تو كلاسيفيكيشن Precision 1 قابل قبول هست يا ايراده
-> Over fitting اتفاق افتاده
-> مطمئن نيستم راه ديگه اي نداره اطمينان پيدا كنيم كه جواب درست هست يا نه"
"-> سلام دوستان و استاد عزیز یک سوال داشتم اینکه ضرب درایه در درایه یک تنسور چ معنی میتونه داشته باشه تو طراحی یک مدل این ضرب معروف به hadamard product که قطعا تو مقالات و مدل ها دیدین میخاستم از لحاظ مفهومی یک توضیحی بدید که چرا از این ضرب برای دو تنسور استفاده میکنن
-> یه نگاهی به مقاله مدل rotatE بندازید که برای امبدینگ سه تایی های گراف دانش طراحی شده فرض کنید سه تایی h r t یه سه تایی توی گراف دانش باشه شما برای اینکه امبدینگ گره h رو بسازی میای برادر امبدینگ گره همسایه ش یعنی t رو ضرب هادامار میکنی در بردار امبدینگ ریلیش بینشون یعنی r حالا این بردارهای امبدینگ با یه تابع هزینه ای یاد گرفته میشن در طول آموزش به لحاظ هندسی مفهوم ضرب هادامار دوتا بردار در یک فضای برداری معادله با عملیات rotation بردار اول در اون فضا البته یه سری ملاحضات خاصی داره اینی که گفتم خیلی کلی هستش بهتره خودت مقاله اصلی رو دقیقتر بررسی کنی و از شکل تعریف تابع اتلاف و نحوه اموزش و یادگیری پارامترها سر در بیاری
-> 
-> ممنونم استاد از شما توضیحات شما شفافیت بیشتری به مسئله من داد"
"-> سلام دوستان برای کلاسترینگ آیا باید feature selection انجام بشه یا کل feature ها رو باید بدیم به مدل
-> 
-> سلام خیلی ممنونم آقای دکتر"
"-> سلام خسته نباشید در مورد استانداردسازی و پری پروسس داده های تصویری کاستوم برای شبکه های کانولوشن آیا مطالبی در دوره دیپ لرنینگ و یا بینایی کامپیوتر گفته شده چون تا جایی که یادم میاد در مثال ها از داده های آماده استفاده میشد و اگر گفته شده کدام جلسه مطرح شده ممنون
-> 
-> آهان ممنون از شما من چون هنوز به مطالب فصل ده نرسیدم به خاطر همون پرسیدم"
"-> ببین تصویر بالا مثلا مدل های بهینه برای تصویر هست شما هم برای حوزه تصویر میخوای
-> بلهپردازش تصویر
-> 
-> این منظورم نبود یکی دیگه پیدا کردم ممنون"
"-> امیدوارم خوب باشید یک سوال اگر خواسته باشیم فقط با خروجی ترنسفورمر انکودر بصورت مولتی استپ رگرسیون کنیم و از دکودر خودش استفاده نکنیم قاعدتا باید بشه خروجیشو به یه دکودر LSTM داد یعنی شبکه transformer encoder lstm یا راحتتر از اون میشه خود اصلا خود predictرو برد توی یه حلقه به ازای مقدار مولتی استپ خواسته شده predict کنه
-> اینو ندیدین
-> دیدم ولی نمیدونم
-> باشه به هر حال ممنون"
"-> سلام به همه دوستان Kaggle رفتم ثبت نام برای دانلود دیتا و کار باهاش که بن شدم دیتا ست برای شروع تمرین کلاس بندی و کلا کار با پایتورچ سراغ دارین
-> سایکیتلرن یکسری دیتاست در ماژول datasets داره که برای شروع خوبن مثلا آیریس و breast cancer سرچ کنید لیست دیتاستها رو در سایت سایکیت ببینید
-> ممنونم"
"-> سلام من یک دیتا ستی دارم ک میخوام کامل برای تست استفاده کنم در نتیجه برای ترین دیتا ندارم ایا می تونم این دیتاست رو با نویز اگمنت کنم و بذارم برای TrainSet
-> سلام کار درستی نیست نتایج حاصل از این کار هم قابل گزارش نیست
-> ممنون"
"-> سلام یه سوال داشتم میخوام شبکه ای که دارم رو با دیتا تایپهای مختلف ترین کنم هم مدل و هم ورودی ها رو half میکنم ولی ایپاک اول درسته ولی بعدش nan میده بنظرتون مشکل از کجاست ممنون میشم راهنماییم کنید و اینکه تو پایتورچ امکان تعریف دیتاتایپ سفارشی است
-> سلام نظری ندارم معلوم نیست از همون half باشه برای ترین کردن روی فلوت 16 از apex هم میتونید استفاده کنید
-> ممنونم"
"-> دوستان سلام به عنوان یه شخص تازه کار بدون سابقه کاری مرتبط با هوش مصنوعی میشه راهنماییم کنید که چجوری یه رزومه خوب برای شروع درست کنم
-> 
-> 
-> خیلی خیلی ممنون از لطفتون تشکر که وقت گذاشتید
-> 
-> ممنون از توضیحاتتون"
"-> سلام دوستان اگه توی یک کار سگمنتیشن موقع ترین تمامی دیتای ترین توی هر تصویر فقط یک آبجکت رو برای سگمنت کردن داشته باشه اگه موقع تست یک تصویر بهش بدیم که چند تا آبجکت ولی از همون نوع آبجکت داشته باشه به نظرتون نتیجه خوبی به دست میاد در مورد آبجکت دتکشن در این حالت چطور
-> بستگی به سختی و اسونی اون ابجکت تک تصویر داره که مدل شما چطور یاد بگیره و بتونه ابجکت های تصویر دیتکت کنه حالا هر چنتا که هستش مثلا در تشخیص پلاک برای تشخیص پلاک حدود ۴۰۰۰ تا تصویر داشتیم که بالای ۳۰۰۰ تا تصاویر تک تارگت بودن ولی در در مد تست براحتی از ۱۰ تا پلاک تو تصویر حداقل ۹ تا رو دیتکت میکرد پس به این موارد مرتبط ۱ جنس دیتا سختی و اسون بودن دیتا دیتاهای پزشکی نیاز به تنوع پذیری زیادی دارن درنتیجه جز دیتاهای سخت محسوب میشن ۲ تعداد دیتای اموزش ۳ مدل ولی این نکته رو در اموزش مدل درنظر بگیرین که تنوع پذیری دیتای اموزش خیلی مهم یعنی اینکه شبکه شما مختص به شرایط خاصی نشه برای اموزش در نتیجه برای حصول اطمینان بهتر است که دیتایی داشته باشین که مثلا تصاویری با تعدد از ابجکتی که قرار است شبکه تشخیص دهد را دارا باشد
-> سلام خیلی ممنونم از توضیحات کامل تون"
"-> سلام خسته نباشید یه سوال داشتم راجع به object detection با کدوم شبکه میشه به صورت نقطه ای در تصویر detection انجام داد یعنی مستطیل و سگمنتال نباشه مثل faster rcnn و به صورت یک نقطه یا حداقل چند نقطه و مرز detection رو انجام داد
-> سلام نمیدونم کاربرد این کارتون چی هست و بنابراین نمیتونم کمکی کنم
-> فرض کنین میخوایم نوک ریشه دندان در رادیوگرافی رو مشخص کنیم به صورت یک نقطه با چه روشی باید انجام داد
-> من تا الان نوک ریشه دندان رو ندیدم ببینید منظورم از پیام قبلی این بود که بیان مساله رو باید شفاف بگید تا دیگران بتونن کمکتون کنن گاهی اوقات اینطور هست که مخاطب فکر میکنه میگه مثلا نیاز به پیدا کردن یک نقطه داره اما وقتی مساله رو شرح میده مشخص میشه که این برداشت خودش بوده و نیازی به این کار نبوده بیان مساله چطوری به پزشک مشکلمبیماریم رو با جزییات شرح میدم در بیان مساله عوامل زیادی کمک کننده هستن نمونه داده مقاله مرجع کلا روش تحقیق رو یاد بگیرید خیلی بدردتون میخوره آدم دنیاش متحول میشه فکر کنم مطالب آموزشی در اینترنت درباره روش تحقیق وجود داشته باشه"
"-> سلام توی سگمنتیشن وقتی بیشتر از دو کلاس داریم باید تارگت رو به چه شکل به شبکه داد
-> شبیه کلاسیفیکیشن به تعداد کلاسها صفحه درنظر میگیریم هر صفحه شامل صفر و یک اول جلسه دوم سگمنتیشن دوره بینایی کامپیوتر گفتم
-> سلام خیلی ممنونم آقای دکتر"
"-> سلام دوستان یه سوال ذهنم رو درگیر کرده اینکه مثلا یه پروژه نرم افزاری موبایل یا طراحی وب اپلیکیشن یا از چندین و چند کلاس فایل وتشکیل میشه در صورتی که مثلا یه پروژه ماشین لرنینگی فرض کنیم با رگرسیون خطی یه اسکریپت مثلا برای پیش پردازش میشه و یه اسکریپت که کدهاش هم تقریبا مشخص هست و تعداد خط کدهای خیلی زیادی هم نداره میشه بخش اموزش و سوالم اینه چرا اینقدر تعداد خط کدهاش کم هست این روال طبیعی هست یا چیزی هست که فقط توی سایت های اموزشی میبینیم یا شایدم ذهن من به پروژه و خط کدهای خیلی زیاد عادت کرده حقیقتش گیج شدمممکنه از دوستان راهنمایی کنند سپاسگزارم
-> درود سعید جان اینا به ۲۲۱ علت بستگی داره که یک علتش در بخش فرانت اند مثلا کار با جاوااسکریپت حالا فریم ورک های انگولار یا مثلا ریئکت و اونجا ها خیلی ریز کاری باید انجام بدیم حالا sass و اینا بماند ریزه کاریاش زیاده در بکند کد نویسی ها متفاوت تره امکان داره یه پروژه خفن باشه و تکنیکال مثل yolo که ریز کاری های زیادی داره برای نوشتن و پیاده سازی ولی استفاده ازش فقط چند خط کده حالا توی همین مواردی که ذهنت رو درگیر کرده احتمالا همیشه توی یک نوت بوک کد زدی و یک فایل بوده و اون سناریوی شکل نگرفته میتونی همین سناریو رو ماژولار کنی مثلا توی پروژه های بزرگ هر بخش ماژول بندی میشه و هر متخصص با یک یا چند فایل کار میکنه و روی گیت پوش میکنه مثلا همین قسمت پیش پردازش یا مثلا EDA و توی موارد بیگ دیتا و بیگ پروداکشن همین یک بخش چند نفره انجام میشه و هر نفر روی ماژول خوش کار میکنه اما اینکه تعداد خط ها کم هست یا زیاد به دیتاها و سناریوی اون پروژه بستگی داره و قراره چه اتفاقی بیوفته یا در فاز بعدی برای اجرا و بک تست هر کد و الگوریتم مجدد ماژولار انجام میشه حالا تمام این داستان ها تهش میشه یک مدل که مثلا ۷۰۰ کیلو بایته همین ۷۰۰ کیلو شاید شش ماه زمان برده که بشه یک مدل نهایی سناریو بعدی احتمالا میاد برای پیاده سازی روی وب و حالا با فریم ورک های مختلف Fast API جنگو و یا شاید هم pure زده بشه حالا این قسمت رو هم باید کد نویسی کنی و یک سناریو ممکنه کلا قابل پیاده سازی نباشه در حد همون local sever بمونه و ازش تحلیل استخراج بشه یک سناریو ممکنه با TKinter و امثالهم باشه که خودش کلی ریزکاری و مسخره بازی داره طبیعتا پیاده سازی با هر سناریو تعداد خط ها یا حتی ماژول نویسی متفاوتی داره حالا مقایسه از جهت اینکه مثلا ما نگران این نیستم که بما أتک بشه خب پس تامین امنیت برامون مهم نیست یه چنین مواردی شبیهش زیاد هست که توی ML از این چیزا نداریم در ظاهر که باعث میشه احساس بشه کد نویسی کم به نظر برسه نسبت به وب سایت و این رو هم در نظر داشته باش خیلی از بخش مهم کتابخانه های پایتون با cc نوشته شده عملا کدهای خیلی زیاده نوشته شده که کار مارو راحت کرده و نیازی نیست مثلا برای یک مرتب سازی دیتافریم ۲ هزار خط با ۲۵۰ تا پارامتر رو چک کنیم مثلا و خیلی از این موارد که باعث شده کد نویسی در ظاهر کم باشه که ۲۰۰۰ خط شده مثلا دو خط اگر با c بخوای هندل کنی خب یکمی اوضاع خیته حتی مینی لایبرری های جاوا اسکریپتی هم کد نویسیش زیاد میشه برای ML یعنی بستگی داره با چی میخوای برای سراغ ML خلاصه این چیزا عادیه سخت نگیر انشاءالله با ریلیز Mojo سه چهار هزار خط میزنیم دگ دوشواری نداریم
-> ماشالا به توضیح
-> این رو هم به توضیحاتت باید اضافه کنی که مبحث ai نسبت به سایر مباحث برنامه نویسی ریاضی تره خود بحث درگیری با ریاضی و فلسفه داستان و تحلیل دیتا و مدل و تابع هزینه نویسی خودش به اندازه هزاران خط کدنویسی انرژی بر و زمان بره واسه همین فلفل نبین چ ریزه
-> درود علی جان بله کاملا متفاوت هست اونم از اون فلفل کوچیکا که خیلی تنده همراه با یک قاشق سرکه هم روش
-> درود علی اقای گل دقیقا
-> سلام ممنون از توضیحات کاملتون خیییییلی لطف کردید ممنون از وقتی که گذاشتید"
"-> سلام دوستان توی kfold باید دیتا رو به دو بخش تقسیم کرد یا ۳ بخش
-> سلام ما داده ترین رو با کافولد به دو بخش ولیدیشن و ترین تقسیم میکنیم تست جداست و اصلا وارد این فاز نمیشه فکر کنم دوره ماشین لرنینگ رو دارید هفته 5 و 6 انتخاب مدل درمورد کراس ولیدیشن صحبت شده
-> سلام خیلی ممنونم آقای دکتر
-> سلام استاد خسته نباشید وقتتون بخیر ببخشید مزاحمتون شدم استاد ببخشید ی سؤال شما در مورد پیاده سازی الگوریتم های هوش مصنوعی روی fpga کار کردین یا منبعی سراغ دارید
-> درود پوریا جان روی چه ایده ای میخای کار کنی
-> سلام نه کار نکردم
-> واسه سریع تر کردن زمان پاسخ دهی الگوریتم های هوش مصنوعی میگم ی سری مقاله هم در موردش خوندم الان مثلا الگوریتم های linear regression رو میتونم پیاده سازی کنم روش ولی مثلا cnn رو نه
-> 
-> آفرین چه خفن"
"-> بعد جمع آوری دیتا دیتا بایدcleanبشه Data cleaning وdata mining
-> مثلا اول بخش تمیز کردن داده ها رو دارم بعد داده های تمیز که مثلا nan و نداشت حالا اوت لایرها پیدا کنم بعد نرمال سازی داده انجام بدم بعد تازه بخش امار و کورولیشن انجام میشه و بعد تصمیم به انتخاب نوع مدل است بعد اموزش و ارزیابی و این گام ها درست هستن بر اساس اولویت
-> اره به نظرم شما اول یه مقاله در مورد نمونه دیتای خودتون بخونید یا کگل سرچ کنید یا گیت هاب و towards data science مدیوم برا شروع بهتره چون ساده تر توضیح داده دو سه مقاله بخونید تا نسبت به دیتای خودتون و مدلهایی که ران شده اطلاعات پیدا کنید مقالات مروری بهترن به نظرم با مقاله دید پیدا میکنید که چی به چیه مقاله مثل مثال میمونه با مثال بهتر میفهمید
-> ممنون از راهنماییتون"
"-> سلام دوستان پروژه های یادگیری ماشين رو اگر بخوایم فلز بندی کنیم دارای چه گام هایی هستند به ترتیب بعد از جمع آوری اطلاعات منظورم است ممنون
-> سوال اولتون باید این باشه که آیا اصلا این مسله پروژه ی ماشین لرنینگ هست یا نه سوالتون خیلی کلی هست و معمولا سوالات کلی جواب های کلی دارند و احتمالا تعداد محدودی افراد وقت میذارن جواب سوال شما رو بگن
-> بله ماشین لرنینگ
-> منظور من اینه خیلی وقت ها مسله با روشهای ساده تر حل میشه و اصلا نیاز بهmlنداره
-> بله متوجه شدم فرض اینه که ماشین لرنینگ باشه
-> اینا استثپهای اصلی هستند در مورد دیتای خودتون یه سرچ اختصاصی در مدیوم بزنید
-> سپاسگزارم"
"-> سلام وقتتون به خیر چرا وقتی در جریان آموزش شبکه عصبی تابع لاس کاهش پیدا میکنه خطای MSE افزایش پیدا میکنه کسی به چنین موردی برخورده اگر احتیاج به توضیحات بیشتری در مورد نوع برنامه و الگوریتم هست بفرمائید تا بگم
-> چرا برای محاسبه خطا از mse استفاده میکنید موضوع چیه میشه دقیق توضیح بدیذ
-> Overfit
-> متوجه نشدم خوب خطای MSE یکی از روشهای ارزیابی مدل هست دیگه میشه خطای دیگری هم استفاده کرد ولی من تو برنامهام ازین خطا استفاده کردم
-> خوب چه باید کرد من ابعاد مدل رو کاهش دادم اوپتیمایزر رو عوض کردم از Weight_decay و dropout هم استفاده کردم ولی نتیجه نداد
-> مسئلتون رگرشن هست یا کلاسیفیکیشن
-> در واقع هیچکدام یه مساله ریاضی هست که تا حد زیادی شبیه رگرشن هست
-> هایپرپارامترهاتونو و شبکتون رو چک کنید تعداد نورون ها و لایهها رو کم و زیاپ کنید میزان دیتاتون چقدره آیا چیزی شبیه Time series هست مسئله
-> همه این کارها رو انجام دادم ولی تغییری حاصل نشد مساله من حل معادله به کمک شبکه عصبی است"
"-> سلام دوستان و استاد عزیز لاس منفی داره حساب میشه بنظرتون ایراد خاصی داره ولی مدل داره باتوجه متریک های دیگه یاد میگیره من میخام بدونم الان لاس منفی بخاطر چیه
-> مساله چیه لاس چیه
-> سلام استاد لاس یا همون خطای شبکه وقتی اعداد منفی حساب میشه یعنی چی
-> منظور اینه که چه تابعی برای لاس در تعریف کردین تا جایی که من می دونم معمولا از قدر مطلق یا توان ۲ استفاده می کنن تا خطا منفی نشه شاید مشکل از تعریف تابع تون باشه
-> ی تابع وزندار شده از نوع انتروپی و dice برای تسک سگمنتیشن
-> اول ببینید کدوم بخش منفی میشه مثلا اگر خروجی از بین 0 تا 1 نباشه کراس آنتروپی منفی میشه
-> سلام استاد خسته نباشید وقتتون بخیر ببخشید مزاحمتون شدم استاد ببخشید برای پیاده سازی این بلوک دیاگرام پردازش تصویری اون قسمتی که خروجی رو هم از استیج 2 میگیره هم از استیج 3 اینا باید باهمدیگه جمع بشند این بلوک دیاگرام کارش اینه که توی ی عکس از زمین کشاورزی محصول کشاورزی و علف های هرز رو از هم تشخیص بده متدی که تو مقاله گفته شده اینه عملیات هایی که تو استیج 2 نوشته شده تو خود مقاله اومده فرمولاش فقط سر اینکه خروجی استیج 1 و 3 رو چطوری باهم ترکیب کنم مشکل دارم منظورم اینه فقط باید جمع کنم
-> سلام نمیدونم باید از دل مقاله اطلاعاتش رو دربیارید گویا یکی ماسک foreground و یکی background هست
-> سلام استاد خسته نباشید وقتتون بخیر ببخشید مزاحمتون شدم استاد ببخشید برای پیاده سازی این عملیات های مورفولوژی باید از چه دستوری تو اوپن سی وی استفاده کرد خودم سرچ کردم دستورای مورفولوژی اوپن سیوی رو ولی دستوراتش شباهتی به اینا مخصوصا 2 و 3 نداشتند
-> سلام نمیدونم ممکن هست واقعا وجود نداشته باشه و نیاز باشه خودتون پیاده کنید به فریمورک سایکیت ایمیج هم نگاهی بندازید"
"-> 
-> توضیح مختصری مینوشتید خوب میشد
-> چشم حتما سرعت نت اجازه اپلود فایل رو نداد شرمنده
-> ما تلاش کردیم تا با ترکیب الگوریتم های یادگیری ماشین مثل SVMKNNDT با الگوریتم آماری FoCCA که سال ۲۰۱۹ به عنوان الگوریتمی موفق برای پردازش سیگنال های مغزی معرفی شد سطح کیفی و کمی طبقه بندی سیگنال های مغزی رو افزایش بدیم در نهایت نتایج رو با الگوریتم های پردازشی معروفی مقایسه کردیم
-> بسیار هم عالی براش پردازش ssvep منظورتون بود درست میگم براساس روش fusion CCA
-> بله البته این کار حاشیه یک تحقیق بزرگتر بود که نتیجه نهایی پروژه امیدوارم تا چند وقت دیگه چاپ بشه
-> انشالله من روی تصور حرکتی کار میکنم البته موفق به نوشتن مقاله نشدم شما کسی رو میشناسید در این زمینه فعالیت داشته باشنپی وی میتونم مزاحم بشم
-> موفق باشید چند سال پیش که من شروع به مطالعه کردم یک مفاله جالب برای تصور حرکتی دیدم که بیس کار دیپ لرنینگ بود CNNBiLSTM فکر کردم شاید براتون جالب باشه"
"-> جالب ترین مطلب اینکه استانبول بودم سرعت اینترنت هتل ۱۵ مگ دانلود میکرد و همه چی انلاین حتی ریپازیتوری های لینوکسمم اپدیت کردم حساب کنید تو ی هتلکاملا معمولی و جالب اینکه الان در تهران تو شرکت میخام پکیج opencv رو اپدیت کنم حدود ۶۷ مگ با پیپ داره ۶۰ کیلوبایت دانلود میکنه و عملا به هیچ جا دسترسی نداره الی کولب یعنی فقط باید اسیر vpn دور زدن تحریم dns زدن و فلان و بیسار باشی یهومیبینی اونروز زندگیت چه شکلی رفتت هم گریه ات میگیره هم خندت حس عجیب زنده باد ایران
-> درود علی جان نت ایرانسل اوضاعش چطوره اونجا منظورم شهرت تست کردی
-> خدا لعنتشون کنه نمیشه رفت تو سایت دانشگاه ها حتی برای خروج و فرار از دستشون هم راهی واسمون نمیذارن
-> والا من نت خطم همراه اول و مودمی هم وایمکس مبین نت دارم"
"-> سلام وقت تون بخیر در faster rcnn انکر ها از فیچرمپ حاصل از cnn بدست میاد ولی cls score ها بعد از یک لایه کانولوشنی دیگه بدست میان پس چجوری میشه که تعداد هر دو N باشه با مثال بخوام بگم فرض کنیم فیچر مپ ۲۰۴۸۳۲۳۲ باشه در این صورت در مرحله anchor generator چند انکر تولید میشه N323299216 ۹ تعداد انکر های مرجع ایا درست متوجه شدم بعد از یک لایه conv 33دیگه فیچر مپ میانی تولید میشه فرض کنیم فیچر مپ میانی مثلا 5121616 باشه در این صورت با توجه به اینکه هدف اولین conv11 اینه که مشخص کنه در هر کدوم از انکر ها چقدر احتمال داره که شی وجود داشته باشه چجوری میشه در فیچر مپ میانی در نهایت9 16162304 داریم خیلی سردر گم شدم در اینکه چجوری بین این دو قسمت ارتباط برقرار می کنه در حالیکه تعدادشون ظاهرا با هم همخوانی نداره البته شما گفتین که هر دو N هستند ولی متوجه نمیشم چطور ممکنه این اتفاق بیوفته و یک سوال دیگه اینکه به نظر شما چرا cls scre ها بعد از conv33بدست میان و چرا روی همون فیچر مپ اصلی اعمال نمیشن لطفا راهنماییم کنید خیلی گیج شدم
-> دوستان کسی هست در مورد جزئیات مراحل faster rcnn اطلاعات داشته باشه و منو راهنمایی کنه ممنونم میشم کمکم کنید
-> سلام برای اینکه به جواب چنین سوالهایی برسید یا باید یک پیادهسازی from scratch ببینید یا باید سورس کدهای فستر رو در پایتورچ بخونید سورس کدهای فستر بهتر هست درمورد انکورها به تعداد مثلا 9k رسیدید درنظر داشته باشید که مراحل post processing داریم که من اینجا نشون ندادم و با اونها تعداد باکسها کاهش پیدا میکنن
-> خییلی ممنونم از راهنمایی تون خدا خیرتون بده صفحه پایتوچ رو دیدم ولی راستش خیلی متوجه نمیشم و نمی تونم کد رو تحلیل کنم احتمالا اگر این کار رو یاد بگیریم خیلی جاها بهمون کمک میکنه ایا هیچ اموزشی در مورد روش تحلیل و بررسی کدهای منبع یا کدهایی که در گیت هاب هست وجود داره یه قلقی چیزی که وقتی کد رو می بینیم نترسیم
-> 
-> 
-> سلام استاد وقت بخیر اجازه هست مقاله خودم رو که به تازگی چاپ شده در گروه به نشر بگذارم تا اگر کسی مایل بود لطف کنه و مطالعه کنن
-> سلام بله حتما تبریک میگم
-> ممنونم
-> عالی و امیدبخش بود یک دنیا ممنون از راهنمایی همراهی و پاسخ گویی تون"
"-> دوستان ببخشید اخیرا کسی به این مشکل نخورده الان فقط دارم ی عکس میخونم و نمایشش میدم ولی هیچی تو خروجی نمیاد
-> سلامبااستفاده از این چک کنید pltshow
-> زدم اینو کار نکرد
-> هر دوش باشه ها
-> اره هر دوش
-> الان مثلا این ی کد دیگس عین همون نوشتم ولی اوکیه
-> عجبمن بعضی وقتا ها که اینطوری میشه کلا runtime رو میبندم و رفرش میکنم از اول کانکت میشم اوکی میشهولی خب دلیلش نمیدونم معمولا با اون pltshow اوکی میشد
-> ممنون تشکر کلا با ی کتابخونه دیگه میرم جلو"
"-> سام التمن خواستار مقررات AI در جلسه سنا التمن در شهادت خود به پتانسیل هوش مصنوعی برای ایجاد و نابودی شغل اذعان کرد او پیشنهاد ایجاد یک اژانس برای نظارت بر توسعه مدل های هوش مصنوعی در مقیاس بزرگ و تعیین مقررات ایمنی و الزامات ازمایش را پیشنهاد کرد در نظر سنجی Generative AI شرکت کنید
-> سلام دیگه هوش مصنوعی به جایی رسیده که نیاز به آژانس قانون و مقررات احساس میشه ممنون از شما و سایر دوستان که اخبار روز رو اینجا به اشتراک میگذارید
-> سلام خیلی هم خوب ممنون از شما و سایر دوستان که اخبار روز رو اینجا به اشتراک میگذارید"
"-> راهی هست داده هایی که توی دیتاست outlier تشخیص داده شده رو توی دیتاست مارکش کنم مثلارنگش عوض کنم
-> اگه درست متوجه شده باشم سوال مربوط به کدنویسی هست گوگل کنید
-> سلام استاد خسته نباشید وقتتون بخیر استاد ببخشید من یک مدل یولو رو ترین کردم الان میخوام استفاده کنم احیانا شما میدونید تو کدوم پوشه میره مدل ترین شده
-> پوریا
-> 
-> درود توی کامندی که میزنی خودت خروجی رو تعیین میکنی
-> سلام پوریا جانفکر کنمcontentyolov5runstrainexpweights توی چنین پوشه های دنبالش بگرداخرین وزن هایی که مدل باهاش ترین شده ذخیره میشد
-> اره دنبال این مسیر میگشتم یادم نمیومد"
"-> اگه دنبال سلسله مراتب هستید تصویر بالا مربوط به دوره یادگیری ماشین ما هست که شامل یک سلسله مراتب کلی برای تحلیل و آماده سازی داده هست اما نکاتی که باید مدنظر داشته باشید این یک پروسه پیشنهادی هست و اینطور نیست که وحی منزل باشه میتونه تغییر کنه مثلا یک مورد رو هم کتاب Hands on ML پیشنهاد کرده فکر کنم در ورژن سوم ضمیمه A در تصویر بالا گفتیم که اول اوتلایرها رو مدیریت کن و بعد نرمالیزه رو انجام بده اما میتونه برعکس هم انجام بشه کتاب Hands on Data Preprocessing میگه که اگه به این نتیجه رسیدید که حذف اوتلایرها کار درستی هست بعد از نرمالیزه انجام بدید متنش رو در پیام بعدی میفرستم
-> As to when and whether you should adopt the approach of removing data objects due to being outliers I would like to share with you an important word of advice First only apply this approach to the preprocessed version of the dataset that youve created for the specific analysis and not to the source data The fact that this analysis needed the data objects with outliers to be removed does not mean all the analysis will need that Second make it a priority to inform the audience of the resulting analytic as they will be aware of this invasive approach in dealing with outliers
-> ممنون از توضیح هاتون
-> این دوره ای که فرمودید رو از کجا میشه تهیه کنیم
-> 
-> سپاسگزارم
-> این دوره به پایان رسید استاد
-> نه انشالله آخرای خرداد تموم میشه حدود 75 ساعت آموزش ازش منتشر شده
-> بی صبرانه منتظر بخش یادگیری تقویتی هستیم استاد ببخشید تو دوره یادگیری عمیق ی عنوان هدیه هم نوشته شده تو آخرین بخش اون کی منتشر میشه
-> بی صبرانه منتظر نباش مطالبی که تا اینجا گفتیم مهمتر از تقویتی هست بعد از یادگیری ماشین و یادگیری عمیق در شما تغییرات مثبت دیدم ادامه بدید تا به سطح مطلوب برسید اون هم تقویتی هست احتمالا همزمان با تقویتی یادگیری ماشین تقویتی یادگیری عمیق هم منتشر میشه
-> سلام چند روز پیش تو استوریهای اینستاتون فکر کنم یه افزونهای معرفی کردید برای ذخیره صفحات وب یا یه همچین چیزی یه بار دیگه اسمشو میگید لطفا
-> استاد وی پی ان تون رو هم بگید لطفا 2 هفتس نتونستم برم اینستا
-> Notion
-> Lite از باباها بپرسید خوباش دست اونهاست
-> دقیقااا"
"-> سلام دوستان میخواستم بدونم بعد از اموزش داده در ماشین لرنینگ و ذخیره مدل با pickle چطوری میتونم از مدل ذخیره شده استفاده کنم یعنی بهش ورودی بدم و خروجی رو ازش بگیرم
-> سایکیت
-> بله
-> 
-> ممنون از توضیح هاتون خیلی لطف کردید"
"-> سلام دوستان یه اموزش شسته رفته latex کسی سراغ داره
-> نگارش فارسی یا انگلیسی البته فرق هم نداره دوره های دکتر مس فروش تو یوتیوب رایگان هستش دوره آقای کلامی هریس در فرا درس و بعدش دیگه خودتون میتونین هندل کنین
-> سلام چی کار میخوان انجام بدین
-> نوشتن مقاله و گزارش به انگلیسی
-> به نظر من برای شروع از تمپلیت های سایت overleaf استفاده کنین چون چیز زیادی لازم نیست یاد داشته باشین در کنارش همون طور که دوستمون گفتن آموزش های رایگان دکتر مس فروش تو آپارات و یوتیوب هم خوبه ولی با هر چیزی بخوان با سرچ راحت جوابش بدست میاد و زیاد لازم نیست آموزش ببینین
-> اگر یک فایل pdf هم باشه که کلیت رو گفته باشه کفایت میکنه نا آشنا نیستم با latex ولی خیلی از دستور ها یادم رفته
-> همین رو به chatgpt گفتید من این تیپ مسائل رو از اون میخوام و اون هم خروجی خوبی میده"
"-> این که هردو یکسان است یک مقدار شک داره
-> روش kfold پیاده کن تا بیشتر مطمئن بشی
-> برای چیه این روش
-> چندین بار داده تست و ترین میسازه و آموزش میده"
"-> سلام دوستان من یک سری داده سری زمانی دارم که این داده ها رو اول window روش زدم بعد sample های که داشتم رو shuffle کردم بعد داده های ترین و ولیدیشن رو جدا کردم در جواب دقت ترین و ولیدیشنم تقریبا یکسان در میاد این موضوع ممکن به علت نشتی اطلاعات باشه اگه کسی پیشنهادی داره ممنون میشم بگه
-> سلام وقتی داده ها رو از یک ممنبع یکسان ساختتید به صورت تصادفی ام انتخاب کردید و در موقع اموزش داده ولیدیشن رو کنار گذاشتید چرا باید نشت صورت بگیره
-> البته باید دقت را هم بگید شاید گلا شبکه underfit کرده
-> گفتم شاید روشم برا جدا کردن داده های ترین و ولیدیشن درست نیست و باید قبل از ویندو کردن جداسازی رو انجام بده
-> 85 درصد
-> نه ربطی نداره پنجره انداختن رو سیگنال شاید حذف اطلاعات بکنه ولی نشتی نداره
-> سلام وقتی window میزنین هر دیتا تبدیل به چند تا دیتا میشه و بعد موقع جدا کردن ترین و تست ممکنه یک سری از اون چند تا توی ترین بیفتن و چند تا توی تست این نشت اطلاعات به نظرتون نیست مخصوصا اگه اون داده سری زمانی حالت repetitive داشته باشه
-> اگر اورلپ باشه بین پنجره ها بله
-> اگر اورلپ نداشته باشه چی"
"-> سلام خدمت استاد عزیز یه سوال داشتم اینکه ما توی پایچارم براحتی فایلهای زیادی توی یک پوشه داریم و در قالب یک فایل main برنامه اصلی را می نویسیم ولی توی کولب آیا باید همه فایلهای مرتبط با فایل اصلی باید در یک صفحه نوشته و اجرا بشوند یا کولب هم همین امکان را فراهم میکند اگر اینگونه هست میشه راهنمایی کنید ممنون میشم
-> سلام تا قبل از اینکه استاد پیام شما رو ببینه بله میتونید توی کولب هم فایل های مجزای زیادی داشته باشید که در قالب تابع یا کلاس به فایل اصلی jupyter شما سرویس بده فقط باید اون فایل یا کلاس های دیگه بصورت فایل پایتون ساده نه jupyter باشه که بتونید راحت import کنید و از توابع و کلاس ها استفاده کنید
-> ممنونم از پاسخ شما دوست عزیز پس این نکته رو رعایت کنم فایلهای مجزا با پسوند py و فایل اصلی با پسوندipynb باشند
-> خواهش میکنم دقیقا همینطوره"
"-> سلام دوستان کسی راجب Physicsguided neural network اطلاعاتی داره یا قبل موضوع ای مشابه ای این رو کار کرده
-> من فقط یه مقاله دیدم برای یه ایرانی بود استاد ریاضی بود یه جایی Physic based deep learning تایتلی شبیه این داشت مقدمش رو خوندم و فهمیدم من برای کارای ریاضی ساخته نشدم
-> یه چیزی هست به اسم model based deep learning مفهومش خیلی شبیه به موضوع شماست"
"-> فهمیدی مشکل من کجاست دیگه من بعضی از یوزرام فقط یه دونه ایتم دارن درحالیکه من برای هرکدوم حداقل دوتا احتیاج دارم که بتونم هم برای تست و هم برای ترین درنظر بگیرم
-> الان مشکلش چیه یک سوم یوزرا فقط یک نوع پستو لایک کردن به نظرم من چون تا حالا سیستم ریکامندر کار نکردم اصلا صورت مسئله رو هم هنوز نمیفهمم طی پروژه باید یاد گرفت
-> اره درسته کار نکرده باشی سخته فهمیدنش ممنون به هرحال"
"-> این هفت روشو بخونید
-> ممنون این ها برای پر کردن فیچر های دیتاست های جدولیه من یه سناریوی recommender sys دارم که تعداد آیتم هام برای بعضی یوزرها کمه
-> من متوجه صورت مسئله سوالتون نمیشم میشه مثال بزنید
-> یه سری یوزر دارم که هر کدوم مرتبطن به یه تعدادی آیتمفرض کن کالاهایی که خریدن یا پست هایی که لایک کردن و مساله اینه که میخوام ایتم های جدید رو برای یوزر پیشنهاد بدم مدل اینجوری آموزش میبینه 80 درصد آیتم های هر یوزر رو برای آموزش و 20 درصد باقی مونده رو برای تست در نظر میگیره حالا مشکلم اینه یک سوم یوزرهام فقط به یه دونه آیتم مرتبطن و من نمیدونم چجوری این ها رو هندل کنم
-> یعنی برای مثال از ده تا پست مختلف یه سری یوزرها یک نوع پست خاص رو لایک کردن
-> بله
-> من تاحالا ریکامندر سیستم کار نکردم گرچه کلیتش میدونم اما سوال خوبی پرسیدید دلیلی شد که در موردش مطالعه کنم
-> حالا من یه سوال بپرسم نظرتو بهم بگو لطفا"
"-> سلام به همگی من یه دیتایی دارم که قصد دارم یه مدل rs رو روش اجرا کنم این مدل میاد از بین آیتم های هر یوزر یه تعدادی رو برای آموزش و بفیه رو برای تست در نظر میگیره و حالا مشکلی که من دارم اینه که یه تعداد قابل توجهی از یوزرهای من فقط یک آیتم دارن میشه لطفا اگر نظری یا راهکاری به نظرتون میرسه بفرمایید ممنون
-> منیج فیچرهای میس شده خودش یه مبحثه کورس ماشین لرنینگ کگل یه مبحثش همین بود چند تا رویکرد داره"
"-> میخواید باmlحلش کنیدیعنی الگوریتمهای غیر شبکه عصبی فک کنم راحته
-> نه لزوما هر عملیات ریاضی که بشه این دوتا رو ترکیبشون کرد
-> خب ماشین لرنینگ بیس تئوریش همش ریاضیه ولی برای کار شما شما قراره فقط استفاده کنید از الگوریتما اما بیس تیوریش کلا جالبتره"
"-> من درواقع دو تا معیار مشابهت شیمیایی رو برای هر جفت دارو محاسبه میکنم حالا میخوام این دوتارو یه جوری ترکیب کنم و یه عدد برای شباهتشون محاسبه کنم
-> خب یه مسله ماشین لرنینگه آخرش منحنی خط یا صفحه بسته به پیچیدگی مسله پیدا میشه که بیشترین فیت رو برای اکثر داده ها داره
-> کانکت کنید ارایه هارو
-> خبری از ارایه نیست برای هر جفت دارو دوتا عدد اعشاری محاسبه میکنم فقط
-> هر داده رو از ۲ لایه متفاوت دنس انتقال بدید و خورجی اونارو کانکت کنید"
"-> دوستان سلام من سعی دارم میزان شباهت یه سری جفت آیتم رو بر اساس دو معیار متفاوت محاسبه و اون دو مقدار رو به نحوی ترکیب کنم که هر دو به طور موثری لحاظ بشن پیشنهادی داری برای این کار
-> یعنی میخواید به هر معیار شباهت یک وزن w1w2 بدید دو تاwتون مجهوله شما میخواید دو تاw رو پیدا کنید و معادله ی خط یا منحنی یا صفحهای که بهترین fit رو داشته باشه بنویسید"
"-> متوجه نمیشم الان چه تغییری ایجاد کنم تا این ارورر و حل کنم
-> اول از mat1 و mat2 یه دونه shape بگیرید تا بدونید دقیقا الان چی هستند ایا اونی که باید باشند هستند یا نه
-> البته ابعاد رو در کنار خود خطا نوشته ولی با دستور بهتره"
"-> سلام دوستان وقت بخیر کسی دیتا اگمنتیشن با داده های سری زمانی کار کرده
-> سلام من کار تخصصی انجام ندادم قبلا مشاوره یک شخص رو داشتم اون موقع دنبال روشهایی برای آگمنت بودیم اما روشها و لایبرریهای چندان رایجی وجود نداشت چند موردی رو دیدیم و کار کردیم اما قابل قیاس با مواردی مثل بینایی کامپیوتر نبود درهرصورت ممکن هست اطلاعات من محدود باشه
-> ممنون از پاسختون من از طریق chat gpt تونستم با کتابخانه پایتورچ این کارو بکنم ولی در قسمت validition به خاط این که ابعاد داده های ترین و تستم فرق میکرد به خطا خوردم"
"-> ممنون از شما برای تشخیص گوینده چکار میتوان کرد ایا مدلهایی مثل whisper هم از روش مشابه شما استفاده میکنند از چه برنامه ای برای تگ زدن استفاده میکنید
-> مدلهای مختلفی هست مدلهایی مثل ویسپر هم از همون ترنسفرمورها دارن استفاده میکنن ما ترکیب چند الگوریتم رو داریم استفاده میکنیم البته از ترنسفرمرها هم استفاده میکنیم اما تجربه به ما نشون داده که چه در صوت چه در متن ترنسفرمرها صددرصد نمیتونن نیاز رو برطرف کنن و ناچاریم از الگوریتمهای دیگه هم استفاده کنیم
-> "
"-> سلام جناب مهندس وقت بخیر میشه لطفا راهنمایی بفرمایید که داده های ویس را چطوری لیبل گذاری میکنند اگر داده ها تلفیق فارسی و انگلیسی باشه باید در دو شبکه مختلف ترین کرد برای تشخیص گوینده چکار میتوان کرد سپاس
-> سلام و درود ما داده ها عبارت عبارت آماده کردیم اگر برای بازشناسی گفتار منظورتون هست که لیبل داده ها همون متن صوتها هست تلفیق فارسی و انگلیسی هم با رویکرد Seq by seq موردی نخواهد داشت"
"-> سلام آیا میانگین گرفتن در راستای عمق فیچرمپها و تبدیلشون به یک صفحه برای مشاهدهی این که مدل کجاها بیشتر ویژگی استخراج کرده راه مناسبی هست مثلا اگه یه فیچرمپ ۱۴ ۱۴ ۲۰۴۸ داشته باشیم بیایم و در عمق میانگین بگیریم که میشه یه تصویر ۱۴ ۱۴ و بعد نمایشش بدیم من میخوام خروجی چندتا شبکه کانولوشنی مختلف رو با هم مقایسه کنم که ابعادشون هم فرق میکنه مثلا یکی ۷ ۷ ۲۰۴۸ هست و یکی دیگه ۱۴ ۱۴ ۲۰۴۸ بهترین راه برای مقایسه این دوتا فیچرمپ چی هست
-> اگر لینک یا پیجی رو هم میشناسین ممنون میشم بفرستین
-> بله کاری که میخواید بکنید منطقی و مرسوم هست در واقع میخواید هیت مپ بکشید میتونید هست مپ رو روی تصویر ورودی بندازید تا دقیقا مشخص بشه که شبکه به کدوم نواحی توجه بیشتری داشته مقایسه شبکهها با هیت مپ هم یک فاکتور ارزیابی خوب هست چون سایز فیچرمپها کوچک هست ما معمولا با درونیابی ریسایز تصویر ابعاد رو کمی بزرگتر میکنیم میشه تا مثلا 64 در 64 بزرگش کرد منبع زیاد هست ولی من الان منبعی ندارم CNN Heatmap visualization
-> خیلی ممنون آقای دکتر
-> برای هیت مپ فکر کنم Lime هم داشته باشدش"
"-> سلام دوستان یه سوال برام پیش اومده اگه یه شبکه رزنت ۵۰ داشته باشیم تصویر ورودی مون ۴۴۸۴۴۸ باشه فیچرمپ خروجی مون میشه ۱۴۱۴ یعنی گام شبکه ۳۲ هست آیا این به این معنی هست که هر پیکسل از فیچرمپ نهایی شامل ویژگیهای یه ناحیه است فقط یعنی یک ناحیه ۳۲۳۲ آیا این تصویری که درست کردم درسته یعنی هر پیکسل در فیچرمپ خروجی فقط ویژگیهای ناحیهی جلوشو داره
-> این تصویر بیشتر نشون میده که شما فیچر مپ لایه آخر رو درآوردی معمولا فیچر مپ همون خروجی لایه میشه بعد از اعمال فیلتر ها به ریسپتیو فیلد حالا شما فرض کن ۱۵ تا فیلتر اعمال میکنی این ۱۵ تا فیلتر کل فضای عکس رو پوشش میده اما بخشی ار فیچرها در هر لایه بیرون میاد مثلا در لایه اول فیچر مپ همون لبه های عکس کادر دور صورت و چشمها وایناست این تصور که فیچر مپ بخشی از ورودی رو پوشش میده در هر لایه درسته
-> پیشنهاد میکنم این مقاله رو ببینی
-> مچکر
-> نه"
"-> سلام دوستان بهتری روش برای پیدا کردن out lier ها چی هیت و بعد از پیدا کردنشون بهتره اونها رو نادیده بگیریم و پاک کنیم یا راه بهتری پیشنهاد میدید
-> بستگی به مسئله شما داره گاها وجود اوتلیر بد نیست
-> از چه نظر میتونه بد نباشه امکانش هست یکم بیشتر توضیح بدید لطفا
-> داده پرت همیشه بد نیست ممکنه داده ای باشه که ما با مطالعه مشخصات اون داده به چیز جدیدی دست پیدا کنیم مثلا تصور کنید نمره دانش آموزی میشه ۱۹ در صورتی که میانگین نمرات همه دانش آموزا بین ۱۲ تا ۱۳ هست آیا اون دانش آموزی که نمرهش شده ۱۹ باید حذف بشه باید مطالعه کنیم شاید بشه کاری کرد که همه دانش آموزا بتونن مثل اون دانش آموز خوبه بشن در کل بستگی به مسئله داره به نظرم داده ای که نویز نداشته باشه یه جای کارش میلنگه اگر بفهمیم چرا اوتلیر اتفاق افتاده شاید مارو به چیز جدیدی برسونه
-> ممنون متوجه شدم لطف کردید
-> درود"
"-> البته این کلی بود میتونید یکthreshold براش در نظر بگیرید و با توجه به اون پرینت کنید خروجیو
-> ممنونم"
"-> منظورم داده های عددی بود تصاویر را به عدد تبدیل کنید و در گروه بذارید لطفا
-> دیتاست اصلا تصویر نیست فکر کنم بد توضیح دادم دیتاست عدد هست
-> این خروجی متد describe هست روی دیتاستی که دارم ولی خروجی اینمتد با خروجی متد مثلا min فرق داره"
"-> سلام دوستان من یه دیتاست عددی دارم وقتی که دستور describe رو اجرا میکنم یکی از ستونهاش در بخش max یه عددی نشون میده که اون داده واقعا ماکزیمم اون ستون نیست یعنی عدد بزرگتر از اونم وجود داره دلیلش چی میتونه باشه
-> سلام وقت بخیر داده های اون ستون را که سورت میکنید چند تا عدد بیشتر از اون هست لطفا عکس سورت شده را در گروه بذارین
-> بله حتما ممنون
-> 
-> 
-> 
-> 
-> سلام این تصاویر داده های عددیتون هست
-> بله"
"-> سلام آیا میشه مدل ساخته شده رو از اول train نگرد و بهش داده اضافه کرد یعنی retrain کرد مدل رو
-> سلام اگر وزن های آموزش دیده مدل را داشته باشید میتوانید ادامه آموزش آن را با داده های جدید انجام دهید
-> مرسی بله دارم میخونم ولی نمیدونم رو مدلی کع ذخیره میکنیم تو FRCCN چه طور میشه اینکارو کرد
-> به نظرتون accuracy مدلی که اینحوری ترین میشه با اینکه از اول همه دیتا ها ترین شن فرق داره
-> بستگی به داده هاتون داره"
"-> سلام دوستان یه سوال داشتم در مورد پروژه های ماشین لرنینگ فرض کنیم یک پروژه برای تشخیص بیماری تعریف شده است و کل موارد و مسائل و مراحل train test و پروژه و انجام شده است و به مدل ما موفق شده به دقتی که مد نظر ما است برسد حالا سوال پیش میاد این مدل طراحی شده در همون محیط ازمایشگاه تشخیص بیماری چطور میتونیم ازش استفاده کنیم مثلا این مدل چطور میتونه بصورت واقعی شروع به کار کند ممنون میشم راهنمایی کنید سپاس
-> سلام بالاخره شما یه مدل رو با یه فیچر هایی آموزش دادید چه داده عددی مانند سن و نتایج آزمایش و چه تصاویر پزشکی در فاز اجرا هم از بیمارتون همون اطلاعات و فیچرها رو جمع آوری میکنید و با مدلی که قبلا آموزش دادید تست میکنید
-> نه منظورم اینه مدل کامل طراحی شده و تمام شده حالا این باید در قالب یه نرم افزار یا چیزی شبیه به این باشه دیگه نه اینو نمیدونم باید چطور بنویسمش که از اون مدل بتونم استفاده کنم چون اون نرم افزار که قرار نیست train یا test کنه قرار هست اطلاعات رو بگیره و جواب بده حالا من توی پروژه باید مدل یا چی رو ذخیره کنم یا اصلا چکار باید کنماینو نمیدونم
-> سلام مدل رو وزن هاشو سیو کنید و یک رابط گرافیکی خیلی user friendly براش بسازید ورودیتون اگر عکس هست یک قسمت براش ایجاد کنید و وقتی فرد عکس رو داد پیش پردازش رو روش انجتم بدید و به مدل بدید خروجیش هم بجای ۰۱ متن بنویسید براش مثال If output0 PrintActive Else"
"-> پرامپت تست فارسی که با chatgpt انجام دادم البته برای انگلیسی کاملتر و دقیقتر هست
-> عالی از چه مدل و دیتاستی استفاده کردین"
"-> GitHub wong2chatgptgoogleextension A browser extension to display ChatGPT response alongside search engine results
-> سلام ممنون این افزونه نیاز به ثبت کردن شماره هم داره درسته شماره ایران رو قبول نمیکنه
-> سلام بله باید در خود سایت openai ثبت نام کرد"
"-> سلام وقت همه بخیر من یک CNN دارم که تصاویر رو به سه کلاس دسته بندی میکنه میشه شاخص AUC رو براش حساب کرد اگر میشه ممنون میشم راهنمایی کنید تو پایتون چطور این کار رو بکنم
-> سلام به سایکیتلرن نگاه بندازید اینها رو آماده داره
-> ممنون استاد بدست اومد تشکر استاد من شبکه inception_v3 رو چند ماه پیش اجرا کردم و پارامتراشو ذخیره کردم الان دوباره میخوام بسازمش و پارامتراشو لود کنم ولی دستور ساخت inception_v3 که قبلا مینوشتم الان ارور میده دستور این بود Torchvisionmodelsinception_v3pretrainedTrueaux_logitFalse الان خطا میزنه __ init__ got an unexpected keyword argument aux_logit انگار داکیومنت پایتورچ عوض شده الان هر کاری میکنم هم با اون شبکه قبلی ست نمیشه که پارامترارو لود کنم میتونید راهنمایی کنید
-> تغییراتی در نسخه جدید و ورودی مدلها داشتیم باید به داکیومنتشون در پایتورچ نگاه کنید"
"-> سلام ASUS TUF Dash FX517ZR 156 Inches Hard Disk Size512 GB CPU Model Core i7 Ram Memory Installed Size16 GB Operating SystemWindows 11 Home Card Description Dedicated Graphics Coprocessor NVIDIA GeForce RTX 3070 CPU Speed 23 GHz از بین لپتاپهای ایسوس کدوم رو برای دیپ لرنینگ پیشنهاد میکنید
-> Gpu ram8"
"-> 
-> عرض سلام و ادب خدمت همه بزرگواران آیا کسی با این دیتا کار کرده"
"-> سلام دوستان یه سوال داشتم در مورد روال پروژه های واقعی در صنعت میخواستم بدون وظایف افراد زیر در یک پروژه واقعی صنعتی چطوری هست متخصص دیتا ساینش متخصص ماشین لرنینگ و احتمالا کسی که یادگیری عمیق انجام بده و همچنین کدوم یکی ار متخصص ها الگوریتم جدید یا مدل جدید ابدا میکنند البته در صورت نیاز فرض کنید موضوع پروژه هم پیش بینی اب و هوا و میزان بارندگی در 20 سال اینده است شاید سوالم یکم بچگانه بنظر بیاد ولی نوع همکاری بین اعضای یک تیم پروژه هوش مصنوعی رو نمیدونم ممنون میشم راهنمایی کنید ممنونم
-> سلام وقت بخیر بطور خیلی ساده و خلاصه Data scientist مقالات روز و بخش تکنیکال و تئوری را هندل میکند Data Engineer قسمت توسعه کدنویسی ML یا DL در فریمورک مربوطه را انجام میدهد MLOPS نگهداری کد قسمت هوش مصنوعی پروژه و چرخه عمر آن را مدیریت میکند
-> سلام ممنونم"
"-> دیتا فریمتون رو میشه دید
-> بله"
"-> سلام دوستان Attentional Graph Neutral Network اگر بخواین در متن فارسی بیارید چی ترجمه می کنید من یکسری کلمات مفهومش میدونم اما نمیدونم با چه کلماتی ترجمه کنم بهتره مثل Ground truth
-> من خودم شبکه های عصبی گرافی مبتنی بر مکانیزم توجه ترجمه ش کردم
-> سپاس از پاسخگویی تون
-> اگر چیز دیگه ای پیدا کردی به منم بگو لطفا
-> بله حتما"
"-> سلام به همگی وقت بخیر کدوم PC رو پیشنهاد میدهید 1 380GHz AMD Ryzen 7 5700G processor along with 16GB 2 x 8GB of HyperX DDR43200 512GB PCIe NVMe M2 SSD NVIDIA GeForce RTX 3070 graphics card with 8GB of GDDR6 2 12th Gen Intel Ci512400F processor with 16GB of DDR4 3200 MHz 1TB 7200RPM hard drive 512 SSD NVIDIA RTX 3060 graphics with dedicated graphic card and 12GB VRAM 3 38GHz AMD Ryzen 7 5700G octacore processor 20MB Cache up to 46 GHz and 16GB DDR4 RAM 512GB M2 NVMe PCIe 30 SSD NVIDIA GeForce RTX 3070 graphics card with 8GB DDR6
-> سلام نمیدونم نمیتونم نظر تخصصی بدم هرسه خوب به نظر میرسن فکر کنم تو گروههای مختلف یادگیری عمیق و هوش مصنوعی این رو مطرح کنید بهتر باشه
-> تشکر اگر ملاک انتخاب بین این ۳ گزینه باشه برای ترین شبکههای عمیق کدوم رو انتخاب میکنید
-> نمیدونم باید جی پی یو هاشون مثلا بررسی بشن و
-> گزینه ی۲ برای ترین فکر کنم بهتر باشه چون vram بیشتری داره"
"-> سلام دوستان این مدل کلاسیفیکیشن loss کم میشه اما accuracyاصلا تغییر نمیکنه کسی به این مشکل برخورده
-> سلام اور فيت شده"
"-> استاد سلام اگر در مدلی دقت مجموعه داده validation خیلی بهتر از دقت تست باشه آیا این به معنی overfittng هست و راهکار چیه آیا باید میزان داده valid رو افزایش بدیم یا باید مدل را تغییراتی بدیم که از overfitting جلوگیری بشه ممنون میشم راهنمایی کنید
-> ولید و تست اصلا از روی این دو تا نمیشه فهمید که اورفیت شده
-> پس چه کاری باید انجام بدم راهنمایی میکنید ممنونم
-> باید نمودار ترین و ولید رو باهم مقایسه کنید اورفیت حین ترین شبکه رخ میده
-> خيلی ممنون
-> ببخشید استاد توی این مقالهتون چجوری mAP رو برای این تسک multiclass classification محاسبه کردین
-> سلام وقتی ولیدیشن خیلی خوب هست ولی دقت تست پایینه چه نتیجهای میگیریم
-> چندتا احتمال وجود داره
-> هیچ نتیجهای سوالتون شبیه این هست که یک نفر بگه اگه گلوم درد بکنه مشکل چی میتونه باشه ما هیچ شناختی از مساله نداریم و در این شرایط نمیشه نظر داد صرفا با یک جمله نمیشه اظهار نظر کرد دلایل زیادی وجود داره که میتونه منجر به این اتفاق بشه"
"-> سلام دوستان کسی از mean average precision میدونه چجوری باید توی کلسیفیکیشن استفاده کنیم توی مقالات کلسیفیکیشن میبینم ولی هرچی میگردم توی نت کدی ازش پیدا نمیکنم چون همهشون در مورد ابجکت دیتکشن هست فقط
-> سلام سایکیت لرن داره
-> این رو میفرمایید درسته این رو من تست کردم ولی برای binary و multilabel classification هست ولی من برای multiclass classification میخواستم
-> همینه دوست من شما فقط ground truth و مقدار پیش بینی شده رو به این تابع میدین و میانگین precision رو بهتون میده
-> خوب این برای باینری کلسیفیکیشن هست من الان ۴۰ تا کلاس دارم اینو باید چجوری حساب کنم"
"-> سلام بدون آنتی فیلتر راهی برای رفتن به سایت هوسم هست
-> سلام با تغییر dns های خودتون به گوگل یا یاهو و با نرم افزار تغییر DNS هم میتونید راحت تر انجامش بدید مثلا برای گوگل به این شکل هست 8888 8844"
"-> سلام دوستان زمانی که ایپوک ها شروع میشه یه سری خروجی چاپ میشه مثل عکس زیر که ابتداش نوشته 33 این عدد معرف چی هستند
-> تعداد بچ داده ترین هست یعنی ۳ تا بچ شده داده ترین برای اموزش جدل که اگر اشتباه نکنم دیفالتش ۲۵ تاس یعنی ۷۵ تا داده ترینو در ۳ تا بچ ۲۵ تایی تقسیم کرده
-> قابل تنظیم هست
-> الان چرا 33 هست پس"
"-> ایا در اکادمی هوسم دوره ای وجود داره که کامل svm و یا nca رو توضیح داده باشه
-> سلام خیر"
"-> یعنی یک چیزی میخوام بهتون معرفی کنم که تو زندگی من نقطه عطفی بوده نرمافزار Mathpix میدونید چیه کافیه که یه اسکرینشات از فرمولتون بهش بدید و فرمول مناسب برای ورد لاتکس و ازش تحویل بگیرید ویدئو بالا رو ببینید
-> برای دور زدن محدودیت این برنامه راهی وجود داره
-> خیلی از این ابزارها برای دانشجوها دانشآموزها شرایط ویژه درنظر میگیرن اینجا تا 1000 عکس رو به دانشجوها رایگان میده اما باید از طریق ایمیل دانشگاهی ثبت نام کنید البته هزینه پولیش هم زیاد نیست چنین پرداختهایی رو احتمالا از طریق سایتهای واسط مثل ایرانی کارت میتونید انجام بدید
-> چرا ریاکشن تشکر رو برداشتین
-> کجا
-> 
-> این گروه اینو نداره
-> فکر کنم آپدیت نکردیم بعضیاش بعدا اضافه شدن الان درستش میکنیم
-> سلام استاد این پیام تقریبا دو سال پیش رو من الان حین سرچ توی گروه دیدم چقدر من دنبال همچین ابزاری بودم ممنون
-> سلام ایکاش بهت زودتر معرفی میکردم
-> و منی که الان به خاطر پیام شما دیدمش خیلی خوبه"
"-> سلام کدام قسمت از محتوی سایت هوسم در مورد Inseption resnet v2 صحبت کرده
-> سلام در هیچ کدوم از دورهها صحبت نکردیم این مدل محبوبیت چندانی نداشت و به همین خاطر جز طرح درس ما نبود
-> ممنونم استاد در مورد ماژول اینسپشن چطور
-> ماژول اینسپشن رو در جلسه چهارم یادگیری عمیق و جلسه دوم بینایی کامپیوتر گفتیم
-> باتشکر از شما"
"-> دوستان کسی میتونه اینو تحلیل کنه مدلی که داره استفاده میشه unet هست تعداد epoch های مختلفی تست کردم الان 200 تا هست
-> بیشتر توضیح بدید
-> اورفیت شده و هر چقدر با پارامتر ها بازی میکنم تغییری ایجاد نمیشه گفتم شاید دوستان بتونن راه کاری پیشنهاد بدهند نسبت به تصویر
-> نه منظورم این هست که در مورد دیتاست و مساله توضیح بدید مساله چيه دیتاست چند تا تصویر داره لرنینگ ریت چقدر هست بهینهساز اتلاف مقدار پارامترهای مدل هیستوری لاس فقط ایپوک 13 رو میبینیم الان چرا به اینها نیازه چون تک بعدی به قضیه نگاه نکنیم از روی نمودار میشه یه چیزایی رو حدس زد ولی نه همه چیز رو به نظرم این مدل اورفیت نیست تازه برای بررسی عملکرد مدل بهتر هست لاس رو نسبت به ایپوکها پلات کنید اطلاعات کم باعث میشه همینطوری یک کلمه بگیم اورفیت یا اندرفیت و شما بیشتر گمراه بشید مشخصههای نمودار شما لاس کم دقت کم نوسان زیاد زمان طولانی بدون هیچ پیشرفتی
-> تعداد 2000 تصویر مساله هم key point detection هست و learning_rate01 گذاشتم optimizers هم Adambeta_109999 loss binary_crossentropy
-> تعداد سمپلها قابل قبوله به نظرم نرخ یادگیری بزرگه و مناسب نیست نرخ یادگیری رو 01 001 0001 00001 بذارید و ببینید توی کدوم لاس سریعتر افت میکنه بتاهای بهینهساز رو تغییر ندید و بذارید دیفالت باشه در عین حال شاید اشتباه در رویکرد حل مساله هم داشته باشید حضور ذهن ندارم که برای این مساله از کراس آنتروپی استفاده میشه یا نه فرض رو بر این میذاریم که یک منبع به عنوان الگو دارید و طبق اون دارید پیش میرید
-> ممنونم"
"-> سلام استاد توی یکی از ویدیوها اجرای کدها در گیت هاب رو توضیح میدین یکی از بزرگترین چالش های من واقعا همینه مقالات متعدد رو میبینم اما وقتی به کد گیت هاب میرسم گیر میکنم استاد و مایوس میشم و بیخیال کار میشم ممنون میشم بگید راهکار چیه و چیکار باید بکنم
-> سلام چرا مایوس من نمیدونم چقدر برای این کار وقت گذاشتید مثلا شده یک پروژه گیتهاب رو انتخاب کنید و تا زمانی که اجرا نشده رهاش نکنید همه این مشکلات با تمرین کردن حل میشه یک پروژه گیتهاب رو انتخاب کنید و تا زمانیکه به اجرا نرسیدید دست برندارید همون مواردی که من در آموزشها گفتم رو انتخاب کنید به نظرم اجرای پروژههای گیتهاب دانش نیست و صرفا مهارته مهارت هم تنها از طریق تمرین و وقت زیاد حاصل میشه شما رو نمیدونم اما بسیاری از بچههایی که من باهاشون صحبت کردم اهل تمرین نیستن یعنی توی ویدئوها اگر a و b رو گفتیم و ازشون خواستیم c رو هم خودتون انجام بدید اصلا پی اون نمیرن
-> خیلی ممنون استاد پس فقط باید انقدر سر به سرش بزاریم تا اجرا بشه یه بحث دیگه درک خود کدی هست که برای پیاده سازی نوشتن بعضی جاها خیلی پیچیده ش میکنن واقعا و اصلا کامنت و توضیحات درست حسابی ندارن
-> بله باید زیاد براش وقت بذارید مثلا تا حالا شده برای یک کدی یک هفته هر روز 8 ساعت وقت بذارید من کدهای دیگران رو به دو شکل بررسی میکنم اول اینکه کدها رو همینطوری بدون اجرا میخونم و میفهمم اگر زیاد کار کنید و کد دیگران رو ببینید مهارت کدخونی پیدا میکنید یعنی بدون اجرای کدها متوجه میشید حدودی خروجی این خط چی هست اگر کد پیچیده باشه در پایچارم با تریس کردن و سرچ کردن بررسیش میکنم بعیده که تریس کنم و کد رو متوجه نشم
-> خیلی ممنونم استاد محبت کردید
-> سلام اقای دکتر ببخشید اگه کد یک مقالهای در گیت هاب نباشه از چه راههایی میشه گیرش آورد من خودم اگه ایمیلی داخل مقاله زده باشن که در اکثر موارد نزدن بهش ایمیل میزنم ولی هیچ کدوم شون تا حالا جواب ندادن حالا دوتا سوال برام پیش اومده که ۱ چجوری یه راه ارتباطی مثل ایمیل یا هرچی از نویسندهها گیر بیاریم ۲ آیا باید نگارش ایمیل مون سبک خاصی داشته باشه من مثلا میگم از فلان مقاله تون خوشم اومده و تزم در همین سمته و و اگه ممکنه کدتونو برام بفرستین که متاسفانه هیچ جوابی نگرفتم تا حالا ممنون میشم راهنمایی کنید با تشکر
-> سلام همه کارهایی که انجام دادید درست بوده اگه ایمیل افراد در مقاله نیست اسمشون رو کامل در اینترنت سرچ کنید و ببینید پیج شخصی دارن یا نه دیگه الان اکثرا پیج توییتر بلاگ شخصی و دارن احتمالا ایمیلشون رو در بلاگ شخصی ذکر کردن متن ایمیل هم در حد 2 الی 3 خط با محتوایی که گفتید اما معمولا این روش جوابگو نیست اگه مقاله پیاده سازی نداره یا پیاده سازی شبیه بهش که این مقاله از اونها الهام گرفته پیدا نمیشه اگه کسی دیگه این مقاله رو پیاده نکرده و اگه ایمیل زدید جواب ندادن دیگه قاعدتا خودتون باید پیاده سازیش کنید بعدش هم بذارید تو گیتهاب و استارشو بگیرید و لذت ببرید
-> سپاس"
"-> سلام دوستان کسی میتونه کمک کنه و این خطا را بر طرف کنم پروژه segmentaion هست و مدل هم Unet ولی نمیدونم این خطا رو چطور برطرفش کنم
-> سلام احتمال زیاد ساختار دادههای ورودی به صورت استاندار تعریف نشده
-> یعنی چی دقیقا متوجه نمیشم منظورتون"
"-> دوستان سلام زمانی که ما برای یه تعدادی کلاس فقط یک سمپل داشته باشیم و کلاس ها رو هم الزاما باید نگه داریم برای جدا سازی تست و ترین چیکار باید کرد همون یه دونه سمپل رو هم برای تست هم برای ترین قرار بدیم یا روش خاصی وجود داره
-> سلام ایدهای که به ذهن من میرسه میتونید داده مصنوعی نیز تولید کنید و اون یدونه رو مقداری بیشتر کنید میتونید یه نویز کوچکی به اون داده اضافه کنید و چند تا داده شبیه به اون بدست آورید کلیدواژههای Data augmentation و oversampling رو هم میتونید واسه تقویت داده و نمونه گیری سرچ کنید
-> ممنون
-> سلام استفاده یه داده هم تو ترین و هم تست نه درست نیست تا جایی که تو بعضی دیتاستهای مربوط به oneshot learing حتی همپوشانی کلاس مابین ترین و تست هم نباید باشهتو تسکهایclassagnostic اگه بتونین واسه کلاس مدنظرتون داده های خیلی نزدیک پیدا کنین اونوقت تکنیکهای oneshot learning مثل Siamese میتونه کمک کننده باشه ولی به هر حال برای تست باز هم باید یه دونه دیگه داده موجود باشهنتیجه غیرقابل اطمینان فک میکنم تو فرودگاهها برای شناسایی چهره از عکس شناسایی واسه finetunning یک شبکه اموزش دیده شده اسفاده میکنند بعد میان از تصویر زنده جهت تست استفاده میکنند
-> ممنون ببینید دیتای من درواقع اینتراکشن های یوزر آیتم در یه سیستم توصیه گره توی عمچین سناریوهایی اطلاع دارین چیکار باید کرد وقتی یه یوزر فقط یه دونه اینتراکشن داره"
"-> سلام ببخشید تو کد رتینا نت با FPN شما فرمودین که این ارور رو خودتون حل کنید و یک چالشی میشه براتون
-> سلام احتمالا قبلش توی یک ویدئوی دیگه مشابه این کار رو کردم یا احتمالا در موردش توضیح دادم چرا نتونستید ارور مشکل رو واضحا گفته ارور رو خوندید به نظرتون ارور چی میگه بگید کمکتون میکنم
-> ممنون لطفا بفرمایید اررور چی میگه
-> اینم خطا
-> 
-> درسته یعنی باید من به جای target خالی مولفه ی boxes این target رو به عنوان تنسور خالی تعریف کنم و اینکه در گروه بینایی عضو نیستم من
-> زمان و شماره ویدئو رو بگید که من چک کنم و مشکل رو بگم"
"-> دوستان مدلهای قوی cnn رو از کجا میتونم پیدا کنم مقاله بخصوصی اگر لیست کرده باشه بخصوص مدلهایی که یک بعدی هستند دنبال مدلی هستم که به عنوان پایه کارم ازش بتونم استفاده کنم
-> رزنت افیشنت نت paperswithcodecom"
"-> سلام دوستان سرور مطمئن و امن برای train کدام شرکت رو پیشنهاد میدید گوگل کلب متاسفانه بدلیل حجم بالای داده ها نمیتونم ازش استفاده کنم ممنون
-> سلام من چند روز پیش به این سه سایت برخورد کردم که خب داخل ایرانن یه نگاهی که انداختم سرور دانشگاه امیرکبیر قیمتش مناسبتر بود
-> ممنون از لطفتون
-> خواهش میکنم"
"-> سلام دوستان یه سری عکس داریم که روی اون عکس ها یه سری نقاط رو مشخص کردیم و فایل csv مربوط به نقاط را هم اماده کردم و ماسک تصاویر را هم ساختم و الان میتونم که سایر قسمت ها پروزه را بنویسم و ادامه بدم ولی یه سوالی دارم سوال این key point detection میشه یا segmentation و اینکه دقیقا چه تفاوتی بینشون هست سپاسگزارم
-> سلام وقت بخیر کار شما Key Point Detection هست تفاوت بین این دو نیز در این می باشد که در Key Point Detection نقاط مهم و تاثیر گذار موجود در تصویر که در پیش بینی موثر هستند را مشخص میکند مانند نوک بینی گوشه چشم و ابرو در صورت در Segmentation هم هر پیکسل تصویر به یک کلاس اختصاص داده میشود و نقاط کانتور یک شی را مشخص مینماید که انواع مختلفی دارد
-> سلام ممنون از توضیحتون"
"-> فک کنم معنی اوورترینینگ میده ولی نمیدونم چی کار میتونم بکنمش
-> سلام به نظر میرسه اورفیت شده باید از تکنیکهای جلوگیری از اورفیت استفاده کنید مثلا دیتا اگمنت رگولاریزیشن دراپاوت و غیره"
"-> سلام و عرض ادب يك مدل رو با minnaxscaler ترين كرديم براي استفاده از مدل با ورودي هاي جديد ايا ورودي مدل بايد اسكيل بشه يا داده واقعي وارد كنيم
-> سلام به صورت یک قاعده کلی هر آن چه که برای داده train اتفاق می افته برای داده تست هم اتفاق می افته و باید همون پیش پردازش هم روی داده تست صورت بپذیره اما برای داده تست نباید از متد fit استفاده بشه و صرفا باید از transform استفاده بشه تا data leakage اتفاق نیفته
-> ممنون مدل تست و ترين انجام شده با داده هاي اسكيل شده حالا كاربر ميخواد داده وارد كنه به predict ايا داده كاربر هم بايد با همون scaler اسكيل بشه يا نه
-> خواهش می کنم بله برای predict هم باید اسکیل کنید داده رو
-> سپاس بزرگوار
-> خواهش می کنم مهندس"
"-> سلام یه سوال داشتم در متلب ما براحتی structure تعریف میکنیم و زیرمجموعه های آن هم با یک دات قابل دسترس هستند آیا معتدل با آن در پایتون چیزی داریم البته من با چند لیست تو در تو تونستم برنامه متلب را به پایتونی تغییر بدهم ولی خواستم بدانم آیا معادلی دارد یا نه ممنون
-> به نظرم لیست یا دیکشنری میتونن جایگزین مناسبی برای استراکچر یا سلول در متلب باشن البته از کلاس در پایتون غافل نشید چیزی که معمولا در متلب کم کاربرده
-> ممنونم استاد"
"-> سلام ذدوستان من باید کد faster RCCN رو از طریق CMD ران کنم و کد من تو کلاب هست و تو نصب تورچ مشکل دارم ممنون میشم اگر کسی میتونه به من کمک کنه و بهم اموزش بده اگر مجازه تو این گروه حتی با هزینه کارم یکم گیر افتاده و خیلی برام حیاتی شده این قضیه
-> سلام برو آموزش نصب پایتورچ با آناکوندا رو بخون خیلی ساده و سر راسته و وقت زیادی ازت نمیگیره
-> نصب کردم اون قسمتو"
"-> سلام عکس تو دایرکتوری نیست احتمالا الگوی اسم عکس از وسط دیتاست عوض شدهمثلا یه نقطه یا یه حرف کم و زیاد داره
-> یعنی تو یک دونه عکس الگوی اسم گذاری عوض شده
-> ممکنه برای من کپی شده بود من دوتا از اون عکس داشتم
-> من خیلی چک کردم و این قضیه ای که شما میگید رو هم برخورد کردم ولی الان هرجوری هست من همچین چیزی پیدا نمیکنم"
"-> سلام و عرض ادب روز معلم رو به استاد عزیز تبریک عرض میکنم امیدوارم خودتون و دانشجوهاتون همیشه افتخارآفرینی کنید
-> سلام ممنون همچنین ممنون بابت دعای قشنگتون انشالله"
"-> ممنون میشم اگر تونستید کد رو چک کنید و بهم خبر بدید که من بتونم ازش استفاده کنم
-> سلام انشالله امروز چک میکنم
-> ممنون
-> گویا به خاطر تغییر نسخه پایتورچ مشکلاتی جزئی پیش اومده بود اصلاح کردم کدها الان کامل اجرا میشن
-> ممنون
-> سلام من نحوه ذخیره و ری لود کردن fasterrccnn رو تو پاستور نمیدونم ممنون میشم اگر امکانش هست راهنمایی کنید یا به کد اضافه کنید
-> سلام کد نمیتونم اضافه کنم ذخیره و لود مدل در دوره پایتورچ گفته شده در اینترنت هم مثل لینک زیر موجود هست و بسیار ساده هست
-> به خاطر تععیرات نسخه پاتورچ ارور No Cuda availible رو گد خیلی وقتا میده حس میکنم آیا درسته
-> ارتباطی نداره شاید کولب به خاطر استفاده بیش از حد جی پی یو ازتون میگیره
-> آخه با دو تا جی میل دارم کار میکنم که این ارور رو میده
-> لطفا عکس از خطا بفرستید دقیق ببینم"
"-> یک مشکل دیگه ای که هست اینکه بعضی قسمتهای برنامه اگر breakpoint بزارم و دیباگ کنم برنامه تا آخر اجرا میشه در حالیکه اگر نقطه توقف دو خط قبل بزارم توقف میکنه و خط به خط که اجرا کنم خط بخط اجرا میشه و مشکلی نداره احتمالش هست پایتون قاطی کرده باشه و لازم باشه کلا حذف و نصب بشههرچند کار سختیه
-> نه ايرادي در پايتون نيست محل breakpoint مهم است مثلا اگر در يك تابع شرطي if شما breakpoint را در محل else قرار دهيد درحالب كه statement if درست true باشد از breakpoint رد ميشود وتا انتها اجرا ميشود
-> دستور if هم نداریمداخل یک حلقه for قرار دارمدقیقا اگر بعد از اجرای دستورات زیر بزارم اجرا نمیشه With torchno_grad OuttorchclampmodelInoisy01 اینجا توقف نمیکنه Psnr
-> اون if فقط مثال بود عرض كردم يك اسكرين شات از پاي چارم بديد تا بهتر بشه بررسي كرد"
"-> سلام دوستان کسی تو ران کردن بسته faster RCNN تو قسمت backbone به این ارور برخورد نکرده Anchors should be TupleTupleint because each feature map could potentially have different sizes and aspect ratios There needs to be a match between the number of feature maps passed and the number of sizes aspect ratios specified
-> یک اسکرین شات از کدی که نوشتید بفرستید
-> 
-> اسکرین شات از کد ارور نه
-> همون کدی هست که تهیه کردم از وب سایت شما
-> دقیقا همون کد رو اجرا میکنید و چنین خطایی دریافت میکنید یا تغییراتی دادید
-> من فقط دیتا ست رو عوض کردم
-> پیشنهاد میکنم یک بار کد اصلی رو کامل اجرا کنید و ببینید خود کد اصلی بدون هیچ تغییری این خطا رو میده یا نه شاید به خاطر آپدیت پایتورچ نیاز باشه کد تغییر کنه اگر نیاز به تغییر هست اصلاحش کنیم
-> ممنون چک میکنم و اطلاع میدم
-> سلام من هم گاهی این ارور رو دریافت می کردم و دلیلش این بود anchor generator رو عوض می کردم
-> با سلام گد اصلی هم همین ارور رو من دریافت میکنم میشه چک کنید و به من بگید ایراد از کجاست چون من نیاز دارم ازش استفاده کنم"
"-> و اتفاقا برنامه نویسای بسیار بیشتری هم هست برای جنگو طبیعتا هرچقدر تخصصی تر بشه تعداد افراد هم کمتر میشه این رو هم در نظر داشته باشید
-> تقاضای نیروی هوش مصنوعی در ایران کم هست معمولا حقوق هوش مصنوعی نسبت به یکسری برنامه نویسها کمتر هست"
"-> سلام دوستان یه سوال کلی داشتم کدوم مبحث پایتون فهم اسون تر و راحت تری داره و در عین حال در اینده پر کاربردتره ممنون میشم راهنمایی کنید
-> سلام برنامه نویسی کار پر چالشیه تو هر حوزهای که باشه ولی دوستام که برنامه نویسی کار میکنن Django رو پیشنهاد دادن و گفتن نسبت به هوش راحتتره
-> و احتمالا درآمد بهتر
-> ممنونم از شما ببخشید اشتباه نوشتم میخواستم بپرسم کدوم حوزه هوش مصنوعی راحت تره
-> به نظر من راحتتر و سختتر نداریم اصلا معتقدم اگه براساس این نوع سوالها کدوم راحتتره یا بهتره یا موارد دیگه بخوایید تصمیم بگیرید احتمال اشتباه بودن تصمیم زیاد میشه سعی کنید چند تا درس خوب بگذرونید و کارآموزی برید بعد خودتون تصمیم بگیرید که به کدوم سمت برید به نظراتی که بهتون میگن این بهتره یا آسونتر با شک و تردید نگاه کنید من گفتم شاید بعضی حوزههای برنامهنویسی درآمد بهتری نسبت به هوش داشته باشن اما این هم یک نظر هست و دیدید که مخالف هم داشت شما باید خودتون به این دیدگاه برسید که کدوم سمت حرکت کنید همین اتفاق روی موضوع پایاننامه ارشد هم هست بچهها میگن چه موضوعی انتخاب کنیم خوبه درحالیکه دانشجوی ارشد باید یاد بگیره که خودش تحقیق کنه و موضوع خوب پیدا کنه شاید من جواب دلخواهتون رو ندادم اما امیدوارم بدردتون بخوره
-> سلام دوست عزيز من اولين mcp رو سال ٢٠٠٢ رو ويژوال استوديو ٦ گرفتم و از اون سال تا به امروز به عنوان برنامه نويس تمام وقت مشغول به كار بودم باتوجه به تجربه خودم عرض ميكنم شايد تجربه ديگران متفاوت باشه برنامه نويسي چيزي به عنوان راحت يا سخت نداره شغليه كه شما هميشه و هر روز با چالش روبرو هستيد هميشه بايد وقت براي مطالعه و اپديت شدن بزاريد اصول رو بايد ياد بگيريد سينتكس اهميتي نداره سويچ كردن از يه زبان يا فريمورك به يه زبان ديگه نبايد بيشتر از دو سه هفته وقت شما رو بگيره و اينجا بخوايد تو اين شغل مدت زمان زياد دووم بياريد واقعا بايد از جون مايه بزاريد من تو اين مدت به ياد ندارم تا به امروز هفته اي حداقل ١٠ ساعت مطالعه نداشته باشم به علاوه ساعت كار بسيار طولاني و هر روز هفته اگر بدنبال مسير راحت هستيد من اين شغل رو پيشنهاد نميكنم طولاني شد ببخشد
-> ممنون از شما در حوره هوش مصنوعی چطور نظری دارید
-> شايد امروز و به اين شكل تقاضاي بيشتري نسبت به شاخه هاي ديگه نداشته باشه ولي اعتقاد خودم بر اينه كه هوش مصنوعي همه چيزه قطعا وقت گذاشتن و هزينه كردن براش سواي اينكه در چه شغلي فعاليت ميكنيد يه سرمايه گذاري درسته
-> با سلام من یه سوال اساسی دارم که شاید جوابش بتونه کمکم کنه ممنون میشم از تجربه تون بفرمایید من کار خودم رو در کد زدن با پایتون به هر بدبختی هست پیش میبرم ولی خیلی دوست دارم ازش فرار نکنم وقت خالی رو روی کد زدن بزارم ولی اصلا انگار علاقه ندارم علتش رو نمیدونم و گاهی میگم شاید استعداد ندارم یا حوصله ندارم ولی واقعا دنبال این هستم که از روی علاقه کد بزنم و مطمئنم خیلی در کارم پیشرفت میکنم ولی این چالش رو از قدیم داشتم و دل ندادم به کد زدن شاید هم ۳۵ سال سن دیره"
"-> سلام استاد توی کتاب خانه apex انویدیا scaled loss فرقی با loss عادی داره چون موقع train گاهی step رو skip میکنه و مقدار scale رو کاهش می ده
-> به خاطر گرادیان تجمعی گاهی ممکنه گرادیان خیلی بزرگ بشه و به اصطلاح exploding اتفاق بیفته به همین این گرادیانها رو این کتابخونه هندل میکنه تا در فرآیند یادگیری مشکلی پیش نیاد
-> خیلی ممنون"
"-> سلام دوستان وقتی من عکساو بارگذاری میکنم هر از گاهی به ارور FileNotFoundError Errno 2 No such file or directory crackkimages11jpg میخورم و هرچی نگاه میکنم اون عکس تو فایل هست و نمیتونم ارور رو برطرف کنم دلیلش رو کسی میدونه چک میکنم لبل ها هم درست خوردن
-> یک space توی آدرس میبینم شاید مشکل از اون باشه
-> مرسی چک میکنم"
"-> سلام وقت بخیر آموزش الگوریتم surf از سایت هوسم برداشتید
-> سلام بله"
"-> سلام وقت بخیر من هم دنبال نرم افزاری هستم که بتونم روی تصاویر سفارشی خودم لیبل بزنم و فایل csv بسازم لطفا راهنمایی فرمایید
-> سلام من تجربه کار با cvat رو داشتم که خوب و راحت بود میتونید تصاویرتون رو داخل سایت cvat آپلود کنید و بعد یکی یکی لیبل بزنید و آخرشم هم csv یا txt بگیرید
-> با تشکر"
"-> سلام دوستان تو اموزش faster RCNN کسی میتونه به من بگه برای لبلینیگ کشیدن باکس از چه اپی باید استفاده کنم من با app متلب زدم که حس میکنم هم خونی نداره از opencv میشه استفاده کرد
-> سلام اگه منظورتون لیبل زدن دستی هست سایت cvat خوبه
-> ایا این سایت محدودیت اپلود نداره برای حجم زیاد عکس باز هم به صرفه هست یا کدوم نرم افزار دیگه ای رو پیشنهاد میکنید
-> سلام نمیدونم
-> ممنون"
"-> سلام دوستان من در این خط کد خطا دارم تو اموزش faster RCNN و خودمم کد نوشتم خطا داش ممنون میشم اگر کسی دلیلشو میدونه بهم بگه چه جوری رفع میشه
-> 
-> مرسی"
"-> سلام من برام سوال شده که وقتی در معماری مدل مون از self attention یا multihead attention استفاده میکنیم آیا میتونیم بردار توجهattention برای هر ورودی رو دربیاریم مثلا فک کنین ورودی مون یه جمله باشه بعدا دربیاریم که مدل ما وقتی کلمه اول از جمله رو دیده چقد به بقیه کلمات توجه کرده یا مثلا در مورد تصویر و ویدیو کسی از دوستان ایده ایی داره در این رابطه
-> سلام بله قطعا وقتی k در q ضرب میشه خروجی از سافت مکس رد میشه که درنهایت خروجی سافت مکس یکسری score هست که دقیقا میزان تشابه بین بردارهای ورودی رو نشون میده اصلا خروجی یک ماتریس هست که میتونید پلات کنید هرچه امتیاز بین دو بردار بیشتر ارتباطشون هم پررنگتر چرا تشابه چون وقتی q در k ضرب میشه در حقیقت دارید ضرب داخلی بین هر بردار رو با سایر بردارها حساب میکنید شاید در جلسه 2 بینایی کامپیوتر مطالبی در این باره گفته باشم
-> پایتورچ در سایت خودش نمونه کد این کار رو دارهمن الان دسترسی ندارم ولی همین کاری که شما میخواین بکنید رو نسون داده چجوریه"
"-> سلام وقت همه بخیر من به مشکلی خوردم که متوجه شدم مشکل متداولی هم هست ولی راه حل خاصی براش پیدا نکردم وقتی برای ارزیابی مدل از دستور modeleval استفاده میکنم مقدار loss به شدت بالا میره بالای ۱۰۰ و شبکه همه ورودیها رو عضو یک دسته پیش بینی میکنه ولی بدون این دستور نتایج ارزیابی منطقی بدست میاد انگار وقتی این دستور رو اضافه میکنم شبکه بهم میریزه ممنون میشم اگر ممکنه در مورد علت و راه حلش اگر اطلاع داشتید راهنمایی کنید
-> سلام من مشکل رو متوجه شدم ولی تا الان چنین مشکلی نداشتم شبکهتون چی هست
-> Mnasnet و mobilenet
-> سلام در ارتباط با مشکلی که ریپلی شده اگر کسی مواجه شد باهاش من خودم دوتا راه حل پیدا کردم دلیل این اتفاق این هست که running mean و running variance توی لایههای batch normalization شبکه خوب برآورد نمیشن به همین دلیل اولا میشه با تغییر مقدار momentum مثلا بزرگتر کردن لایه های bn اصلاحش کرد و دوم اینکه با تبدیل track_running_statsfalse توی لایههای bn شبکه کلا میانگین و واریانس متحرک رو لحاظ نکرد و از میانگین و واریانس خود اون batch استفاده کرد ممنون میشم اگر صحیح نگفتم استاد اصلاح کنن"
"-> استاد من میخواهم کد یکی از مقالاتی که خوندم را بررسی کنم و در مقاله نوشته شده با پایتون ۲۷ و پایتورچ ۰۴۱ اجرا شده بنظرتان راهکار دیگری بجز نصب این نسخه های قدیمی و اجرای کد وجود داره
-> خب میتونید نسخه 27 نصب کنید و روش نسخه قدیمی پایتورچ رو نصب کنید این کار شدنی هست سازگار کردن با پایتون و پایتورچ جدید احتمالا زمان بیشتری نسبت به حالت قبلی میبره
-> ممنونماگر بخواهم مجدد کدها را خودم بنویسممیشود دستورات معادل را براحتی پیدا کرد یا نه
-> بله میشه منتها باید ببینید ارزشش رو داره یا نه همچنین بدونید که زمانبر هست"
"-> سلام من نسخه ۳۷ پایتون را ازقبل نصب داشتم ولی برای اجرای یک پروژه نسخه ۲۷ نیاز بود و این نسخه را هم نصب کردم منتهی هیچ پکیجی را نشان نمی دهد آیا من درست نصب نکرده ام یا اینکه مجدد باید پکیج ها را نصب کنم ممنون میشم راهنمایی نمایید و آیا با نصب pyenv امکان رفع این مشکل هست
-> سلام برای پایتون 27 باید همه پکیجها رو دوباره نصب کنید
-> ممنون از پاسختانمنتهی الان خواستم پایتورچ نصب کنم نوشته پایتورچ فقط روی نسخه ۳۷ تا ۳۹ ساپورت میشه و نسخه ۲۷ ساپورت نمیکنه چجوری نسخه قدیمی مثلا ۰۴۱ پایتورچ نصب کنم
-> میتونید این کار رو بکنید ولی باید صبور باشید دیگه یک عالمه از این مشکلها ممکن هست داشته باشید
-> ممنونم راهی جز صبوری و تلاش نمانده
-> استاد امکانش هست بجای نصب pip در نسخه جدید پایتون با conda پایتورچ را نصب کنمچه فرقی بین pip و conda هست
-> "
"-> سلام وقت بخیر یه سوال کلی داشتم شبکه هایی که برای یک ورودی باسایز خاص ترین شدندفرض کنیم u net رو وقتی بخوایم ازش استفاده کنیم در قالب تست و ریزالت آیا بهتره تصویر رو ریسایز کنیم و خروجی رو هم در نهایت ریسایز و به سایز اصلی برگردونیم یا کلا از همون اول با توجه به معماری شبکه یک سری پدینگ روی تصویر اعمال کنیم و خروجی همسایز با ورودی رو بگیریم
-> "
"-> استاد سلام یک سوال داشتم اگر مدلی در یک ران نتیجه خوبی بده و بعد اون مدل رو با همان وزن ها و تنظیمات ذخیره کنیم وبعد لود کنیم و دوباره آموزش بدیم آیا باز همان نتیجه خوب ممکنه تکرار بشه سوال دوم اینکه اگر در یه مسپله رگرسیونی میزان دقت تست بهتر از ترین باشد این خطاست و غیر طبیعیه
-> 
-> خیلی ممنون استاد بدون augmentation ترین نتیجه بهتری میده در مقایسه با تست دقیقا این حالتی که تست بهتر از ترین شده زمانی رخ داده که تعداد سمپل ها رو بیشتر کردیم از هر نمونه که بیشتر دیده بشن پس الان میشه گفت اوکیه"
"-> سلام وقتتون بخیر ببخشید من یه دیتاست عکس سی تی اسکن دارم که ۵۱۲۵۱۲ هست با inceptionv3 و resnet50 اجرا کردم ولی خطا میزنه که cuda پر است سایز عکس رو نصف کردم عکس رو grayscale کردم حتی بچ سایز رو هم روی ۲ گذاشتم ولی بازم همینو میگه نمیدونم مشکلش چیه موبایل نت رو ولی اجرا کردم مشکلی نداشت از گوگل کولب استفاده میکنم ممنون میشم اگر راهنمایی کنید
-> سلام اگه یک جیپییو با 8 گیگ رم داشته باشید و رزنت 50 استفاده کنید و ورودی 256256 با بچسایز 2 بهش بدید باید اجرا بشه و مشکل پر شدن رم نداشته باشید اما گاهی خطاهای انسانی هست که باعث میشه شما به مشکل بخورید شاید بهتر باشه که کدتون رو دقیقتر چک کنید و اگر چند بار مدل رو فراخوانی کردید یا کلاس مدلتون رو اصلاح کنید
-> خیلی ممنونم باز هم چک میکنم شاید جایی اشتباه نوشته باشم فقط اندازه عکس برای res و inception چند باید باشه بهتره و استاندارده
-> اینها معمولا با 224224 ترین شدن و وزنهاشون در دسترس هست اما به نظرم ابعاد بزرگتر هم بدید مشکلی نداره و حتی ممکنه نتایج بهتری هم بگیرید
-> سلام این مشکل رو منم داشته و دارم از دیپ لب ۳ پلاس استفاده میکنم از xception استفاده میکنم سایز ورودی ۵۴۰ در ۳۰۱ هست و خروجی ۲۰۱ در ۳۰۱ با بچ ۳ بیشتر از این نمیشه بچ جی پی یو هم ۸ مشکل از خطای انسانی نبود شبکه ش در عین حال که خوبه ولی بزرگه و تقریبا ۵۸M پارامتر داره
-> سلام کار شما فرق میکنه شما سگمنت کار میکنید هم سگمنت دارید و هم شبکه تون دوبرابر رزنت 50 هست با پایتورچ کد زدید
-> خیلی ممنون
-> بله
-> 
-> چه قدر جالبه البته تلاش ما بهبود کیفیت داده خروجی هم هست شپا میفرمایید تاثیر چندانی نداره یه سوال دیگه برای بزرگ کردن تصویر استفاده از sub pixel layer بهتره یا transposed conv و اینکه اگه قرار جایی اینترپولیشن استفاده بشه Bilinear یا Bicubic کدوم بهتره و سوم اینکه استفاده از ۱ یا ۲ بلک رزیجوال در اخر شبکه میتونه جزییات رو بهتر استخراج کنه و اگه استفاده بشه بعدش لازم هست مثلا چند لایه کانولوشنی دیگه هم بزنیم یا کافیه همون بلاک رزوجوال
-> سلام وقتتون بخیر این سوالات منو اگه لطف کنید جواب بدید ممنون میشم
-> 
-> سلام خیلی ممنون از توجه تون بله من رشته فیزیک هستم و حرف شما تا حد زیادی درسته در مورد رویکردی که اتخاذ کردم من شبکه دیپ لب پلاس ۳ رو انتخاب کردم که ماژول های خوبی داشتن و نتابج خوبی هم روی بخش بندی معنایی داشته روی تصاویر خودم تعییر دادم و نتایج بهتر شد با همین تعییرات کوچک که فرمودید مشکل اینه که هر بار ران واسم ۲۰ روز هزینه داره و به همین خاطر انتخاب شده سراغ برخی تعییرات میرم ببینم جواب میده یا نه مثال دیدم برخی جاها ۳ تا بلاک رژیجوآل خوبه ولی ایا این همیشه هست یا نه یا بحث sub pixel layer و جایگزین بشه به جای trans conv یا اینکه ماژول ASPP در دیپ لب اگه در آخر شبکه استفاده بشه بهتر جزییات بدست نمیاد یا نه مقاله خیلی پیخونم از CVPR و ICML البته در حوزه بخش بندی
-> ادامه بدید انشالله موفقیت نزدیکه شاید ترین کردن روی یک زیرمجموعه کوچیک از دیتاستتون کار بدی نباشه یا کوچیک کردن سایز تصاویر و کلا من ترجیح میدم نتیجه آزمایشهام رو زود ببینم اگر هم شرایطی مثل شما داشته باشه مساله رو سادهسازی میکنم تا بتونم سرعتم رو بالا ببرم بعضی از آزمایشهای ما اونقدر بد هست که حتی روی یک مجموعه کوچیک هم نتیجش بده و نیازی نیست مدتی طولانی منتظر بمونیم من این کار رو میکنم ایدههای کوچیک پیادهسازی و آزمایش سریع
-> خیلی ممنونم استفاده میکنم همیشه از نکات و توضیحاتتون قشنگ عشق رو در تخصصتون میشه دید
-> ممنون لطف دارید
-> دقیقا کلمه خوبی بکار بردید برای استاد اشرفیعشق در تخصصی انشاله همیشه با انگیزه و با انرژی باشندبا آرزوی موفقیت روز افزون
-> ممنون"
"-> دوستان سلام ببخشید من میخوام یه مدلی ترین کنم که بتونم جای مکان چهره رو توی عکس پیدا کنه البته الگوریتم های mtcnn و cascade اینا و میدونم هستش و میشه باهاشون صورت رو تشخیص داد و پیدا کرد ولی میخوام خودم یه مدلی ترین کنم که بتونه صورت رو پیدا کنه یه دیتا ست چهره پیدا کردم و آموزش دادم و فقط میتونه مثلا چهره تو عکس هستش یا نه رو بگه ولی اینکه بخواد جای صورت رو پیدا کنه نمیدونم چه جوری میشه
-> سلام پیشنهاد من کار روی مقاله retinaface اول مقاله رتینانت و رتینافیس رو بخونید بعد کدهای رتینافیس رو اجرا کنید اول ارزیابی بعدا ترین بعدش هم میتونید خط به خط کدها رو تریس کنید و کامل بفهمید
-> 
-> سلام استاد وقت بخیر ما تو مقالمون لازم مراحل استاندارد سازی و فرمول هایی که برای این کار استفاده کردیم رو توضیح بدیم کلا چقدر از مراحل پیش پردازش باید بیان بشه"
"-> ما هم اینها رو میدونستیم ولی تنها راه بیان اعتراض قانونی به این اتفاق بد تاریخی همین کارزار بود طرح صیانت که ناقض آزادی و حریم شخصی هست در تاریخ ایران ثبت میشه اتفاقا کارزار اینترنت هم در تاریخ ثبت میشه و اونهایی که امضا کردن باید افتخار کنن که برای حق مسلم خودشون تلاش کردن چه خوب میشد که افراد بسیار بیشتری مشارکت میکردن و اطرافیان خودشون رو قانع میکردن که شرکت کنن شخصا تلاش کردم که اطرافیانم رو قانع کنم که شرکت کنن اشرفی
-> استاد جان هیچکس امید به تغییر مثبت در این مملکت نداره نمونش انتخابات های اخیر رای یکی از نمایندگان از دست اندرکاران برگذاری انتخابات توی اون منطقه کمتر شده و رفته مجلس
-> سلام استاد سوتفاهم نشه من خودمم اون کارزار رو شرکت کردم و امضا کردم بیشتر یه درد و دل بود ک به جایی نمیرسه"
"-> سلام وقت همه بخیر برای بدست آوردن یک مقدار loss برای کل دادهها باید میانگین مقدار loss هایی که از بچها بدست میاد رو حساب کنیم تابع ضرر cross entropy هست
-> بله
-> خیلی ممنون"
"-> سلام کسیهست داکر بلد باشه
-> سوالتون چیه"
"-> سلام به همگی در مورد fine tuning سوال داشتم یه جا خوندم نوشته بود برای دیتاست های کوچیک خیلی مناسبه کسی میتونه یه شهودی به من بده چرا اینجوریه
-> سلام فرض کن یه مدل شبکه های عصبی داری که آرتیفکت توی تصویر رو از بین می بره ورودی تصویر با کیفیت پایین خروجی تصویر با کیفیت بهتر اگر این مدل برای همه تصاویر توی دنیا تصاویر عادی ماهواره ای پزشکی جواب بده که چقدر خوب ولی این معمولا امکان پذیر نیست چون تصویر هر اپلیکیشن ویژگی های خاص خودش رو داره حالا فرض کن اپلیکیشن شما روی تصاویر سلولی ماکروسکوپی باشه که جمع آوری تصویر هزینه بره و به همین دلیل دیتا ستی که دارید کوچیکه اینجا هست که فاین تیونینگ می تونه کمک کنه که به مدل بهتری برسید نسبت به حالتی که اگر از ابتدا فقط از دیتا ست کوچک استفاده بشه
-> خیلی ممنون"
"-> سلام وقت بخیر من دانشجوی ارشد عمران هستم و برای پایان نامه در حال یادگیری ماشین لرنینگ هستم با مفاهیم ابتدایی ماتریسها آشنا هستم آیا نیاز هست سرفصل جبر خطی که در مقطع کارشناسی رشته کامپیوتر گفته میشه یادبگیریم
-> سلام یادگیری ماشین رو شروع کنید"
"-> سلام یه سوالوجود لایه stemدر معماری هایی مثلregnet برای چه کاریه
-> 
-> ممنونم از پپاسختون اما مثلا تو معماری هایی مثل eff فقط یه لایه کانولوشن 33دارند یعنی هیچ کاهش ابعادی رخ نمیده خب تو این مدل ها کاراییstem چی هست
-> فکر میکنم درگیر اسمش شدید stem یک لایه یا ماژول نیست که در هر شبکهای وجود داشته باشه بعضی شبکهها لایههای اول رو به اسم stem معرفی کردن مثلا شبیه لایه کانولوشنی الزامی وجود نداره که در همه شبکهها وجود داشته باشه
-> ممنونم ازتون"
"-> استاد سلام شما استفاده از این package های که برای تیون کردن hyperparams ها بکار میره رو چقدر توصیه میکنید
-> سلام من ازشون استفاده نکردم چه پکیجهایی مثلا من طبق روتینی که توی جلسه صفر بینایی کامپیوتر گفتم عمل میکنم و معمولا به نتیجه میرسم
-> بکیج های متفاوتی هست کلی گفتم ببینم اصلا ارزش استفاده کردن رو دارن یا بهتره خودمون وقت بزاریم تیونینگ رو انجام بدیم
-> روتین جلسه صفر بینایی ماشین کدومه چون من فقط یکی از دوره هاتون رو خریدم دور ادونس رو تهیه نکردم
-> جلسه صفر بینایی ماشین فکر کنم رایگان هست"
"-> سلام ببخشید می خواستم بدونم هوسم برای کنکور ارشد کامپیوتر مشاوره داره
-> سلام خیر شایان عزیز
-> سلام یه چرخی توی این کانال بزن مطالب خوبی دارن معمولا"
"-> سلام دوستان کسی هست که با شبکه های yolov3 yolov4 yolov5 کار کرده باشه یه سوال فوری داشتم
-> خب چرا یکسره سوالتون رو نمیپرسید"
"-> سلام وقت همگی بخیر اساتید محترم اگر پرسیده بشه چرا بین لایه ها از batch normalization استفاده شده جواب علمی براش چیه
-> 
-> از پشتیبانی دائمی شما واقعا خیلی ممنونم استاد"
"-> سلام ایا میشه از f1 صحت و حساسیت رو حساب کرد
-> نمیشه یک معادله دو مجهول
-> بله دقیقا گفنم شاید راه دیگه ای باشه"
"-> دوستان سلام می خواستم لطفا اگر کسی تجربه داره یه کم من رو راهنمایی کنه من دانشجوی ارشدم میخوام بدونم برای شروع به کار و یا کارآموزی تو این حوزه باید دقیقا توی چه سطحی باشیم و اینکه مدرک دانشگاه خوب واقعا چقدر تاثیر داره توی این قضیه ممنون
-> سلام دانشگاه تاثیر داره در واقع میتونه ذهنیت مثبت نسبت به شما ایجاد کنه اما همه چیز نیست باید از لحاظ علمی حداقلها رو داشته باشید معمولا کسی از دانشجوی ارشد انتظار اجرای پروژههای سطح بالا مقاله با کیفیت نداره ولی انتظار میره درسهای خوب گذرونده باشید و مباحث رو خوب بلد باشید تو بخش کدنویسی تسلط داشته باشید قبلا ما مواردی رو در شرکت میدیدیم که فرد در یک دانشگاه خوب درسهای خوب گذرونده بود و معدلش بالا بود اما چیزی بلد نبود و همه رو فراموش کرده بود از طرفی در سالهای اخیر تقاضا برای کار هم زیاد شده و یکسری افراد با اطلاعات بسیار کم دنبال شغل هستن و به علمشون عمق ندادن کار در شرکت خوب نیاز به دانش خوب داره کار نسبت به دانشگاه فضای جدیتری داره و لازم هست که عمقی مطالب رو بلد باشید علم جزوهای جوابگو نیست به نظرم تسلط به یادگیری ماشین یادگیری عمیق کدنویسی پایتون سایکیت و پایتورچ یا تنسورفلو خیلی مهم هست یعنی اگه همین الان یک دیتاست بدون هیچ توضیحی به شما بدم و بگم موارد مشکوک رو میخوام خودتون بتونید راه حلش رو طراحی کنید البته این مواردی که گفتم برای کار در شرکتهای خوب هست شرکتهای سطح پایین زیاده و انتظاراتشون پایین هست در آخر هم بگم اهدافتون بعد ارشد بسیار مهم هست معمولا سر کار رفتن از کیفیت تحصیل شما کم میکنه مدیریت همزمان هردو کار بسیار سختی هست حقوق گرفتن و کارهای جدی انجام دادن بسیار شیرین هست و نتیجه اینکه ذهن شما بیشتر معطوف به کار میشه و از دانشگاه فاصله میگیرید نظر شخصی من این هست که بهتره دانشجوهای ارشد تمام تلاششون رو بکنن که یک دوره ارشد با کیفیت رو سپری کنن در کنارش نهایتا دنبال پروژه کسر خدمت باشن کار رو میشه دیرتر هم پیگیر بود اما ارشد به سرعت برق و باد میگذره و این فرصت تکرار نمیشه گذروندن درسهای خوب انجام یک پایان نامه با کیفیت مقاله دادن اینها برای ارشد خیلی خوبه
-> خیلی ممنون استاد فقط یه سوال دیگه اینکه شرکت های خوب تو این حوزه چه شرکتایی هستن واقعا چه بسترهایی به نظرتون برای دیدن فرصتای شغلی شرکتای خوب وجود داره
-> "
"-> سلام یک سوال داشتم من مدلم بعد چند epoch یهو loss اش توی ترین و تست زیاد میشه دلیلش چی می تونه باشه
-> سلام بنظرم نرخ یادگیری را باید کاهش دهید
-> نرخ یادگیریم 1e5 هستش با از اون هم پایین تر برم دیگه باز یادگیری انجام نمیشه
-> اگه همونطوری ادامه پیدا کنه نتیجه چی میشه
-> مقدار loss حتی از اولین epoch هم بیشتر میشه
-> سلام ببخشید من پی وی پیام دادم"
"-> استاد معماری رزنت رو با هردو روش کراس و بایتورچ ایجاد کردم چرا بایتورچ انقدر کند هستش توی آموزش باور کنید کراس خیلی زود جواب داد به نظرتون مشکل از نحوه بیاده سازی من بوده یا کلا بایتورچ کند عمل میکنه
-> 
-> خیلی ممنونم از راهنمایی تون"
"-> سلام استاد در تکنیک ارزیابی k_fold با k10 هربار یک قسمت را برای تست میگذاریم و با ۹ قسمت دیگر train میکنیم دفعه بعد که یک قسمت را برای تست میگذاریم آیا از همون ۹۰درصدی که برای train گذتشته بودیم یک بخش برای تست جدا میکنیم یا از همان داده های قبلی اگر از داده های اولیه هربار یک بخش برای تست جدا کنیم و ۹ بخش برای آموزش و این پروسه ۱۰ بار تکرار شود اون وقت داده های تست بی معنی نمی شوندچون باهاشون مدل آموزش دیده
-> نه بی معنی نمیشه چون انگار هربار شما یک مدل جدید رو ترین میکنید یعنی ۱۰ تا مدل جدید با دیتای ترین و تست مربوط به هر فولد
-> خب از صفر که ترین نمیکنیم هربار شبکه یک وزن میگیره و دفعه بعد همون وزنها اصلاح میشن و در این بین داده تست دیگر معنی تست را نخواهد داشت
-> نه شما بعد از هر فولد باید یه تابع بنویسید که وزن های قبلی رو ریست کنه و شبکه از نو ترین بشه
-> پس چرا باید ۱۰ بار ترین کنیم یعنی از بین ده بار ترین بعدا بهترین را انتخاب کنیم
-> برای اینکه ثابت کنیم دیتای پایداری داریم و دیتاهامون همه خوبن و پراکندگی دیتا نداریم در مدل های ۸۰ به ۲۰ ممکنه که دیتای خوب بیفته تو قسمت ترین و تو قسمت تست دیتای بدتری داشته باشیم در نتیجه نتابج خوبی هم بدست نمیاد یا برعکسش هم ممکنه یعنی دیتای تست بهتر باشه و دیتای ترین بدتر برای کافولد شما در نهایت میانگین نتایج هر k مدل رو بعنوان نتیجه نهایی گزارش میشه
-> ممنون از راهنمایی تون"
"-> سلام وقت بخیر آیا لازمه بعد از هر شبکه ما از svmبرای طبقه بندی استفاده کنیم در چ حالتی لازمه
-> سلام من این کارو رو مطلوب نمیدونم آخر شبکه یک فولی کانکتد به عنوان کلاسیفایر بذارید همین کافی هست
-> خب من همین کارو کردم اما بعضی از مقالات اومدن یه svmهم گذاشتن شک کردم فک کردم که واقعا ضروریه ممنونم ازتون"
"-> 
-> اگه داده هاتون کمه و به annotate کردن نیاز دارین این سایت این کارو براتون انجام میده"
"-> سلام من یه شبکه کلاسفیکیشن دارم که توی loss وقتی bce می زارم مقدار loss زیاده ولی وقتی mse می زارم مقدار loss کمه دلیلش چی می تونه باشه
-> سلام از mse برای کلاسبندی استفاده نمیکنیم برای رگرسیون استفاده میکنیم باتوجه به این نکته مقایسه این دو لاس و نتیجهگیری ازش کار اشتباهی هست
-> ممنون"
"-> سلام وفت بخیر برای طبقه بندی باینری مثلا بگیم دقت ۹۰ درصد حساسیت۹۰ صحت ۸۹ ایا این برای گزارش کافیهیا باید مشخص کنیم که کدوم کلاس حساسیت صحت و دقتش فلان مقداره
-> سلام ما recall و precision رو برای دو کلاسه محاسبه میکنیم به صورت خودکار یک کلاس positive هست و دیگری negative پس یک بار محاسبه میشه
-> ممنونم ازتون مثلا بگم precision 0 98درسته یعنی کلاس های مثبت درست کلاس بندی شدنددیگه نیازی نیست نوع کلاس مشخص بشه"
"-> ممکنه الان جلوی تراکنش رو میگیرن برای همین مجبور شدم برای جیمیل جدیدم لوکیشن هلند رو تنظیم کنم
-> ممنون مهندس جان"
"-> سلام وقت همگی بخیر بعد از خرید کولب پرو چطور باید اکتیو کردش
-> خودکار اکتیو میشه زیر لوگوی کولب گوشه بالا سمت چپ مینویسه pro
-> ظاهرا برای من فعال نشده
-> اول به جیمیلت مراجعه کن ببین خرید درست انجام شده یا نه من یکبار درخواست خرید کولب دادم برام یک چیز دیگر را خریدند نکته دوم اینکه لوکیشن جیمیلت نبایستی ایران باشه
-> ای وای ایران باشه فعال نمیشه"
"-> بعضی مجلات هستند تحت عنوان اپن اکسس که برای چاپ پول از نویسنده میگیرن اینها معمولا نه الزاما سریعتر این فرایند رو طی میکنند اپن اکسس ینی دیگران برای دانلود مقالت محدودیتی ندارن و رایگان میتونند دانلود کنند برای سایت خورد مقاله خیلی خوبه این مجلات اما خوب هزینش اصولا زیاد هست و باب طبع ما بی پول ها نیست
-> دیگه دوستان کامل تشریح کردن ممنون
-> خواهش میکنم در مورد کد باید عرض کنم که مجلات ترنزکشن ieee اصولا الگوریتم کلی کار رو به صورت کدوار میخوان اما سورس کد اصلی رو ممکنه یه داور خیلی گیر بده و بگه به صورت ضمیمه برام ارسال کن وگر نه کد اصلی و دیتا نیاز نیست"
"-> دوستان سلام کسی تا حالا شبکه کانولوشنی روی دیتای tabular آموزش داده کانولوشن که باید یک بعدی باشه فقط سوالم اینه که همچین کاری اصلا توجیه داره من با یه اتوانکدر با لایه های معمولی dense عمل denoising انجام دادم ولی میخوام بدونم به نظرتون کانولوشن یک بعدی میتونه بهبود چشمگیری ایجاد کنه یا اصلا توجیهی داره روی داده tabular ممنون
-> میشه cnn رو روی دادههای جدولی هم اعمال کرد من این کار رو قبلا انجام ندادم و معمولا از mlp استفاده میکنم ولی سرچهایی کردم و مطالبی درمورد این مساله دیدم درمورد مفید بودنش نظری ندارم
-> خیلی ممنون"
"-> سلام وقت بخیر میشه روند ارسال مقاله رو توضیح بدین باید داده ها و کد رو بفرستیم یا فقط مقاله رو میفرستن
-> 
-> ممنونم از راهنماییتون سابمیت مقاله چ مدت طول میکشهو در چ صورتی ممکنه ریجکت بشه
-> بسته به ژورنال متفاوت هست بین 6 ماه تا یک سال معمولا طول میکشه اگر سطح مقاله پایینتر از معیارهای اون ژورنال باشه یا مقاله کپیبرداری باشه ریجکت میشه البته کپی کردن مجازات بیشتر از ریجکت هم داره
-> ممنونم ازتون اخه بهم گفتن دو هفته ای سابمیت میشه
-> مجلاتی که پول بگیرن بله دو هفته ای هم نه دو ماه
-> سابمیت ده دقیقه کار داره بعد از سابمیت سردبیر یه بررسی کلی میکنه ببینه به اسکوپ مجله میخوره یا نه اگه نخوره ریجکت میکنه با عنوان اوت او اسکوپ معمولا دو هفته این زمان میبره اگه اوت او اسکوپ نشه میره برای داوری راند اول این داوری بین یک الی سه ماه طول میکشه نظر داورا سه حالت داره یا ریجکت هست از نظر کیفیت یا میجر ریویژن یا مینور معمولا اگه ریجکت نکنند یک الی سه ماه به نویسنده زمان میدن که نظ داورها رو با تغییر مقاله جلب کنه سپس مجدد باید سابمست کنید و میره برای راند دوم و مجدد داوران که سه الی نه نفر هستند بررسی میکنند و مجدد اگه نیاز به میرایش داره ارجاع میدن مقاله رو به نویسنده و مجدد نویسنده تغییرات میده و در نهایت ممکنه اکسپت کنند تمام این زمان ها و سخت گیری ها برای ایرانیان رو ضرب در دو کنید مجلات معتبر زیر یک سال نمیشه مگه اینکه سطح مقالت خیلی خوب باشه و سریع داوری بشه و سریع جواب بدی به داورها
-> بسیار کامل و آموزنده تشکر
-> ممنونم ازتون"
"-> سلام ببخشید توی پایتورچ با چه دستوری میشه ساختار تصویر رو به هم ریخت مثلا تصویر بالا رو به تصویر پایین تبدیل کرد آیا توی خود دیتالودر این امکان هست
-> منظورت از به هم ریختن نویزی کردنه
-> نمیدونم اصطلاح نویزی کردن دقیقا به چی اشاره داره ولی منظورم اینه که جای پیکسلهای تصویر به صورت تصادفی جابجا بشن
-> من سرچ کردم اما دستوری برای این کار پیدا نکردم البته یک راه حلی به ذهنم میرسه تصویر باینری به نظر میاد اگر همه تصاویر باینری باشن میشه به صورت رندوم یکسری پیکسل انتخاب کرد و بعد مقدار هر پیکسل رندومی رو not کرد
-> این از تصاویر MNIST هست یه روشی رو امتحان کردم جواب گرفتم تصویر رو با reshape به بردار یک بعدی تبدیل کردم بعد با shuffle عناصر رو جابجا کردم دوباره reshape کردم به دو بعدی ممنون که وقت گذاشتید
-> بله روشتون خوبه"
"-> دوستان من یه مسله رگرسیون خطی رو با رزنت ۵۰ انجام دادم که نتیجه ش افتضاحه به نظرتون مشکل از دیتای من میتونه باشه البته بگم داده های من مرتبط با عکس نبوده و اینجا من فقط از cnn استفاده کردم ببینم با این نوع شبکه جواب میده یا نه ممنون میشم شما هم نظر بدید استاد
-> سلام لازم هست درمورد مساله توضیح بدید تا بتونن کمکتون کنن
-> چه توضیحاتی لازمه که بگم در مورد مساله فقط دیتاست مربوط به حوزه bioinformaticهست در واقع یکسری داده عددی هست و دیتاست تصویر نبوده نمیدونم که آیا از شبکه های عصبی کانولوشن اصلا میشه برای غیرتصویر استفاده کرد یا نه
-> سلام میشه از cnn واسه غیر تصویر هم استفاده کرد ولی شاید لزومی نداشته باشه اگه تعداد فیچر ها خیلی زیاد نیستند اینکه نتایج خراب میشه ممکنه علت های زیادی داشته باشه دیتاستتون نامتوازن باشه یا تعداد سمپل ها کم باشه
-> بله آقای درست میگن از شبکه cnn میشه برای غیرتصویر هم استفاده کرد باید بیشتر درمورد تسک و دیتاست توضیح بدید مثلا ممکن هست انتظار داشته باشید شبکه اعداد بزرگی رو براتون تخمین بزنه که معمولا برای شبکه سخت هست ممکن هست خطای کدنویسی داشته باشید ممکن هست دادهها رو به شکل مناسبی به شبکه نداده باشید شاید این داده شما برای شبکه مناسب نباشه میشه یک لیست بلندبالا از دلایل نوشت
-> خطای کدنویسی ندارم چون برای کلاسیفیکیشن تست کردم دقت ۹۹ درصد میداد برای رگرسیون اینطوری میشه ولی بحث داده های نامتوازن من بیشتر شنیدم برای مسائل دسته بندی مطرحه تا رگرسیون اگر اشتباه میکنم تصحیح کنید استاد منظورتون رو از تخمین اعداد بزرگ متوجه نشدم منظورتون اینه مقادیر تارگت ها خیلی بزرگ باشه تعداد سمپل ها ۷۰۰۰ تاست که ۶ هزار رو برای آموزش در نظر گرفتم دیتاست درواقع شامل موقعیت یکسری ساختار مولکولی هست و هر دیتا سمپل موقعیت و ویژگی های یک ساختار مولکولی رو نشون میده دیتاست یه ماتریس 27x27 هست هر سمپلش
-> استاد برای اینکه بفهمم مشکل از دیتاسته بهتر نیست یه دیتاستی که برای مسله رگرسیون خطی هست دانلود کنم و با اون شبکه رو تست کنم و اگر جواب داد متوجه میشیم که داده من برای شبکه مناسب نیست
-> 
-> خیلی لطف کردید استاد اتفاقا به نکات مهمی اشاره کردید رزنت رو من خودم پیاده کردم یعنی از رزنت آماده استفاده نکردم و خطایی نمیده بهم حیلی محبت کردید وقت گذاشتید
-> من ترجیح میدم از رزنت آماده استفاده کنم اگر ارور نمیده و واقعا 132 برابر میشه باید بررسی بشه چون انتظار میره با 132 خطا بده
-> رزنت آماده مشکلی که داشت ترجیح میده ورودی سایزهای تعیین شده خودش باشه ورودی من سه تا کانال نداره مثل تصاویر درواقع دیتاست من اصلا تصویر نبست سایزش 12727 هست"
"-> سلام به همگی یک سوالی راجع به شبکه عصبی داشتم به نظرتون شبکه عصبی معمولی backprop و تابع هزینه MSE میشه برای مسئله رگرسیوننه طبقه بندی بکار برد مثلا تخمین یک تابع رو بخوام با این شبکه انجام بدم برای این میپرسم چون خودم شبکه رو نوشتم ولی خطام تا 002 رفته ولی با متلب انجام دادم تقریبا به صفر رسید
-> سلام بله از شبکه عصبی با mse میشه برای رگرسیون استفاده کرد
-> سلام ممنون از پاسختون من میخواستم تابع سینوسی رو تخمین بزنم و با یک لایه پنهان 5 نورونی به نظرتون مشکلی نداره فکر میکنم حداقل یک لایه پنهان داشته باشیم میونه مدل غیرخطی باشه درسته تابع فعالساز لایه پنهانم رو Tanh و لایه خروجی sigmoid انتخاب کردم داده هام که هم بین نرمال شده است"
"-> سلام وقت همگی بخیر جدید ترین مقالات در زمینه Real time dense object detection چیه ممنون میشم بفرمایید
-> سلام من مقالههای جدیدش رو نمیشناسم من مقالهها رو معمولا از سایت paperswithcodecom IEEE sciencedirect arxiv پیدا میکنم"
"-> سلام دوستان یک منبع خوب از پردازش زبان طبیعی معرفی میکنید دوره یا کتاب
-> سلام اگر در حد شروع و آشنایی میخوایید فصل 5 و 6 دوره یادگیری عمیق رو مشاهده بفرمایید در حد آشنایی بد نیست من از سایر دورههای فارسی زبان متاسفانه اطلاعی ندارم اما میدونم که دوره پردازش زبان طبیعی دانشگاه استنفورد دوره سنگین و پرمحتوایی هست"
"-> سلام یه سوال که شاید برای همه پیش بیاد آیا نرم افزاری واقعا هست که کد زبان C را به پایتون تبدیل کنهاگر کسی تابحال کار کرده دقتش چطور هست ممنون میشم پاسخ دهید
-> سلام حتما هست اما فکر میکنم کیفیت کد هم مهم هست کدی که در سی بهینه هست لزوما در پایتون بهینه نیست تبدیل خط به خط کدها شاید کد بهینهای ایجاد نکنه شاید نیاز باشه توسط یک متخصص توی پایتون بازنویسی بشه این نظر من هست شاید افراد خبره در حوزه برنامهنویسی نظر من رو تایید نکن در هرصورت اگر اطلاعات خوبی از افراد متخصص دریافت کنید ممنون میشم من رو هم در جریان بذارید
-> ممنونم استاد"
"-> میشه در مورد mean average precision هم یک توضیح بدین یا اگه کدش رو داریم قرار بدین
-> "
"-> سلام کسی دلیل این ارور رو می دونه
-> گاهی اوقات کتابخونههای جی پی یو خطاها رو هندل نمیکنن و خطاهایی کلی میدن وقتی چنین خطایی میگیرید یعنی یک اتفاقی افتاده که نمیتونه دلیلش رو بهتون در قالب پیغام بگه گاهی وقتی کد رو روی cpu اجرا میکنید خطا رو نشون میده بعد میتونید رفعش کنید و بعد باز هم روی جی پی یو اجرا کنید دلایل خطا متنوع هست مثلا ممکن هست در محاسبات nan ظاهر شده باشه
-> ممنون"
"-> یک حالت اینه که قبل آموزش مدل رو طوری بسازید که دو خروجی بده حالت دوم که به ذهنم میاد اینه که یک مدل جدید بسازید که ورودیش اینپوت مدل باشه و خروجیش آوت پوت لایه میانی
-> خیلی ممنون که اینقدر سریع جواب دادین الان من بعد از fit کردن مدل این دوستور رو بذارم features modelget_layerfeatures منظور از model اسم شبکه م هست که fit کردمش و features هم نام لایه ایه که میخوام خروجیش رو دربیارم حالا مطمعن نیستم که این کار اطلاعات رو بعد از training میده یا نه صرفا اطلاعات اون لایه تو همون ابتدای کاره
-> در مورد ایده اولتون اونوقت انتظار میره دوتا تابع loss هم براش تعریف بشه درحالیکه من اینو نمیخوام
-> شما وقتی مدل رو آموزش دادید بعدش روشهایی که داخل لینک بالا پیشنهاد شده رو انجام بدید
-> خیلی ممنونم"
"-> دوستان سلام توی تنسورفلو اگر بعد از fit کردن مدل بخوام خروجی لایه میانی اتوانکدرم رو دربیارم چجوری باید اینکارو بکنم میشه لطفا راهنمایی کنید
-> سلام منظورتون latenet space
-> دقیقا لایه میانی یه اتونکدر رو میخوام بعد از fit کردن برگردونم"
"-> استاد سلام من مطابق شکل یه اتوانکدر با ورودی چندگانه دارم که دارم با پایتورچ پیاده سازی میکنم من برای انکدر اول به تعداد ورودی هام یه moduleList ساختم حالا سوالم اینه که چجوری توی forward باید یه تابع سیگموید روی هر انکدر هر ورودی اعمال بکنم و نهایتا خروجی تک تکشون رو concat بکنم
-> توی فوروارد هرجایی که میخوایید سیگموید بذارید از torchsigmoid استفاده کنید
-> def forwardself x xx for i l in enumerateselflinears xxappendfsigmoidselflinearsix lx من این رو نوشتم فعلا طبق مستندات پایتورچ روی لیستم لوپ زدم و هر بار حاصل رو توی یه لیست دیگه append میکنم فقط متوجه نمیشم اون داخل لوپ lx چیکار میکنه دقیقا"
"-> سلام من شبکم بعد از چند step مقدار loss رو nan بر میگردونه دلیلش چی می تونه باشه
-> دلایل مختلفی میتونه داشته باشه یکی از مهمترین دلایلش نرخ یادگیری بزرگ هست
-> نه نرخ یادگیریم در مرتبه ۱۰ به توان منفی ۵ هست"
"-> سلام دوستان یادگیری federated جز یادگیری semisupervised و unsupervised حساب میشه
-> 
-> خیلی عالی ممنونم"
"-> یک سوال دیگه من می خوام وزن های یولو دو رو از weights به pt تغیر بدم باید چیکار کنم
-> سلام من این کار رو انجام ندادم قبلا راه حل پیشنهادی من این هست باید دنبال کد converter باشید اگر بتونید کد یولو 2 به پایتورچ رو پیدا کنید احتمالا توی اون converter باشه
-> سلام اگر تونستید انجام بدید لطفا فرآیند انجامش را به اشتراک بزارید ممنون میشم
-> چشم حتما"
"-> سلام یک سوال چطوری می تونیم دیتاست های بزرگ رو توی کولب بیاریم مثل imagenet یا voc
-> 
-> خیلی ممنون یک سوال دیگه ام داشتم من وقتی می خوام روی دیتاست voc کار کنم توی کولب هم سرعتش خیلی از gpu دستگاه کمتره هم بعد از یک مدتی که میزارم بمونه خودکار disconnect میشه چیکار کنم چون روی gpu خودم مجبورم با batch_size 4 برم
-> احتمالا با کولب پرو سرعت آموزش شبکه بیشتر و اتصالیها کمتر بشه"
"-> سلام شما برایkfold اطلاع دارید که تستش به چه صورتیه اون فولد هایی که جدا میکنه و تست میکنه آیا برای تست مدل کافیه اگه کافی نیست برای تست مدل باید یه فولدر جدا بدیم و باز هم فولدینگ بزنیم بعد تست کنیم
-> بله برای تست کافی هست
-> ممنونم از پاسختون پس وقتی pthمدل رو لود میکنیم و دیتا به مدل میدیم کل دیتاست رو بدیم اوکیه اینجا نیاز به فولدینگ دیگه نیست"
"-> سلام وقتتون بخیر یک سوال داشتم ممنون میشم راهنماییم کنید می خواستم بدونم اگه از داده های تست به عنوان ولیدیشن هم استفاده کنیم کار اشتباهی انجام دادیم یا اشکالی نداره
-> کار اشتباهیستچون دیگر داده تست نخواهید داشت
-> ممنون که پاسخ دادین من هم فکر می کنم کار درستی نباشه انگار که غیر مستقیم به شبکه تقلب رسونده باشیم اما فکر میکنم در پروژه تشخیص کرونا که در سایت هوسم قرار داره از داده تست به عنوان ولیدیشن استفاده شده
-> خواهش می کنممن این پروژه را نگاه نکردم"
"-> کلا کتابی هست که الگوریتم یولو را کامل توضیح بده
-> فکر نکنم مقاله یولو رو باید بخونید
-> 
-> ممنون"
"-> سلام یک سوال در مورد الگوریتم یولو داشتم الان توی خروجی یولو یه confidence پيش بيني میشه یه confidence هم با توجه به مقاله ما از iou بین پيش بینی و مقدار حقیقی به دست میاریم الان باید این دوتا رو درهم ضرب کنیم یا کار دیگه ایی باید انجام بدیم
-> سلام نیازی به ضرب نیست اگه از معیار mAP استفاده کنید خودش هر دو رو اعمال میکنه و بهتون معیار درستی میده اگر هم میخوایید برای یک تصویر تعبیر و تفسیر کنید باید اینها رو جدا تعبیر کنید یکی برای این هست که مثلا با احتمال 70 درصد میگه این گربه هست اون یکی هم میگه مثلا با 80 درصد کادر دور گربه به درستی کشیده شده"
"-> سلام کسی تابحال نسخه کامل پایچارم را روی اوبونتو نصب و کرک کرده
-> سلام وقت بخیر من اینکارو کردم
-> من البته نسخه community اونو نصب کردم ک اگه منظورتون این باشه"
"-> سلام برای تدریس آنلاین مثلا با گوگل میت چه نرم افزاری هست که هم از صفحه فیلم برداری کنه هم صدای دوطرفی که با هم صحبت می کنند با گوگل میت را ضبط کنه
-> سلام Camtaziaنرم افزار خوبیه
-> tnx"
"-> سلام ممنون عزیزان
-> تبریک میگم استاد واقعا خیلی خوشحال شدم با ارزوی موفقیت بیشتر برای شما و همه اعضای گروه
-> سلام ممنون با آرزوی موفقیت و سلامتی برای شما"
"-> تبریک به استاد اشرفی بابت پذیرش مقالشون در یک مجله معتبر اشپرینگر با ارزوی موفقیت روز افزون برای شما
-> تبریک میگم اقای دکتر اشرفی
-> تبریک عرض میکنم خدمت آقای دکتر اشرفی و آرزوی موفقیت روزافزون برای شما
-> ممنون همچنین آرزوی موفقیت برای شما
-> استاد عزیز آقای دکتر اشرفی تبریک عرض می کنم انشاله همیشه موفق و پیروز باشید
-> منم تو همین ژورنال قبلا مقاله دادم کیو دو ایمپکت یک و دو
-> به شما هم افرین کیو وان اسکوپوس هست و ضریب تاثیرش ۲۵
-> به امید موفقیت های بیشتر برای استاد اشرفی
-> با آرزوی سلامتی و موفقیت های بیشتر بنده هم تبریک عرض میکنم
-> سلام آقای دکتر ان شاءاالله شاهد موفقیت های بیشتر شما باشیم دکتر پرحوصله و صبور و خوش فکر
-> تبریک باتاخیر مجدد به استاد اشرفی بابت چاپ مقاله معتبر دیگر در یک مجله عالی توفیقات روزافزون برای شما ارزومندم
-> سلام ممنون"
"-> سلام در مباحث یادگیری ماشین و دیپ لرنینگ آیا لازمه مفاهیم ریاضی مربوطه رو عمیقا یاد گرفت توابع و مباحث ریاضی که توی آموزش ها میگن بسیاری از آموزش های حتی سایت های معتبر همش کدنویسی دارن و اصلا با این فرمول هاکار نمیکنن
-> 
-> بسیار ممنونم از راهنماییتون
-> بنده تخصصی میخوام دیپ لرنینگ کار کنم و ماشین لرنینگ رو صرفا در حد الگوریتم ها و مباحث عملی کار کردم و نه ریاضیاتی اگر دوره یادگیری عمیق سایت رو کامل از اول پیش برم نیاز ریاضیاتی رو برای آینده برطرف میکنه
-> فکر میکنم برای شبکههایی مثل mlp cnn و بازگشتی به اندازه کافی از ریاضیات گفته شده طوری که شما در سطح خوبی قرار میگیرید طبیعتا ریاضیات و تئوری پشت انتها نداره اما ما به اندازه کفایت گفتیم
-> ببخشید استاد امکان داره پاسخ سوال بنده رو هم بفرمایید
-> سلام چقدر کامل و خوب این موضوع رو شرح دادید
-> خواهش میکنم
-> سلام استاد شما فرمودید که اگر آیندتون به دیپ لرنینگ هست لازمه عمیقا مفاهیم ریاضیاتی پشت قضیه رو یادبگیرید تمام الگوریتم های ریاضیاتی مثل محاسبات خطا و بهینه سازی وتوسط توابع طراحی شده توی کراس یا پایتورچ و پیاده میشن استاد میشه یه توضیح بدید دوره یادگیری عمیق با هدف بینایی ماشین در چه حد لازمه اینهمه تئوری رو یادبگیریم
-> اگر یه مثال عملی توی بینایی ماشین بفرمایید که با ریاضیات مستقیم سر و کار داشته باشه ممنون میشم
-> 
-> ممنونم
-> سلام استاد شب بخیر میشه خواهش کنم اگر ممکن کمی درباره Wandb توضیح بدید
-> 
-> سلام استاد خیلی ممنون آموزش هاشو از کجا میشه پیدا کرد منابع فارسی که چیزی نداره و این که با وی پی آن هم من نتونستم واردش بشم
-> سایتش وبلاگ خوبی داره یوتوب رو ندیدم با وی پی ان من میتونم وارد شم
-> برای من ارور ۴۰۳ میده"
"-> سلام خدمت تمامی دوستان محترم و استاد گرامی یک سوالی داشتم این که در یک سیستم با دو ورودی و دو خروجی سیستم ۴تانک میخوام حمله وارد کنم و داده غلط تزریق کنم و میخوام توسط شبکه عصبی این حمله رو تخمین بزنم تا در نهایت به کنترلر سطع لغزشی ترم تخمین شده رو اضافه کنم و اثر حمله رو خنثی کنم دوستان من دیدم داخل یک مقاله با شبکه عصبی گوسیgusian radial basis fun حمله رو تخمین زده ایا با شبکه های عصبی دیگه هم میشه این کارو کردشبکه باید چه مشخصه ای داشته باشه مننون میشم راهنماییم کنید
-> من در این زمینه تخصصی ندارم و نمیدونم
-> خیلی ممنونم از شما اطلاعی هن راجعبه نحوه کارکزد شبکه RBFنذارید"
"-> سلام علت نوسان تو نمودار های دقت و لاس چیه و چطور باید برطرفش کرد
-> چون batch های کوچک برمیداریم باعث میشه دچار نوسان بشه فکر کنم اگر کل دیتاست رو یکجا بدیم به مدل نمودار ها هموار تر میشن که البته سخت افزار مناسب برای اینکار معمولا نیست
-> ممنونم ازتون"
"-> سلام استاد ممکنه بفرمایید دلیل شما برای استفاده از pythoch در آموزش ها نسبت به tensorflow چیست
-> "
"-> سلام دوستان اگر ما چند فایل text داشه باشیم که حاوی ترتیبی از اعدا د باشند و بخواهیم بین این چند فایل یک sub string پیدا کنیم که مشترک باشند شما چه متدی را پیشنهاد میکنید
-> regex
-> خیلی ممنون تحقیق میکنم اینو
-> این مورد رو که بررسی کردم مربوط میشه به جستجوی یک الگوی خاص من الگو رو ندارم و باید یک الگو مشترک بین همه ی فایل ها رو پیدا کنم که بیشت رکاربرد امنیتی داره"
"-> سلام کسی کد yolo برای pytorch داره
-> scratch benevisi
-> scratch رو می خوام چون می خوام کد اون جعبه های خروجی yolo رو ببینم"
"-> سوال در پایچارم برای هر پروژهای که میسازیم باید پکیجها را مجدد دانلود و نصب کنیم
-> واقعا ممنونم از توضیحات جامع و کاملو همینطور وقت و حوصله ای که بخرج دادین
-> سوال دیگه اینکه حالا که متوجه اشتباهم شدم آیا میشه پکیج های نصب شده در محیط venvرا به محل درست انتقال داد یا مجدد باید نصبشون کنماگر مجدد نصب کنم چگونه قبلی ها را پاک کنم تا حجم اضافه را اشغال نکنند ببخشید سوالم طولانی شد
-> خیر نمیشه برید به آدرس هریک از venvها و اون پوشه رو حذف کنید"
"-> سلام در پایتون برای هر پروژه ای که می سازیم باید پکیج ها را مجدد دانلود و نصب کنیم یا راهکاری داره من نمی دونم
-> خیر لازم نیست اگر از سیستم local استفاده میکنید اگر یکبار نصب کنید میتونید بعدا در پروژه های بعدی import کنی اگر در فضای کولب هستید ممکنه بعضی کتابخانه ها نصب نباشند به عنوان مثال tensorboard که هر سری باید نصب کنسد و سپس import کنید
-> سیستم localهست ولی ایراد میگیرهمی زنه syntax error"
"-> سلام وقت بخیر من میخوام از گوگل درایوم یه فایلی رو به کولب بریزم و از gdown استفاده کنم متاسفانه هر کاری کردم نشده روش خاصی دارد
-> سلام من بعد از مانت کردن گوگل درایو از دستور cp برای کپی داده از گوگل درایو به کولب استفاده میکنم
-> خیلی ممنونم استاد"
"-> سلام چجوری میشه در پایتون یک متغیر را که چندین بار تکرار شده را با فشار کدام کلید میشه تمام متغیرهای هم نام را انتخاب و تغییر داد
-> سلام در پایچارم altj در کولب ctrld در vscode رو نمیدونم
-> Vs code هم مشابه کولب هستش
-> ممنون استاددر پایچارم منظورم بود"
"-> سلام وقت همگی بخیر صرفا با داشتن وزن های شبکه آموزش دیده میشه پردیکت انجام داد
-> وزنها و گراف ساختار شبکه بله
-> خیلی ممنون روند جفتشون یکی هستش یا همراه وزن ها باید یک شبکه هم ساخت کرد
-> بستگی به فریمورک داره در پایتورچ گراف رو در قالب کدها داریم و وزنها رو هم ذخیرهلود میکنیم
-> خیلی ممنونم"
"-> بخوام سوالمو به صورت تصویری بگم اینه که از قسمت ۱ یک hidden state خارج میشه یا مجموعا از قسمت ۲ یک hidden state خارج میشه اگر از قسمت ۱ یک hidden state خارج بشه این hidden state در unit های مختلف چطور باهم ترکیب میشن و کلا یک hidden state نهایی برای هر تایم خارج میشه
-> یک هیدن استیت به ازای هر یونیت
-> خب مثلا اگه ۳ تا unit داشته باشیم پس ۳ تا بردار hidden state در هر تایم داریم ولی در lstm در هر تایم یک hidden state خارج میشه این hidden state نهایی هر تایم چجوری تشکیل میشه ایا جمع عضو با عضو بردار های هر unit هست ممنون عذرمیخوام سوالهام زیاد شد
-> پیشنهاد میکنم دوباره شبکه LSTM رو از وبلاگ بخونید
-> ممنون حتما"
"-> سلام مهندس جان وقتت بخیر در این مطلب که ورودی و ht1 باهم concat میشن تعداد نورون های fully connceted دوتا لایه قانون خاصی داره یا فقط اینه که خروجی concat شده ابعادی به اندازه Ct1 داشته باشه تا ضربشون قابل انجام باشه ممنون
-> 
-> بسیار سپاسگزارم بابت توضیحات خوبتون یه مفهوم مرتبط دیگه ای هم اگر راهنمایی کنید کمکبزرگیه طبق شکل زیر اگر هر یک unit یک بردار به طول hidden size بده یعنی مثلا اگر ۳ تا unit داشته باشیم در نتیجه ۳ تا hidden state به طول hidden size ۳ داریم در نهایت زمانی که خروجی نهایی LSTM از hidden state های unit های مختلف میخواد تولید بشه این hidden state ها چطور باهمترکیب میشن منظورم در تصویر زیر هست ممنون که علمتونو بدونهیچ توقعی به اشتراک میذارید
-> مهندس سایت تمرین و اموزش پایتون رو میشه لینک بدید
-> گاهی اوقات فقط هیدن استیت آخرین استپ رو برمیدارن گاهی هم از همشون استفاده میکنن بستگی به تسک داره نمیشه فرمول کلی صادر کرد مثلا در کلاسیفیکیشن همه خروجیها رو میشه به کلاسیفایر داد
-> بله متوجه ام که هیدن استیت اخرین استپ روبرمیدارن ولی سوالم رومتوجه نشدین شاید منظورمو نمیتونم درست منتقل کنم دوتا برداشت دارم که نمیدونم کدومش درسته فرض کنیم یک lstm داریم با hidden size 3 منظورم از هر یک lstm cell به تعداد time steps هست برداشت اول ۱ یک lstm cell وجود خواهد داشت و مجموعا یک hidden state استپ اخر داریم که طولی برابر hidden size داره و همون به عنوانخروجی lstm برمیگرده برداشت دوم ۲ به تعداد hidden size ما lstm cell داریم که هرکدوم به صورت جداگانه اون عملیات گیت ها رو انجام میدن و در واقع هرکدومشون یک hidden state استپ اخر دارن که مجموعا ۳ تا hidden state استپ اخر خواهیم داشت که یه جوری این سه تا بردار ترکیب میشن و یک hidden state نهایی به عنوان خروجی lstm برمیگردونن که نوع ترکیب شدنشونو نمیدونم که جمع میشن یا جور دیگس ممنون"
"-> سلام بنده یک مدل تشخیص اشیا پیاده کردم الان میخوام اون رو ماژولار کنم یا به صورت API درش بیارم ممنون میشم دوستان راهنمایی کنند چطور میتونم این رو انجام بدم
-> ای پی ای آنلاین یا مثلا sdk سی شارپ
-> نه آنلاین نباشه"
"-> سلام دوستان من این مدل object detection روبا TF ObjectDetection API پیاده کردم که دقت تشخیص هم بر اساس پارامترهایی که تنظیم شده بود قابل قبوله هرچند نیاز به پیشرفت داره فقط سوالم اینه آیا لازمه بعضی وقت ها شبکه های عصبی رو خودمون از اول طراحی کنیم یا مدل های pre train کارمون رو راه میندازن
-> معمولا از شبکههای آماده استفاده میشه"
"-> سلام اگه بخوام یه سیستم تشخیص تصویر بنویسم که توی شرایط جوی مختلف باز بتونه اشیا رو تشخیص بده مثلا دوربینی که حتی در گرد و غبار هم خودروهای خیابان رو تشخیص بده چه راه حلی وجود داره
-> سلام راه حلش این هست که شروع کنید و دست کیبورد بشید هادی جان این نوع سوالها هیچ جوابی ندارن و شما باید این پروژه رو شبیه یک سفر پرفراز و نشیب ببینی و خودتو برای همه نوع مشکلات آماده کنی با این سوالها فقط زمان از دست میدید اگر سیستمی تا این اندازه دقیق و قوی میخوایید پس باید در اثر کار زیاد با شبکههای مختلف به سطح بالایی از دانش و مهارت در کدنویسی برسید که خودتون به این سوال جواب بدید تجربه کار با دانشجوهای مختلف به من میگه که شما الان در وضعیتی هستید که نمیدونید چکار باید انجام بدید اگر درست حدس زدم پیشنهادم این هست که فقط شروع کنید ترین و ارزیابی دقیق یک شبکه دیتکشن روی یک دیتاست بهترین نقطه برای شروع هست من به سوالاتی که مربوط به دورهها نباشه کمتر جواب میدم
-> Adverse weather condition بزنید گوگل مقالات زیادی داره"
"-> سلام در جلسه detection با اضافه کردن کتابخانه albumation تغییراتی رو روی ورودی انجام دادید موقعی که transform رو با albumation انجام دادید کد زیر رو به بخش خواندن دیتاست اضافه کردید targets 2 targets 1 targets 3 targets 0 اما این بخش باعث میشه باندینگ باکس نسبت به حالت معمولش بزرگتر بشه اگر هم این بخش حذف بشه باندینگ باکس اندازهش درسته میشه ولی موقع train خطای زیر میاد all bounding boxes should have positive height and width found invalid box for target at index 0 تو نت هم سرچ کردم نتونستم برطرفش کنم پیشنهادی برای رفع این خطا دارید سوال دوم شبکه رتینا برای باندینگ باکس Xmin Ymin width hight رو میگیره کد زیر targets 2 targets 1 targets 3 targets 0 باندینگ باکس رو به به حالت Xmin Ymin Xmax Ymax تبدیل میکنه درست میگم ممکنه خطای positive hight width به خاطر این باشه که شبکه رتینا مقادیر کوچکتری نسبت به طول و پهنا میگیره
-> سلام باید به کدها نگاه کنم چیزی که برای من عجیب هست اینه که جمع اندیس 1 و 2 یا اندیس 0 و 3 شاید درست نباشه معمولا اندیس 0 و 2 یا اندیس 1 و 3 باهم جمع میشن با این جمع کردن از حالت xywh به xyxy تبدیل میشه
-> شما درست میگید کد دقیقا 2 با 0 هست و 3 با 1 ولی بازم همون خطاهایی که گفتم رخ میده
-> پس باید دقیق نگاه کنم درهرصورت اگر باکس بزرگتر از اندازه واقعی میشه احتمالا در جمع کردن و تبدیل مختصات اشتباهی رخ داده البته خود کتابخونه آلبوم بعد از آگمنت کمی باکسها رو بزرگتر از اشیای درون تصویر میکنه
-> مخصوصا وقتی تصویر رو ریسایز میکنیم تغییرات باندینگباکس زیاد میشه البته اگر قسمت جمع 0 و 2 1 و 3 رو از بخش خوندن دیتا حذف کنیم تصویر و باندینگباکس به یک اندازه ریسایز میشن ولی در این حالت موقع ترین خطای positive hight va width رخ میده"
"-> سلام سوالم در رابطه با یادگیری_انتقالی هست فرضا شبکه ای رو آموزش دادیم که ۸۰ کلاس داره و وقتی از صحت شبکه مطمئن شدیم و خواستیم اون شبکه رو واسه نمونه های جدید تعمیم بدیم دیگه نمیخوام اون ۸۰ تا کلاس قبلی داخلش باشه فقط همین کلاس جدید راه حلی وجود داره
-> این سوال رو به این دلیل پرسیدم که چون تعداد نمونه ها زیاد بشه دقت کم میشه حالا من میخوام از شبکه های پیش تعلیم داده شده ای استفاده کنم که پیش فرض روی دیتاست COCO آموزش دیدن میخوام این دیتاست داخلش نباشه چون نمونهام یه کلاس شخصی شده هست
-> سلام از توضیحات متوجه شدم که کار آبجکت دیتکشن هست کافی هست شبکه مدنظر رو با وزنهای پریترین روی دیتاست مدنظرتون ترین کنید همزمان با یادگیری آبجکتهای جدید آبجکتهای قدیم رو کمکم فراموش میکنه
-> استاد ممکنه بیشتر راهنمایی کنید میخوام در حین یادگیری تعداد کلاس ها رو تغییر بدم چیزهایی که قبلا ترین شده از بین بره
-> 
-> ممنون از راهنماییتون کامل متوجه شدم"
"-> سلام دیتاستی که سایز عکسها متفاومت هستو وقتی ریسایزش میکنیم باندینگ باکسهاشم ریسایز میشه یا برای باندینگ باکس باید کار دیگه ای کرد
-> بله باید ریسایز بشه"
"-> سلام وقتتون بخیر عذر میخوام من دارم از کولب پرو استفاده میکنم اما فقط ۲۵ گیگ از رم ۲۵ گیگی و ۴۳ گیگ از دیسک داره استفاده میکنه و هر اپکم ۱ ساعت طول میکشه که در حالت عادی چنین نبوده بنطرتون مشکلش چیه
-> سلام احتمالا مشکل از کدهای شماست مثلا ممکن هست مدل و داده رو به کودا منتقل نکرده باشید یا مثلا داده رو از گوگل درایو میخونید طبق تجربه یکی از دوستان این مورد خیلی خیلی زمان اجرا رو زیاد میکنه
-> خب من قبلا هم داده رو از گوگل درایو میخوندم اما انقدر زمانبر نبوده و اینکه دارم هم از کودا استفاده میکنم
-> ما معمولا داده رو از گوگل درایو کپی میکنیم توی کولب و بعد آدرس داده در کولب رو به دیتاست میدیم خوندن مستقیم از گوگل درایو میتونه زمان رو زیاد کنه جز این دو مورد من دیگه موردی به ذهنم نمیرسه
-> ممنونم ازتون اقا
-> این کار چقدر روی حجم اینترنت مصرفی تاثیر داره مثلا یه دیتاست ۲۰ گیگ رو بخوایم بریزیم روی کلب
-> نمیدونم راستش چه سوال جالبی شاید در آینده بررسی کنم"
"-> وقتی لود میکنم اینجوری میاد
-> 
-> "
"-> شبکه رو با لایه هاش لود میکنه منم نمیدونم چطوری باید بیارم تو اون حالتی که دوباره اجرا بشه
-> اما این رو خوب متوجه نمیشم ولی فکر کنم منظورتون این هست که وزنها بود نمیشه"
"-> سلام من وزن های مدل رو با روش save dictسیو کردم حالا میخوام اونها رو لود کنم با loadهم لود میکنم اما نمیدونم چطوری باید برگرده همون نقطه ای که قطع شده
-> سلام همون نقطه که قطع شده یعنی چی لطفا بیشتر توضیح بدید
-> مثلا تو اپک ۵ قطع شده و جی پی یو رو گوگل گرفته من میخوام دوباره از اپک ۵ به بعد اجرا کنم
-> معمولا این کارها انجام میشه مدل با torchsave ذخیره میشه بعدش هم لود میکنید اما باید پارامترهای بهینهسازتون رو هم ذخیره کنید که در ادامه کار اون پارامترها رو هم روی بهینهساز لود کنید
-> خب اینکارو انجام دادم با save dictو اپک و بهینه ساز و تابع لاس رو ذخیره کردماما موقع لود خوده شبکه رو لود میکنه
-> راستش من این جملات رو متوجه میشم اگر درست ذخیره کرده باشید به راحتی با یک دستوری شبیه دستور زیر میتونید وزنها رو لود کنید modelload_state_dicttorchloadweights_pathpt
-> ببینید خوده مدل لود میشه من میخوام دوباره برگرده به همون حالت اجرا و اپک اپک اجرا کنه
-> یه ویس بفرستید قشنگ توضیح بدید همون حالت یعنی چی کدوم حالت"
"-> ببخشید یه سوال دیگه هم داشتم مزیت یادگیری عمیق نسبت به دیگه روش ها توی زمینه با ناظر چی هست و چه چیزی یادگیری اون رو مهم می کنه
-> 
-> واقعا ممنونم از توضیح کاملتون"
"-> سلام خداقوت ببخشید یه سوال داشتم دوره یادگیری عمیق جز مباحث با ناظر هست یا میشه موارد دیگه رو هم با یادگیری عمیق حل کرد
-> سلام دوره یادگیری عمیق ما که کلا برپایه یادگیری با ناظر هست اما یادگیری عمیق مسائل بدون ناظر نیمه نظارتی و خودناظر رو هم میتونه حل کنه اتفاقا مباحث بدون ناظر نیمه نظارتی و خودناظر از مباحث داغ حوزه هوش مصنوعی در یک دو سال اخیر هستن
-> "
"-> سلام سی درصد تخفیف دوره ها تا کی هست
-> سلام احتمالا تا جمعه"
"-> سلام وقت بخیر امکانش هست برای دوره بینایی کامپیوتر حرفهای تخفیف بگذارید
-> سلام بله حتما"
"-> سلام توی یکی از دوره هاتون یه سایتی رو معرفی کردید که مقاله های جدید و مربوط به حوزه کاری رو میتونستیم سرچ کنیم لطف میکنید اینجا بگید مجدد
-> سلام arxiv arxivsanity paperswithcode"
"-> سلام برنامه نویسی پردازش تصویر پایتون تو حوزه سنجش از دور هم کاربرد داره
-> اگر گرایش remote sensing از عمران رو میگید بله
-> من سال نود و چهار یادمه یه شرکت کامل همه شون در این زمینه کار می کردن اون موقع همه با متلب کار می کردن الان احتمالا با اومدن یادگیری عمیق اونها هم رفتن پایتون
-> تازه میخواستم سنجش از دور کار کنم منتهی از قبل پردازش تصویر و یادگیری عمیق کار کردهم میخواستم ببینم کاربرد داره داخلش
-> دقیق نمی دونم احتمال میدم داشته باشه"
"-> تنسورفلو
-> می تونید یه سرچ بزنید من به دستورهای تنسورفلو مسلط نیستم
-> خیلی ممنون همین که راهنمایی کوچیکی باشه در حد معرفی که بدونم چی سرچ کنم خیلی خوب ممنون و متشکرم"
"-> تقریبا میشه گفت هردو مکانیزم به بلوغ نرسیدند اقای دکتر به نظر شما واسه پیاده سازی تجاری این سیستمکدوم رو شما توصیه میکنید
-> سلام من در این حوزه مطالعه نداشتم نمیدونم چه رهیافتی بهتر هست
-> خیلی ممنون"
"-> سلام و عرض ادب در دوره پیشرفته ی بینایی ماشین به صورت کامل open cv تدریس شده
-> سلام خیر توی بینایی کامپیوتر با ابزار یادگیری عمیق اوپنسیوی چندان جایگاه مهمی نداره معمولا کارها براساس پایتورچ و تنسورفلو انجام میشه"
"-> سلام این تابع زیان و اگر بخوام تقسیر کنم میگم تا اپک 10 لاس فانکشن خوب اومده پایین ولی چون دو باره داره میره بالا یعنی اورفیت شده پس یه بار دیگه کلا مدل رو میذارم 10 بار ترین بشه
-> این چیزی که گفتم درسته
-> 
-> از lr scheduler استفاده کردم
-> این تصویر داده های ترین هست یعنی هیچ ربطی به اورفیت شدن نداره
-> ببینید تا وقتی که روی داده های ولیدیشن ارزیابی نکرده باشید متوجه نمیشید که اورفیت شدید یا نه
-> سلام شب بخیر میشه لطفا چنتا از روش هایی که لرنینگریت اسکجولر داره معرفی کتید
-> سلام از چه فریم ورکی استفاده می کنید پایتورچ تنسورفلو و
-> 
-> خیلی خیلی ممنون
-> خواهش می کنم"
"-> به نظر شما بهترین روش درک و یادگیری همه ی مدل شهای شیکه GAN مطالعه ی مقالات این شکه هستش یا اینکه از رفرنس هیا معتبر استفاده کنم سرع تر و با کیفیت تره به این خاطر که برا ی اجرای پروژه ای نیاز به درک بهتر GAN دارم
-> میتونید انواع مقالات گن رو سرچ کنید و اون هارو بخونید
-> بهترین راهکاره
-> من فکر میکنم خوندن مقاله در این مورد بهتر بتونه کمک کنه"
"-> اساتید محترم سلام عرض ادبیک سوال داشتم تو بعضی از مقالات بعد از اینکه بلوکهای کانولوشنی رو گزاشتن آخرش به جای یک fc گاها دیدم دو تا fc گذاشتن دلیلشو نفهمیدم امکانش هست راهنماییم بفرمایین ممنون
-> سلام توی شبکههایی که اوایل پیشنهاد میشد تعداد لایههای فولی کانکتد بیشتر از یکی بود مثل vgg و قبلش اما شبکهای مثل گوگلنت فقط یک لایه گذاشت که نقش کلاسیفایر رو داشت اینطوری هم میزان پارامترها به شکل قابل توجهی کم شد و هم شبکه قادر بود ورودی با هر سایزی رو دیگه قبول کنه این شد که بعد گوگلنت شبکهها مبتنی بر یک لایه فولیکانکتد پیش رفتن البته سالهای بعدش کارهایی توی تسکهای دیگه بود که با بیش از یکی کار میکردن بهتر هست که یک لایه گذاشته بشه
-> ممنون استاد
-> 
-> پس تقریبا میشه گفت بستگی به مقدار تفکیک کنندگی فیچر های خروجی لایه های کانولوشنی داره
-> 
-> ممنون استاد
-> "
"-> عزیزان طرح کارزار اینترنت رو لطفا امضا کنید نزدیک هست که یک میلیون امضا بشیم لطفا نگید چه فایده ما باید تلاشمون رو بکنیم و مخالفت خودمون رو به شکل درستی نشون بدیم به دور از اینکه نتیجه چه خواهد شد اینترنت آزاد حق من و شماست یاعلی
-> وقتشه یاد این موضوع بیافتیم و نگرانی ای که اون موقع داشتیم الان این کارزار بیش یک میلیون و صد هزار امضا داره اما به نظر می رسه هیچ تاثیری نداشته هیچ کدام نماینده هایی که امروز کلیات طرح صیانت رو در چهل و پنج دقیقه تصویب کردن همه با هم روی هم ششصد هزارتا رای نداشتن اما این تصمیم رو برای همه مون گرفتن
-> کارزار بی فایده س من امروز یه ویدیو دیدم اون یه نفر نماینده ایی هم که داشت مخالف صحبت میکرد اون ۱۸ تا بهش مجال صحبت کردن نمیدادن بنده خدا چندین بار اعتراض کرد اون ک نماینده س بغل گوش شون هست ب حرفش گوش نمیدن چه رسد به کارزار و این داستانا"
"-> سلام بچه ها من تو کولب با کتابخونه patches تصاویرم رو بخش بندی کردم ولی الان نه تو درایو پچ ها رو میتونم پیدا کنم نه تو کانتنت کجا رفته فایل ها
-> توی خود متغیر patches هست دیگه ظاهرا
-> الان خود پچ ها رو میخوام ببینم باید چیکار کنم
-> باید دستوراتی باشه که بتونید به صورت عکس ذخیره کنید در فولدر دلخواهتون
-> نمیدونم"
"-> سلام در مورد اینفرنس و ارزیابی با پایتورچ دو تا کار انجام میدن torchno_grad modeleval یک این دو تا کار هر دو ضروری هستن دو فکر کنم no_grad فقط سرعت کار رو بالا می بره چون دیگه فریم ورک نیاز نداره گرادیان ها رو ذخیره کنه سه بعضی لایه ها مثل دراپ اوت و بچ نرمالایزیشن باعث میشن رفتار شبکه در فوروارد در حالت آموزش و ارزیابی تفاوت داشته باشه چهار با modeleval در واقع همین تغییر رفتار شبکه رو اعمال می کنیم
-> "
"-> We use n 3 in this paper noting that the effective receptive field on the input image is large 171 and 228 pixels for ZF and VGG respectively
-> این نوشته هم تو مقاله اصلی اومده مگه receptive field تو VGGیک پنجره سه در سه نبود 228 رو از کجا اورده
-> در لایه های انتهایی که ویژگی های سطح بالا رو داریم وقتی برمیگردیم و به ورودی نگاه میکنیم تعداد پیکسل هایی به این اندازه میبینیم یک فیلتر 3در 3 وقتی اعمال میشه روی ناحیه ای از یک تصویر خروجی یک پیکسل میده و receptive field اون یک ناحیه ۹ پیکسلی خواهد بود"
"-> Each sliding window is mapped to a lowerdimensional vector 256d for ZF and 512d for VGG
-> تعداد sliding window ها هست فک کنم
-> یعنی تو نقشه ویژگی 256 پنجره لغزان داریم
-> کلا ۲۵۶ تا فیلتر داریم که روی تصویر ورودی اعمال میشه و وزن های اون در روند training ست میشه
-> "
"-> دوستان کسی از جزءیات طرح اینترنت خبر داره واقعا سرویس های خارجی قطع میشه با این کار یعنی مرگ برنامه نویس و حوزه ای تی
-> به ویس بالا گوش بدید شاید کمی کمکتون کنه"
"-> سلام به همه دوستان میبخشید من دانشجوی ارشد علوم کامپیوترم و در دوره کارشناسی و حتی الان در ارشد هم هیچ درسی در زمینه پردازش تصویر و بینایی ماشین نگذروندمدروس مرتبطی ک گذروندم داده کاوی والگوریتم هاش و یادگیری ماشین بوده وبا توجه به اینکه در زمینه کدنویسی تواین زمینه مبتدی و شاید ضعیفم الان واسه پایان نامم خیلی نگرانم ودچار ابهامم آیا پیشنهاد میدید درزمینه داده کاوی واردشم یا صرف اینکه علاقم به پردازش تصویر یکم بیشتره سمت پردازش تصویر بیام و اگه بخام تو این حیطه واردشم دوره پردازش تصویر استاد چقدر میتونه کمکم میکنه و آیا باید دوره بینایی رو هم بگذرونم تا بتونم از پس پایان نامه بربیام خیلی ممنون میشم اگه بتونید راهنماییم کنید که زودتر شروع کنم واسه یادگیری بیشتر و تقویت کدنویسیم متشکرم
-> 
-> سلام به نظر من آسون ترین راه انتخاب کنید ارشد زمان کمی دارید و در ایران قطعا وقت زیادی برای ارشد گذاشتن صحیح نیست هر مسیری را انتخاب کنید از علاقتون دور نمیشید در کنارش یادگیری عمیق هم کم کم شروع کنید که عالی میشه انشالله اگر دکتری خواستید ادامه بدید و خارج از کشور خواستید برید می تونید علاقتون را دنبال کنید داشتن سیستم قوی از پیش نیاز های وارد شدن به بینایی ماشین است مگر اینکه خواسته باشید کار های سبک انجام بدید گوگل کولب کار شما را حل نمیکنه
-> خیلی ممنون و متشکرم استاد از توضیحات جامع و کاملتون درسته فرمایشتون و با توضیحاتی که فرمودین اگه تو زمینه یادگیری ماشین کار کنم احتمالا راه سهل تری در پیش دارم فقط یه سوال دیگه اینکه توضیحاتتون بیشتر پیرامون یادگیری پایتون بود در زمینه یادگیری ماشین هم بهتره که پایتونم رو تقویت کنم یه دوره مقدماتی پایتون رو گذروندم و با متلب هم تاحدودی اشنا هستم پیشنهاد شما اینه ک الان تمرکزم رو بیشتر روی کدام بزارم بازم ممنونم از پاسخ گویی شما و بقیه دوستان
-> و همینطور متشکرم ازشما
-> پایتون برای یادگیری ماشین هم نیاز هست
-> سلام منم ارشدم سخت افزار میخونم ولی استاد راهنمام علاقش به حوزه هوش مصنوعی خصوصا یادگیری عمیق بیشتر بود کاملا از صفر صفر شروع کردم چه برنامه نویسی پایتون چه تئوری ها چه خوندن مقاله ها خیلی زمان ازم گرفته حدود ۱۱ ماهولی کلا من مدلم اینجوریه که هر چیزی یادبگیرم علاقم بهش زیاد میشه باید ببینید ترجیح شما چیه رفتن دنبال علاقه کلی استرسم همراش هست اما اینکه پرسیدین تو برنامه نویسی ضعیف هستین وارد کدوم حوزه بشین بنظر من اگه تسلطتون تو درسایی که گذروندین بیشترههههه تو همون حوزه کار کنید تمرین پایتون بیشتر کنید از سایتی مثل querair سوالایی که بیشتر حل شده رو وقت بذارید با سرچ زیاد سعی کنید به جواب برسید من اینجوری کار کردم این کار کمک میکنه بعدا که کدای دیگران میخونین بهتر درکشون کنید اما بخش سوم سوالتون دوره پردازش تصویر با پایتون به من در حدی که مقاله بخونم و بفهمم منظورش از هیستوگرام چیه dailation و خوب بوده و کمکم کرده اما نکته اخر ببینید استاد راهنماتون چه زبان برنامهنویسی مد نظرش و چه فریم ورکایی
-> خیلی ممنونم از راهنماییتون"
"-> سلام وقتی یه مدل ترین شد و بعد خواستیم ازش استفاده کنیم مثلا روی ویدیو خب از کجا بفهمیم چه پیش پردازش هایی باید روی تصاویر انجام بشه تا با مدل همخوانی داشته باشه مثلا تغییر رنگ سایز و
-> سلام طبیعتا اگر خودتون مدل رو ترین کرده باشید میدونید چه پیش پردازشهایی موقع ارزیابی در فرآیند ترین انجام شده اگر هم خودتون ترین نکردید و از مدل آماده استفاده میکنید باید به سورس کدهای مدل نگاه کنید ضمن اینکه به خاطر داشته باشید که دیتا آگمنت فقط در ترین انجام میشه معمولا ارزیابی صرفا شامل یک ریسایز ساده هست
-> پس هنگام استفاده از مدل باید دقیقا همون پیشپردازش هایی رو انجام بدیم که قبل از ترین شدن اعمال کرده بودیم
-> همون پیش پردازشهایی که در validation حین train انجام دادید
-> ببخشید برا ریسایز چطور تشخیص بدیم چه مقدار باید تغییر بدیم
-> بازهم باید به کدهای ترین نگاه کنید مدل با چه سایزی ترین شده با همون سایز ترین باید بهش ورودی بدید
-> تشکر از راهنماییتون"
"-> سلام و عرض ادب برای تمرین کردن CNN من امدم ۷۰هزارتا عکس در mnist ریختم داخل یک پوشه و سپس train و test و validation با دستور random_split جدا کردم اینکار من درسته یا اینکه طبق روش تدریس شده برم جلو و فقط validation از train جدا کنم توی دیتاست های موجود در سایت پایتورچ validation نداره
-> سلام کار اشتباهی انجام ندادید البته میتونید در پوشه تصاویر رو ذخیره نکنید و مستقیم بعد از لود دیتاست به سه قسمت تقسیم کنید
-> ممنون"
"-> سلام بعضی از دیتاست های تصویر یه فایل csv کنارشون هست و معمولا حجمشم زیاده ولی بعضی دیتاست ها فقط عکس هستن که توی فولدر های جدا دسته بندی شدن تفاوتشون توی چیه اصلا اون فایل annotation ینی چی
-> سلام دیتاستها شکل استاندارد و واحدی ندارن از طرفی به کاربرد دیتاست هم بستگی داره مثلا annotation معمولا به ground truth اشاره داره مثلا اگر مربوط به کلاسیفیکیشن باشه میشه لیبلهای هر تصویر اگر دیتکشن باشه میشه باندینگ باکسها بعضی دیتاستها این annotation ها رو در فایلی مثل txt csv xml میذارن"
"-> سلام واسه تشخیص تصاویر مدل رو train کردم و خروجی رو به صورت فایل h5 ذخیره کردم حالا یه لیست ساختم و نام کلاس ها رو داخلش ریختم برای استفاده از مدل داخل ویدیو آیا ترتیب نام کلاسا داخل لیست توی تشخیص تاثیر میذاره مثلا این دوتا توی تشخیص فرق دارن Labelscat dog bird Labelsbirdcatdog
-> سلام داده های train اگر لیبل داشتن لیستم باید به همون ترتیب باشه وگرنه ممکنه درست پیش بینی کنه که مثلا عنصر اول لیست جوابه ولی خب میاد به ترتیبی که تو لیست نوشتین نگاه میکنه اگه لیست اول استفاده کنید dog رو میگه اگه لیست دوم cat
-> داده هام لیبل ندارن عکس ها داخل فولدر های جدا هستن بخاطر همین اومدم یه لیست جدا واسه لیبل ها ساختم که به مشکل برخوردم جابجا نشون میده راهی وجود داره که ذخیره مدل لیبل ها هم خودکار ذخیره بشن
-> نمیدونم"
"-> سلام معادل فارسی mAP چیه
-> من چیزی نشنیدم خودم تو مستندسازی هام می نوشتم معیار mAP اگر پیدا کردید به من هم بگید ممنون میشم اگر معادل فارسی precision رو بدونید می تونید بگید میانگین کلی precision البته این یه پیشنهاده"
"-> سلام بعد از دانلود و ران کردن پایگاه داده MNIST ارور زیر می گیرم چطوری می تونم برطرفش کنم UserWarning The given NumPy array is not writeable and PyTorch does not support nonwriteable tensors This means you can write to the underlying supposedly nonwriteable NumPy array using the tensor You may want to copy the array to protect its data or make it writeable before converting it to a tensor This type of warning will be suppressed for the rest of this program Triggered internally at torchcsrcutilstensor_numpycpp180 return torchfrom_numpyparsedastypem2 copyFalseviews
-> عکس از اون بخش رو هم بفرستید"
"-> سلام هدف از اینکه یه دیتاست مثلا mnist به صورت 600002828 رو میایم ریشیپ میکنیم به صورت 60000784 چیه چرا باید اینکار رو انجام بدیم
-> Mnist رو میشه هم با شبکه عصبی زد و هم کانولوشنی برا شبکه عصبی میاند همچین کاری میکنند تعدادش که ثابته و شصت هزار تا هست که چون عکسه ۲۸ ۲۸ هست و سیاه سفید رنگی نیس که میاند فقط یک بردار درست میکنند و میدند به شبکع ۲۸ ۲۸ ۷۸۴ چون خروجی هم یه بردار هست خروجی باید وان هات باشه که بگه از صفر تا نه کدومه
-> ممنون"
"-> سلام چطوری میشه از روی نمودار فهمید overfitting رخ داده یا نه
-> اورفیتینگ یعنی خطا روی تست دوباره زیاد بشه یعنی بعد از سیر کاهش دوباره سیر افزایشی پیدا کنه درحالیکه روی داده آموزش همچنان کاهشی باشه یک اشتباه رایج این هست که وقتی خطا روی داده ترین خیلی خیلی کم باشه یا دقت در ترین خیلی خیلی بالا باشه اما دقت روی تست کمتر باشه رو اورفیت میگن درحالیکه این اصلا اورفیت نیست نمودار شما اورفیت نیست"
"-> سلام receptive field رو هم میشه تغییر داد یا یه چیز ثابته
-> با تغییر استراید و لایههای شبکه میشه تغییرش داد
-> میشه یه مقدار بیشتر توضیح بدید لایه استراید مگه فقط برای همپوشانی نیست به خود اندازه receptive filed هم مربوطه"
"-> سلام وقت همگی بخیر در یک مدل یادگیری عمیق که با کراس برای مسئله طبقه بندی طراحی شده چطور میتونم ماتریس کانفیوژن رسم کنم
-> سلام یکی از سادهترین راهها استفاده از scikitlearn هست دستوری برای confusion matrix داره دو خروجی predictions و targets رو میگیره
-> خیلی ممنون بله این دستورات رو دیدم اما به چند جور ارور برخوردم یکی این که مثلا میگفت این کتابخانه فقط برای مسئله کلاس بندی
-> متاسفانه موفق به استفاده ازش نشدم نیاز عملیات خاصی روی خروجی های پردیکت انجام بشه
-> یک اشتباه رایج در پایتورچ این هست که تنسورها pred و target رو تبدیل به آرایه نامپای نمیکنن و از gpu به cpu انتقال نمیدن در کراس نمیدونم به چه شکل هست
-> اهان خیلی ممنونم پس باید به ارایه تبدیل کرد و بعد به تابع داد"
"-> همون selective search عه
-> بله
-> ممنونم برام جای تعجب داره چرا یه مقاله به این مهمی و با اینهمه نویسنده های با سواد و نخبه همچین نکته مهمی رو تو مقاله اصلی شون نمیگن واقعا ای کاش میتونستم خودشون و از نزدیک ببینم سوال کنم
-> درمورد صحبت کردن و در بخش experiments تحلیل هم انجام دادن
-> "
"-> تا چند روز این چوری نبود فقط اون گزینه رو میزدم راحت به درایو وصل میشدم
-> جدی نگیرید احتمالا گوگل داره تست میکنه
-> حله"
"-> سلام نوت بوک های شما هم جدیدا این جوری شده من درایوم نصب میشه ولی اون خطی که روی علامت درایو بعد از نصب شدن میکشید رو دیگه نمیکشه
-> دکمه رفرش کنار دکمه درایو رو زدید
-> اره
-> امروز انواع و اقسام دردسر ها رو داشت
-> دیگه چی"
"-> سلام دوستان وقتی با کراس مدل train شد چه زمانی بدونیم مدل واقعا خوب هست یا نه الان مدل من دقتش ۸۰ درصده ولی واقعا درست تشخیص نمیده
-> با ارزیابیهای مختلف
-> ممنون میشم یکم بیشتر راهنمایی کنید
-> بستگی به جنس داده تون داره و اینکه چه کاری میخواید انجام بدین مثلا برای object detection میتونید از متد هایی مثل iou و استفاده کنید بهتره چند تا مقاله تو اون زمینه نگاه کنید خیلی راحت متوجه میشید"
"-> سلام یه سوالی که ذهنم و درگیر کرده اینه که چرا تو این لیست یولو نیست و مجبوریم یولو رو از گیت دان کنیم
-> سلام چون توسعهدهندههای پایتورچ نمیتونن همه مدلها رو در فریمورک بیارن و البته کار بی فایدهای هم هست تصمیم گرفتن نسخههای بهتر و محبوبتر رو بیارن رتینانت رو آوردن که بهتر از یولو هست ولی در ایران گویا محبوبیتی نداره
-> "
"-> من رو ویندوز ۷ استفاده میکنم ولی ویندوز ۷ تون باید آپدیت جدیدتر باشه اگر آپدیت کنید ممکنه حل بشه و اگه نشد یه ویندوز ۷ آخرین آپدیت دانلود کنید و نصب کنید
-> بله در متن مشکل نوشته win 7 SP1 مشکلی نداره و قابل نصبه
-> متشکر سرویس پک 2 یعنی نصب باشه
-> متشکر"
"-> ویندوز تازه نصب شده باید مجدد نصب کنم
-> نه منظورم این هست که ویندوز 7 قدیمی هست الان اکثرا ویندوز 10 نصب میکنن"
"-> ویندوز 7
-> احتمالا مشکل از همین هست ویندوز شما قدیمی هست شاید روی سرویس پک 1 نصب بشه"
"-> سلام دوستان من ویندوز تازه برام نصب شده اما چنین خطایی میگیرم هنگام نصب پایتون به نظرتون ویندوز کامل نصب نشده
-> چه ویندوزی"
"-> سلام واسه پیاده سازی شبکه های کانولوشن پارامتر ها مثلا تعداد لایه ها بر چه اساس تعیین میشه از کجا بفهمیم کی باید مثلا ۳ لایه بگذاریم و
-> 
-> تشکر از توضیحات بسیار کاملتون کاری که میخوام انجام بدم تشخیص اشیای خاص هستش حالا گفتیدResNet آیا استفاده از این معماری به عنوان یک تمپلیت هست که بشه برای شروع همه جا ازش استفاده کرد
-> بله تقریبا همه جا قابل استفاده هست"
"-> سلام من یگ پروژه با tensorflowkeras در زمینه segmentaion انجام دادم و بعد از train و test وقتی خروجی کار را دیدم که درست بوده میخوام اسم اون نقاطی که بر اساس اون mask تصاویر ساخته شده است را روی خروجی نمایش بده ولی اصلا بلد نیستم ممکنه یکی از دوستان راهنمایی کنه با تشکر
-> مزخاید رو تصویر سگمنت شده متن بزارید
-> اره روی همون نقاط که میحواستم detect کنه
-> Cv2puttext
-> بله اینو میدونم منظورم این هست میخوام همون اسمی که بعنوان lable بهش دادم را خودش بذاره کنار همون lable که detect کرده است
-> کسی ایده ای نداره"
"-> AttributeError module kerasbackend has no attribute control_flow_ops
-> 
-> "
"-> شما راهی به ذهنتون میرسه که یه کاری کنم با همین دستور تو کولب بتونم این کتابخونه رو فراخوانی کنم
-> سلام شما میتونید فایلهای موردنیاز رو در کولب آپلود کنید در واقع کدها و فایلها در مسیر content قرار داشته باشه یا اینکه ميتونيد کدها رو در گیتهاب بذارید و بعد در کولب clone کنید این روش بهتر هست با این دو روش مشکل شما حل میشه
-> "
"-> ثبت نام کاراموزی چیه جریانش
-> کجا"
"-> فایل csvرو چطور تشکیل میدین
-> یه ستونش میشه همین ماتریس تصاویر سایر ستون ها مختصات نقاظی است که روی تصویر گذاشته شده است"
"-> تو فایل csvببینین تصاویر هم اندازه هستند
-> اره از سایز تصاویر ایراد میگیره ولی دوتا موضوع است اول اینکه من همه تصاویر با کد resize کردم و اینکه توی csv پطور تست کنم اونحا که فقط ماتریس است البته باز هم همونماتذیس ها را میدادم به کد و نمایش میداد با همون سایزی که resize کرده بودم
-> توی فایل csv ابعد تصاویر با width وhight نشون داده شده اند"
"-> سلام دوستان من در لود داده در دیتاست به مشکل بر خوردم کل تصاویر را resize یعد reshape کردم و بعد هر تصویر را بصورت ماتریس به فایل csv اضافه کردم یعنی یکی از فیلدهای فایل csv میشه همون تصاویر بعد وقتی که میخوام load کنم تصاویر را خطای زیر را میدهد ValueError all the input array dimensions for the concatenation axis must match exactly but along dimension 1 the array at index 0 has size 8281 and the array at index 955 has size 8250 از دوستان کسی هست بتونه کمکی کنه ممنون
-> سلام بعد همه تصاویر شما باید یکسان باشه اینو امتحان کنین چیزیه که به نظر من رسید
-> من تمام تصاویر را از این طریق رقتم imgcv2resizeimg9696 img imgreshape9216
-> من موردی که فهمیدم از اخطار هم اندازه نبودن سایز تصاویر است"
"-> سلام با اجرای کد زیر خطای صادر میشه کسی میتونه راهنمایی کنه ممنون data_frameImage data_frameImageapplylambda i npfromstringi sep TypeError a byteslike object is required not float def data_loader Load dataset file data_frame pdread_csvdatatestcsv data_framePic data_framePicapplylambda i npfromstringi sep data_frame data_framedropna imgs_array npvstackdata_frameImagevalues 2550 imgs_array imgs_arrayastypenpfloat32 imgs_array imgs_arrayreshape1 96 96 1 labels_array data_framedata_framecolumns1values labels_array labels_array 48 48 labels_array labels_arrayastypenpfloat32 return imgs_array labels_array
-> بهتر است اول نوع خطا رو بررسی کنید اینکه کد رو بفرستید خیلی کمکی نمیکنه بهتون
-> بله درست میفرمایید خطا رو بررسی کردم متوجه نشدم کد و خطا را باهم گذاشتم ممنون ازشما
-> بله من خطا رو ندیدم
-> ایده ای ندارید برای حل مشکل
-> دوست من واضح هست خطا نوع type هست گفته نوع نباید float باشه پیشنهاد میکنم یک breakpoint بزارید و تریس کنید کد رو بهتر متوجه میشی همین خطا رو هم سرچ کن stackoverflow بهترین منبع هست
-> بله متوحه شدم ولی نمیدونم دقیقا نوع float کجا هست
-> اگر با pycharm یا vs code کار میکنی یک breakpoint بزار و دیباگ کن متوجه میشی دقيقا کجاست
-> ممنون"
"-> سلام Faster rcnn تصاویر رو به صورت پیش فرض به 1024در 1024 ریسایز میکنه
-> min_size800 max_size1333
-> مهندس منظورم اینه که اگه مثلا ما بیاییم یه تصویر بیست هزار پیکسلی در بیست هزار پیکسلی رو تو پایتورچ به faster rcnn بدیم برای اینکه این تصویر بتونه بعد از اینکه عملیاتی مانند ادغام و کانولوشن و روش انجام بشه در نهایت این تصویر رو به یه سایز fc مشخص برسونه همون اول میاد این تصویر رو به یه سایز مشخصی ریسایز میکنه که مشکلی پیش نیاد درسته یعنی اگه به این سایزی که ریسایز میشه حتی یه سطر و ستون ضافه یا کم بشه توی لایه fc مشکل پیش میاد درسته بعد این min saze و max size ای که شما اشاره کردین یه سایز ثابت نیست یعنی اگه بک بن ما مثلا vgg16 باشه به یه سایز مشخص باید تبدیل کنه و اگه مثلا resnet باشه باید به یه سایز مشخص دیگه تبدیل کنه باز هم نه یک پیکسل کمتر نه یک پیکسل بیشتر
-> حالا این min size و max size رو من متوجه نشدم ما خودمون که نمیتونیم به شبکه بگیم به چه سایزی ریسایز کن یعنی نمیدونیم اصلا بر طبق بک بن خود faster rcnn باید تشخیص بده که به چه سایزی تبدیل کنه پس این min size و max size چی هست
-> 
-> ممنونم مهندس از لطف تون خیلی از ابهاماتم حذف شد سپاس"
"-> سلام دوستان شرمنده شايد غير مرتبط باشه اما ميشه قيلتر شكن خوب يا سايتي معرفي كنيد براي ايفون من مدام قطع وصل ميشم
-> Poroton VPN به صورت مجانی چندتا سرور ارایه میشه
-> واقعا متشكرم نصب كردم و عالي شد سپاس گزارم"
"-> در دایرکتوری نیز ماژولها رو دارم
-> سلام فایل runexamplepy کنار بقیه ماژولا گذاشتید اجرا کنید ببینید میشه"
"-> سلام وقت به خیر من به یه پارادوکسی بر خوردم ما تو تعریف لایه کانولوشن میگیم که نورون ها تنها به یک ناحیه از تصویر یا نقشه ویژگی قبل از خود متصل هستند receptive field یعنی از کلمه ناحیه استفاده کردیم و نه نورون در صورتی که توی یه تعریف دیگه میگیم که کانولوشن یعنی ضرب نقطه به نقطه و ضرب نقطه به نقطه یعنی ضرب هر درایه یه ماتریس با درایه ی متناظرش یعنی مثلا اگر کرنل سایز 3 در 3 باشد پس receptive field ما هم قطعا 3در 3 خواهد بود پس حالا برای اینکه عملیات کانولوشن انجام شود درایه 1و1 فیلتر مون با درایه 1 و 1 receptive field ضرب میشه بعدش درایه 2و1 فیلتر وزن در درایه 2و1 receptive field ضرب میشه و در نتیجه هر وزن در کرنل سایز فقط به یک نورون با لایه قبلش ارتباط داره و نه یک ناحیه پس چه طوری عه که میگن در لایه کانولوشن هر نورون به یک ناحیه با لایه قبلش ارتباط داره که اسم اون رو receptive field گذاشتن یا به عبارتی دیگه اشتراک پارمتر چه طوری صورت میگیره
-> "
"-> سلام دوستان عزیز من میخام یک مدل ssd را با داده های شخصی train کنم تصاویر مربوط به داده های ریه هستش که تعدادی مربوط به افراد سالم هستش و bounding box ندارن من میخام داده هارو به فرمت pascal voc دربیارم تو فایل xml این داده ها جای bndbox چی قرار بدم درواقع ground truth وجود نداره اگر کسی میدونه ممنون میشم راهنمایی کنه
-> سلام از labelImg استفاده کنید میدونم میتونه labelهارو به فرمت pascal voc ذخیره کنه
-> متوجه سوال بنده نشدید یکسری تصاویر اصلا ground truth ندارن مانند تصویر اسکن ریه فرد سالم حالا برای اینکه شبکه بخاد loss حساب کنه نیاز داره که xywh را داشته باشه ولی برای چنین تصاویری اصلا bb تعریف نمیشه حالا میخام ببینم توی فایل xml چی باید بنویسم
-> نمیدونم تا حالا به این جور مسئله ای برخورد نکرده بودم
-> 
-> سلام آقای جواب خوبی دادند شبکه مدنظر شما سوپروایزد آموزش میبینه بنابراین داشتن Ground Truth ضروری هست باید مختصات شی هدف رو به شبکه بدید خیلی ساده بگم شبکه از کجا بفهمه شی هدف در تصویر چی هست
-> desirable that your training dataset include images with nonlabeled objects that you do not want to detect negative samples without bounded box empty txt files use as many images of negative samples as there are images with objects
-> سلام جناب اشرفی تو یولو ورژن ۳ این نکته اومده برای تصاویری که ground truth ندارن که فایل لیبل ا بصورت فایل تکست خالی قرار بدین
-> 
-> سلام اشتباه متوجه شدید این به هدف شما ارتباطی نداره در پیام قبلی خدمت شما گفتم اگر میخوایید تشخیص اشیا آموزش بدید داشتن Ground Truth برای شی هدف لازم هست"
"-> سلام وقت بخیر میشه گفت در واقع یادگیری عمیق همون شبکه های عصبی عمیق هستن
-> بله"
"-> سلام وقت همگی بخیر یک سیستم طرحی شده و این سیستم بین ۸۵تا۹۰ درصد در مینیمم لوکال گیر میکنه چه راه حل هایی برای حل این مشکل هست
-> دوستان لطفا اگر ممکن راهنمایی بفرمایید چطور میشه این مشکل رو رفع کردش"
"-> سلام خدمت استاد عزیز و دوستان گرامی ممکنه اسم نرم افزاری که ازش در ساخت annotations ها روی تصاویر برای پروژه های object detection segmentation و استفاده میشه را بفرمایید یک دیتاست دیدم خیلی جالب بود که با یکی از همین نرم افزار ها ساخته شده بود به این شکل که خود عکس راهم تبدیل کرده بود به یک ماتریس و در یک ستون مثلا بنام image ذخیره کرده بود و البته نقاطی هم که زمان لیبل کذاری انتخاب شده بود مختصات اونها رو هم در ستون های دیگه ای ذخیره شده بودند در واقع یک ردیف شامل مختصات نقاط لیبل گذاری شده بهمراه ماتریس خود عکس ذخیره شده بود میخوام همچین دیتاستی درست کنم ولی نمیدونم با چه نرم افزاری درست میشه ممنون میشم اگر کسی میدونه راهنمایی کنه با تشکر
-> Labelme"
"-> Think of loss function what to minimize and optimizer how to minimize the loss
-> سلام این جمله در مورد تفاوت بین loss function و optimizer درسته"
"-> سلام ؤقت بخیر دوستان می خوام در پایتورچ یک لایه شخصی و کاستوم شده بسازم مثل conv2d یا dense کسی می تونه راهنمایی کنه یا لینک مرتبط بهم بده با تشکر
-> سلام اگر یه لایه ساده باشه میشه به صورت یه تابع پایتونی تعریف کرد و توی قسمت فروارد مدل استفاده کرد اگر هم که یه لایه پیچیده باشه شبیه تعریف کردن یه مدل هست که از کلاس nnModule باید استفاده کنید اینجا یه مثال زده
-> "
"-> سلام وقت بخیر سایز فیلتر یا همون سایز کانولوشن همون receptive field هست یا فرق میکنه
-> سلام با هم دیگه رابطه مستقیم دارن ولی ایده کاربردشون فرق میکنه مثلا نمیخوایم یه دفعه با اعمال یک سایز فیلتر بزرگ receptive field مون رو بزرگ کنیم بلکه روالش اینکه که کم کم فیلترای کوچیک بزنیم تا receptive field مون بزرگ تر بشه البته اگه درست گفته باشم
-> منظورم اینه که اگه کرنل سایز ما سه در سه باشه receptive field مون هم حتما سه در سه هست
-> خیر
-> مگه کانولوشن ضرب نقطه به نقطه نیست پس باید سایز شون یکی باشه دیگه چرا ممکنه فرق کنه سایز هاشون
-> اره الان که فکر میکنم حرف تون درسته فک کنم
-> خودم هم شک دارم البته
-> منم همینطور
-> یعنی جواب این سوال بله است
-> بله
-> ممنون مهندس"
"-> we use the global average pooling to make predictions as well as 11 filters to compress the feature representation between 33 convolutions
-> این جمله رو هم اگر توضیح بدید ممنون میشم"
"-> سلام وقت بخیر floating point operations یعنی عملیات ریاضی با اعداد ممیز دار
-> عملیات اعشاری"
"-> سلام من یک دینا ست ساختم برای یگ پروزه segmentaion هر کدام از نواحی یک اسم دارد میخوام بعد از train هر کدام از این نواحی که detect میشه اسامی آنها را از فایل csv خوانده شودو کنار اون ناحیه نمایش داده شود ابا اینکار امکان دارد در ضمن با tensorflowkeras نوشتم ممنون
-> دوستان عزیز کسی ایده ای نداره
-> سلام فکر میکنم خواندن از csv شدنی نیست فرض کنید برای یک تصویر تست دو ناحیه سگمنت شده از کجا میدونید نام این دو سگمنت چی هست که از فایل CSV بخونید اگر فرضا شما 5 کلاس مختلف دارید بهتر هست که مساله رو به شکل semantic segmentation ببینید
-> یعنی چی خیلی با semantic segmentaion اشنا نیستم میتونم چند لجطه وقتتون بگیرم
-> سپاس"
"-> سلام دوستانمن یه سری پیش پردازش روی دیتای ماموگرافی پستان با تنسورفلو انجام دادم بعد با اتواینکودر فیچراشو استخراج کردم و به کلاستر دادم تا خوشه بندی کنه اما یه مشکلی اینجا دارم تنها به ازای سایز بچ سمپل تست میشه و دوباره به اندازه بچ سمپل جدید از دیتاست خونده نمیشه نمیدونم مشکل از کجاست اگه کسی بتونه کمکم کنه ممنون میشم
-> سلام اگر کدتون و بفرستین فکر میکنم بچه ها بهتر میتونن کمک تون کنن
-> دوست عزیز متاسفانه کدی برای این فسمت ننوشتم هنوز یعنی نمیدونم از چه تکنیکی استفاده کنم برای اینکار
-> آخه نمیدونم ایراد از کدوم قسمت کدارور ندارم"
"-> برای من هوسم نماد علم به روز و پاگرفتهی حتی بزرگتر از پنج ساله چه خوب که بزرگتر از سن خود پیش میری و چه خوب که همزمان با پیشرفتت دیگران را نیز با دنیای خودت آشنا می کنی من یکی از همون دیگرانم پنج سالگیت مبارک
-> سلام ممنون چه متن قشنگی انشالله بتونیم کیفیت کار رو بهتر کنیم"
"-> ممنون از شما استاد عزیز برای تشکیل این مجموعه بسیار جذاب که سبب رشد همه ی ما شده
-> سلام ممنون"
"-> سلام استاد روزتون بخیر و شادمانیتولد مجموعه هوسم رو به شما و همکارانگرامیتون تبریک عرض میکنمامیدوارم همیشه شاد و پرانرژی در اینمسیر گام بردارید و مثل همیشه بدرخشید
-> سلام ممنون"
"-> پنج سالگی هوسم howsamorg امروز هوسم پنج ساله شد در یک سال گذشته مصممتر و بیشتر از قبل روی این پروژه کار کردیم اتفاقهای خوبی رو تجربه کردیم و البته دوستان جدیدی به گروهمون اضافه شدند نشونش این هدایای زیبا از طرف خانم احمدی به مناسبت تولد هوسم خداروشکر البته هنوز ابتدای راه هستیم
-> تبریک میگم استاد انشالله شاهد پیشرفت مداوم تون باشیم
-> تبریک به شما و ممنون از شمابنده راه خود را توانستم با مطالب و بیان زیبای شما پیدا کنمانشاله بتوانیم در این راه مثل شما پیشرفت کنیم
-> سلام ممنون
-> سلام قطعا شنیدن اینکه این آموزشها برای شما و سایر دوستان تاثیر مثبت داشته بسیار خوشحالکننده هست ممنون
-> من با اموزش های شما الان سر کارم
-> عالی موفق باشید خدا رو شکر
-> سلامت باشید خیلی ممنون
-> 
-> 
-> سلام خدمت شما جناب اشرفی عزیز و همکاران گرامیتون بنده هم تبریک عرض میکنم و از زحمات شما و تیم هوسم بسیار متشکرم که واقعا آموزش ها و فعالیت های ارزشمند و مناسبی رو داشتید قطعا در ادامه شاهد درخشش بیشتر هم خواهیم بود
-> 
-> سلام ممنون
-> سلام خیلی تبریک می گم امیدوارم همیشه موفق باشین و برقرار
-> سلام ممنون آرزوی موفقیت برای شما
-> سلام استاد وقتتون بخیر خیلی تبریک میگم و واقعا خوشحالم دانشجوی شما هستم و از اموزش هاتون استفاده میکنم انشاالله هر روز پر قدرت تر پیشرفت کنید و همگی در کنار هم به موفقیت های بزرگ برسیم
-> سلام ممنون امیدوارم به اهدافتون برسید
-> سلام ممنون انجام وظیفه بوده آرزوی موفقیت براتون دارم
-> به امید موفقیتهای بیشتر و توسعه مداوم آرزوی بهترینها را برای شما دارم
-> سلام خیلی تبریک میگم آموزش هاتون واقعا عالی هست چیزهای زیادی یاد گرفتم به دوستان هم معرفی میکردم اونها هم واقعا راضی بودن خیلی خوشحالم که با شما آشنا شدم به امید پیشرفت های بیشتر و بیشتر
-> سلام ممنون موفق باشید
-> سلام ممنون برای مجموعه هوسم آشنایی با شما افتخار هست از اینکه در اندازهای بودیم که به سایر دوستان معرفی کنید بسیار خوشحالکننده هست موفق باشید"
"-> usage trainpy h pre PRETRAINED TRAIN TEST GPU TASK trainpy error the following arguments are required TRAIN TEST GPU TASK
-> ورودیهای argparse موقع اجرا کد رو وارد نکردید"
"-> ببینید من با تصاویر راداری کار میکنم مثلا در تصاویر راداری ماهواره سنتینل یک دو تا اصطلاح داریم یکی pixel spacing هست که یعنی هر پیکسل در تصویر معادل چند متر در زمین هست که برای سنتینل یک معادل ده متر در ده متر هست و یه اصطلاح دیگه به اسم resolution داریم که یعنی کوچکترین فاصله ای که دو تا شی در تصویر قابل تشخیص هستند که معادل 20 در 22 متر هست یعنی حدود هر بیست متر ماهواره یه اندازه گیری روی زمین انجام داده و سایز تصویر هم که مفهوم واضحی هست برابر تعداد پیکسل های عرض در ارتفاع
-> الان این مفهوم رزولوشن که تعریف کردم کاملا چیز متفاوتی با اون مفهوم رزولوشن ای هست که عکس شو فرستاده بودم"
"-> ما تو رشته سنجش از دور به این میگیم سایز تصویر رزولوشن یه چیز دیگه هست تو سنجش از دور
-> در پردازش تصویر رزولوشن یکی از اولین اصطلاحاتی هست که معرفی میشه"
"-> سلام تو این نوشته که در مورد یولو هست منظور از رزولوشن همون بعد هست درسته
-> ابعاد تصویر
-> به نظرتون اگه به جای رزولوشن مینوشت dimension بهتر نبود
-> رزولوشنی که در این جا مطرحه رزولوشن فضایی که درواقع ابعاد تصویر بر حسب پیکسل رو میگه
-> به نظرم بجای فضایی مکانی کلمه مناسبتری هست"
"-> سلام خدمت تیم خوب هوسم ایا اموزشی در بحث NLP هوسم برای ارایه داره
-> سلام هوسم آموزش در حوزه NLP نداره در دو فصل شبکه های بازگشتی از دوره یادگیری عمیق کمی از nlp صحبت شده"
"-> سلام کسی هست با نرم افزار fsl کامل آشنا باشه من چندتا سوال بپرسم ممنون
-> من با fsl کار کردم و با آنالیز fMRI آشنایی دارم
-> من دو سری تصویر ام ار ای دارم باز که میکنم و overlay نسبت به هم یکم بالا پایینن چطوری میشه یکی کنم مختصات قرارگیریشونو میدونین و البته مینویسه که orientationview تصاویر متفاوته اون پایین چطور باید یکی کرد"
"-> سلام میدونید چجوری توی گوگل کولب میتونم ورژن کراس و پایتون و تنسورفلو رو downgrade کنم
-> سلام معمولا با دستوری به شکل زیر میشه downgrade کرد pip install tensorflow113
-> ورژن خود پایتون رو میخوام بکنم 363 اما نمیشه ممکنه راهنمایی بفرمایید"
"-> استخراج بر مبنای پیدا کردن سخت ترین نمونه داخل دستهبچ یه چیزی در این حدود
-> سپاس"
"-> سلام دوستان کسی اموزش خوب برای شبکه عصبی با پایتون میشناسه معرفی کنه میخوام از پایه تا پیشرفته باشه خیلی ممنون میشم
-> اکادمی هوسم مجموعه اموزش هایی رو در سایتش قرار داده و من از چند موردش استفاده کردم و راضی ام و به شما هم توصیه میکنم استفاده کنید
-> میشه معرفی کنین اسم پکیج هاشو"
"-> من یه کتابخونه به اسم patchify رو هم تو cmd نصب کردم هم تو anaconda propmt منتها وقتی تو پایچارم میخوام اجرا کنم خطای ModuleNotFoundError No module named patchify رو میده
-> در صورتی که نصب کردم
-> سلام در پایچارم زمانی که میخاین پروژه جدید بسازین باید تنظیم کنین که Environment جدید بسازه یعنی برای پروژه کتابخانه ها بصورت مجزا باشن یا از کتابخانه های که قبلا روی سیستم نصب شده استفاده کنه که پیش فرض اون نصب کتابخانه بصورت مجزا برا پروژه هست
-> متوجه نشدم بالخره باید تو اناکوندارا نصب کنم یا cmd
-> ببینید در زمان تعریف پروژه دو حالت داره حالت اول اینکه باید از ترمینال خود پایچارم کتابخونه هارو براش نصب کنی و کاری به کتابخونه های پایتونی که نصب کردی نداره حالت دوم هم گزینه ای که قبلا روی cmd نصب کردی و روی کامپیوتر هست استفاده کنی ولی چون پیش فرض حالت اول هست هیچ کتابخونه ای رو نمیشناسه و برای شناختن تک تک باید روی ترمینال پایچارم نصب کنی
-> ترمینال پایچارم همون آناکوندا پرامپ هست
-> من راستش با آناکوندا کار نکردم و آشنایی ندارم تو خود پایچارم قسمت های پایین چنتا تب داره میتونی بببنی
-> آهان ترمینال منظورتون اونجا هست متوجه شدم ممنونم
-> روش دیگه هم داره برا نصب ولی کلا همون اول که داری پروژه رو تعریف میکنی گزینه هارو ببین خودت متوجه میشی و از کتابخونه های که قبلا نصب کردی استفاده کن متاسفانه اسم گزینش تو ذهنم نیست حالت پیش فرض برا هر پروژه باید کتابخونه نصب کنی و وقتتو میگیره
-> اون کتابخونه هایی که از ترمینال پایچارم نصب میکنیم فقط برای همون یه دونه پروژه است یا کل پروژه ها
-> تو حالتی که الان داری فقط برا همون پروژس"
"-> دوستان سلام من اين مقالع ميخوام دانلود كنم اما نميتونم كسي ميتونه كمكم كنه
-> لینک مقاله داخل این سایت وارد کنید شاید بشه دانلودش کرد
-> خدمت شما
-> واقعاااا ممنونم
-> فقط چه جوري دانلود كرديد من هر كار كردم در sci hub نشد
-> ممنونم ازتون
-> سلام خواهش می کنم از وب سایت ایران پیپر"
"-> سلام وقت بخیر ترنسفر لرنینگ یعنی شما از یه شبکه از پیش اموزش دیده شده استفاده کنی ولی فاین تیونینگ یعنی همون شبکه از پیش اموزش دیده شده یک سری از لایه هاش رو فریز کنی و یک سری رو بذاری اموزش ببینن
-> به زبون ساده چیزی که متوجه شدم و گفتم درست متوجه شدم
-> سلام ترنسفر لرنینگ مبحث جامعتری نسبت به فاینتیون هست روشی برای انتقال یادگیری از تسکهای قبلی به تسک جدید محسوب میشه حالا اینکه تسک مقصد و مبدا چقدر متفاوت باشند یا اینکه چقدر داده برچسب ار داشته باشیم رویکردها متفاوت میشه اما فاینتیون یک روش ساده و کارآمد از ترنسفر لرنینگ هست همونطور که از اسمش مشخص هست یعنی تنظیم دوباره و دقیقتر وزنها با توجه به تسک جدید حالا ممکن هست فقط برای یک لایه اینکار انجام بشه و بقیه لایهها فریز بشه یا اینکه فولفاینتیون باشه یعنی همه لایهها رو بذاریم دوباره ترین بشه تعداد لایههایی که باید ترین بشن تابعی از تسک و دیتا هست
-> اگه همه لایه ها رو بذاریم دوباره ترین بشن فول فاین تیون فرقش با ترین معمولی چی هست
-> نمیدونم منظور شما از ترین معمولی چی هست در فاینتیون هم شبکه رو ترین میکنیم اما در فاینتیون همیشه وزنهای اولیه وزنهای یک شبکه آموزشدیده روی یک تسک هست
-> مثلا تو ویدیو اموزشی faster rcnn in pytorch شما کل شبکه رو مگه ترین نکردین یعنی این کاری که شما کردین فول فاین تیونینگ حساب میشه
-> 
-> ممنونم استاد متوجه شدم"
"-> سلام آقای اشرفی یه سوال از دوره YOLO داشتم من یه YOLO v3 با دوتا کلاس اختصاصی رو با استفاده از روشی که تو پیج گیت هاب YOLO v4 بود آموزش دادم سوالم اینه با استفاده از اون کدی تو دوره هست و میتونه وزنای YOLO رو به وزنای قابل قبول برای کراس تبدیل کنه بهترین وزنی که بدست اوردم رو به اون converter بدم ازش استفاده کنم این کار شدنیه
-> سلام اگر شبکهای که آموزش دادید همون شبکه یولو 3 در دوره هست احتمالا converter بتونه وزنها رو تبدیل کنه اما اگر شبکه متفاوت هست نمیدونم چه اتفاقی میفته احتمال اینکه converter جواب نده زیاد هست"
"-> سلام خسته نباشید من دوره پایتورچ خریداری کردم ولی داخل سایت لینک گروه جدید رو نذاشتن امکانش هست برام اینجا بفرستید
-> سلام ممنون که اطلاع دادین اصلاحش میکنیم شما رو به گروه کلاس اضافه کردیم
-> سپاسگذارم
-> سلام من رو لطفا به گروه دوره بینایی کامپیوتر اضافه کنید"
"-> سپاس بیکاران بخاطر زحمت و دقت لازم در نحوه اموزش واقعا سپاسگزارم از این مجموعه عالی که با اشتیاق و پشتکار عالی برای ما دانش اموختگان زحمت می کشید
-> "
"-> سلام به همه جمعیتمون زیاد شده و لازم دیدیم که تغییراتی در ارائه پشتیبانی ایجاد کنیم این تغییرات برای ارائه پشتیبانی باکیفیت تر و بهتر هست تغییراتی اساسی در گروه های رفع اشکال هوسم ایجاد کردیم درسها و دوره ها رو جدا کردیم برای اینکه گروه ها خلوت تر بشه و سوال پرسیدن برای شما راحت تر چهارتا گروه جدید زدیم گروه پردازش تصویر گروه یادگیری ماشین گروه یادگیری عمیق گروه بینایی کامپیوتر از این به بعد هر دانشجو میتونه توی گروه مدنظر خودش سوالهای مربوط به دوره رو بپرسه مدرس هم در گروه حضور خواهد داشت برنامه هایی هم داریم مثلا گاهی voice chat مدرس رو بذاریم یا لایو در اینستاگرام و این گروه به گروه دانشآموختگان هوسم تغییر نام پیدا میکنه قدیمی های عزیز منت بر سر ما بذارن و در گروه فعال باشن ما هم سعی میکنیم جبران کنیم پشتیبانی از دوره ها برای هر فرد 6 ماه خواهد بود اگر شما در شش ماه اخیر از هوسم دوره ای خریدید لطفا به صفحه آموزش دوره در سایت برید و وارد گروه جدید بشید یا اینکه به پشتیبانی پیام بدید تا به گروه اضافه بشید
-> عالی مثل همیشه
-> سلام چه قدر کار خوبی کردید خدا خیرتون بده
-> سپاسگزارم بابت این پیشنهاد فقط لزفا بفرمایید منظور از این جمله چیه پشتیبانی از دوره ها برای هر فرد 6 ماه خواهد بود با تشکر
-> تا 6 ماه هرسوالی از دوره داشته باشید مدرس جواب میدن بعد 6 ماه پشتیبانی از دوره تموم میشه
-> یعنی دیگر توی گروه نمیتونه سوال بپرسه
-> میتونه بعد 6 ماه بیاد تو این گروه و سوال بپرسه دیگه پشتیبانی مستقیم هوسم تموم میشه میتونه اینجا سوال درباره دوره بپرسه پایان نامه کار و
-> در هر صورت قدردان کمک و راهنمایی شما هستیم
-> ممنون
-> ببخشید چطوری عضو گروههای جدید بشیم
-> سلام وقتتون به خیر من دوره پایتورچ حدود یک ماه پیش خریداری کردم لطفا من رو هم به گروه پایتورچ اضافه کنیدخیلی ممنون
-> سلام گروه پایتورچ همون گروه یادگیری عمیقه"
"-> سلام استاد من بنظر همه مراحل را رفتم برای نصب کودا و استفاده از gpu ولی هنوز هم طبق گفته شما موقع اجرا نمی تونه از gpu استفاده کنه ۱نصب کودا در پایتورچ در قسمت ترمینال ۲ دانلود فایل exe کودا و نصب آن ۳دانلود cudnn و انجام سه تا مرحله کپی کردن فایلها موقع دانلود کودا نسخه exe شامل local و network می باشدکه من local انتخاب کردم موقع نصب پایتورچ هم pip انتخاب کردمشاید اشکال همینجا باشه و باید conda نصب می کردم
-> Cuda Cudnn PyTorch gpu همین سه مرحله رو باید طی کنید البته حواستون به ورژنها باید باشه مثلا اصلا پایتورچ از کودای نسخه 113 ساپورت میکنه یا نه pip هم مشکلی نداره جز این موارد دیگه من راهنمایی بیشتری ندارم جواب نگرفتن در اینجور موارد هم بسیار رایجه دیگه باید با جستجو و مطالعه بیشتر مشکل رو پیدا کنید
-> ممنونمنسخه کودای انتخابی در نصب پایتورچ ۱۱۱ بودولی کودایی که دانلود کردم و cudnn ۱۱۳ بودندچطور بفهمیم پایتورچ از کودای ۱۱۳ پشتیبانی می کنه"
"-> سلام دوستان آیا برای نصب کودا و استفاده از gpu حتما باید ویژوال استودیو نصب کنیم
-> سلام خیر نیازی نیست"
"-> در پیاده سازی mlp در دوره یادگیری عمیق دو لایه فولی کانکتد در دوره مطرح شد منظور همین دولایه مطابق شکل بالاست که دو لایه مخفی هستند
-> در سادهترین حالت ممکن یک شبکه MLP دو لایه داره لایه ورودی و لایه خروجی این دو لایه ثابت هستند حالا اگر بگیم شبکه سه لایه یعنی یک لایه پنهان هم داره گاهی در گفتن این نکتهها رو رعایت نمیکنیم و مثلا ورودی رو جز لایهها حساب نمیکنیم اگر جواب سوالتون رو نگرفتید از کدهای مدل عکس بگیرید و بفرستید
-> 
-> لایه پنهان نداریم Fc1 لایه ورودی و fc2 هم لایه خروجی هست پس در اون شکلی که از mlp فرستادید لایه ورودی و خروجی وجود داره اما پنهان خیر
-> ممنونم"
"-> برای نمایش مدل بصورت پلات در مدل های mlp از چه کدی مبتونیم استفاده کنیم و همچنین تغییرات اپاک برای بهینه شدن
-> برای نمایش مدل از کتابخونه torch summary میتونید استفاده کنید همچنین استفاده از netron و tensor board هم خوب هست برای دیدن فرآیند آموزش دیدن اتلاف دقت و غیره از تنسور برد میتونید استفاده کنید سایت wandb هم بسیار عالی گزارش میده"
"-> استاد کدهای جلسه چهارم چقدر حجیم هستن ۱۷ گیگ به سختی دانلود می شن
-> سلام کدها همون نوتبوک هست بقیه دیتاست widerface هست بهنظر نیازی نیست دانلود کنید میتونید به گوگل درایوتون ادد کنید یا اینکه لینک مستقیم بسازید و بعد توی نوتبوک اول دانلود و بعد استفاده کنید
-> سلام به شما باشه چشم حتما ممنونم از توضیحات خوب شما فیلم های پردازش تصویر رو هم شروع کردم به دیدن واقعا خیلی از مفاهیم کلیدی و اساسی توی اون دوره هست به همه عزیزان پیشنهاد می کنم این فیلم ها رو ببینن تا دیپ لرننیگ و کامپیوتر ویژن واسشون بهتر جا بیفته امیدوارم تا قبل از شروع جلسه بعدی یک دور کامل مفاهیم تئوری پردازش تصویر رو مطالعه کرده باشم"
"-> سلام دوستان این طبیعی هست که در مدل mlp دقت نهایی داده های ارزیابی در مدل به صد برسه ولی دقت مدل در داده های تست ۹۶ درصد باشه
-> سلام اطلاعات کامل نیست اما با همین اطلاعات مشکلی نمیبینم این اندازه اختلاف غیرمنطقی نیست"
"-> سلام اقا کریمی وقت بخیر من تا فصل ۲ دیدم همه سوالاتم رو گفتم یکجا بپرسم ۱ چرا توی سوال خانه ها کالیفرنیا مقادیر خالی با میانه پر میشن میانگین بهتر نیست ۲ برای همه معیار های ارزیابی مثل RMSE MAE مفهوم ۶۸ درصد و دو برابر اون ۹۷ درصد صادق هست ۳ بهتر نیست پاکسازی داده قبل از مرحله نمایش داده ها باشد چون روی ظاهر نمودار تاثیرگذار است ۴ توی مبحث کتگوری توی کتاب اومده کتگوری های قسمت اموزش رو کد کدگذاری کرده در صورتی که اگه بعضی کتگوری ها توی داده اموزش نباشن وقتی می خوایم داده های تست رو با توجه به پایپ لاینی که کتگوری ها رو بر اساس داده اموزش کد می کنه اگه یه کتگوری توی تست باشه ولی توی اموزش نباشه ارور می گیره نباید کل داده رو برای کد گذاری کتگوری به پایپ لاین داد ۵ ترکیب ویژگی ها حتما باید توی دنیای واقعی هم معنادار باشه مثلا بیایم بدون توجه به مفهوم دو تا داده ای که بیشترین رابطه خطی رو دارن تقسیم کنیم و یک ویژگی جدید تولید کنیم یا باید معنادار باشه ۶ و اینکه یه سوالی مطرح کردید که معیار ارزیابی بگیم که خود عدد هم تاثیر گذار باشه معیارش RMSLE هست ۷ یه سوال دیگه هم داشتم برای lat و lang این ویژگی خوب هست چون تمرین داده بودید فاصله طول و عرض از مبدا رو در نظر بگیریم من حساب کردم همبستگی 02 داشت البته منظورم از مبدا توی نموداری که رسم شده ممنون ببخشید زیاد سوال پرسیدم
-> سلام من دونه دونه جواب بدم 1 اگر داده ها مقادیر پرت outlier داشته باشن میانگین شاخص مرکزی خوبی نیست و تحت تاثیر اون مقادیر پرت قرار می گیرن مثلا داده ها اینها باشن 1 1 12 12 13 14 27 میانه این داده ها 12 هست که توصیف دقیق تری از مرکز این داده هاست اما میانگین شون عدد بزرگی میشه فقط به خاطر وجود یک داده پرت
-> 2 اون بحث فقط در مورد RMSE صادق هست چون فقط RMSE میاد و انحراف معیار رو حساب می کنه بقیه معیار خطاها انحراف معیار رو حساب نمی کنن پی نوشت البته یک شرط دیگه هم باید وجود داشته باشه و اون اینه که خطا از یک توزیع نرمال یا گوسی پیروی کنه که در مورد رگرسیون خطی این فرض صدق می کنه
-> 3 بهترین حالت اینه که نمایش داده ها هم بعد و هم قبل از پاکسازی انجام بشه چون اولا نمایش به پاکسازی کمک می کنه ثانیا نمایش داده های پاکسازی شده به انتخاب مدل کمک می کنه
-> 5 بعضی وقت ها روش های یادگیری ماشین و روش های ریاضی یه چیزی به ما پیشنهاد می کنن که ما نمی تونیم براش توجیه پیدا کنیم اما نتیجه کار ما رو خوب می کنن اینکه ما بتونیم برای برخی چیزها توجیه پیدا کنیم خوبه اما حتما نباید اگر نتونستیم توجیه پیدا کنیم کنار بگذاریم شون باید اگر تو ارزیابی ها نتیجه رو بهبود دادن حفظ شون کنیم بنابراین معنادار بودن الزامی نیست
-> 4 تو تقسیم داده ها به مجموعه های آموزش و تست باید حتما مطمئن باشیم که توزیع داده ها تو این دو مجموعه شبیه به هم هست وگرنه مدلی که آموزش دادیم نمی تونه روی مجموعه تست تعمیم پذیری داشته باشه به خاطر اینکه توزیع داده ها هم شبیه به هم باشه از روش stratified sampling استفاده کرده اگر اشتباه نکنم اتفاقا این روش نمونه برداری رو روی همون متغیر categorical پیاده کرده تا مطمئن باشه نه تنها از همه category ها تو مجموعه آموزش و آزمایش وجود داره بلکه نسبت تعدادشون به کل هم تقریبا رعایت شده چون بهتره هیچ نمونه ای از تست در تعیین هیچ هایپرپارامتری نقش نداشته باشه به خاطر همین کدگذاری کتگوری بدون استفاده از داده های تست انجام شده
-> 6 آفرین من خودم معیار RMSLE رو نمیشناختم شما گفتید جستجو کردم دیدم بله جواب سوال من هم هست دقیقا تو این لینک گفته جایی که درصد خطا مهمه نه خود مقدار مطلق اش میشه از این معیار استفاده کرد
-> 7 من الان دقیقا نمی دونم چه تمرینی داده بودم اگه میشه بفرمایید دقیقه چند کدوم ویدیو بوده تا من برم دقیق ببینم و بتونم به سوال تون جواب بدم من خیلی خوشحال میشم وقتی می بینم دقیق می بینید و سوال هاتون رو می پرسید برای من مایه دلگرمی میشه
-> ممنونم خیلی لطف کردید سشن ۶ فصل ۲ دقیقه ۲۸۵۰
-> ببخشید یه سوال دیگه هم داشتم وقتی کتگوری ها رو کد می کنیم توی الگوریتم واقعا شرکت می کنن یعنی اگه یه کتگوری باعث همبستگی بشه مدل می تونه تشخیص بده یا فقط به این منظور کد گذاری می کنیم که الگوریتم ارور نگیره
-> و اینکه توی کتاب اومده پایپ لاین رو اینجوری تعریف کرده بخش عددی و کتگوری رو جدا کرده برای بخش عددی میانه و نرمال سازی رو روی داده های عددی اعمال کرده ولی برای بخش کتگوری ورودی فقط اسم ستون هست که متغیر cat_attribs هست چرا برای بخش کتگوری فقط داده های کتگوری رو نداده یعنی فقط اسم ستون رو داده ولی برای داده های عددی کل داده های عددیرو داده نمی دونم منظورم رو تونستم برسونم یعنی این پایپ لاین ورودی کتگوری نیاید کل کتگوری ها رو داشته باشه که بر اون اساس وقتی بهش داده می دیم کد کنه واینکه اگه بخوایم یک ردیف داده فقط بدیم به الگوریتم که پیش بینی انجام بشه فقط یک کتگوری رو توی اون ردیف استفاده کردیم در صورتی که کتگوری های مختلفی می تونه داشته باشه وقتی این پایپ لاین رو روی اون یک ردیف اعمال می کنیم ستون کتگوری توی داده رو چجوری کد می کنه چون به کل کتگوری ها دسترسی نداره
-> شاید سوالم رو اینجوری مطرح کنم بهتر باشه ما برای اموزش داده هایی که به پایپ لاین دادیم شامل کل کتگوری ها بود داده ها رو جوری جدا کردیم که start باشه مدل رو اموزش دادیم حالا من می خوام یک رکورد رو پیش بینی انجام بدم اون رکورد وقتی وارد پایپ لاین بشه چون فقط شامل یک کتگوری هست فقط بر اون اساس کد میشه دیگه کل کتگوری ها رو نداره حالا اگه کد شده ی کتگوری توی اموزش مدل تاثیر داشته باشه و فقط صرفا برای رفع مشکل عددی بودن نباشه ما توی این دو وضعیت یعنی اموزش و تست ۱ نمونه برای یک ردیف یکسان کد کتگوری های مختلفی داریم و به مشکل می خوریم ولی اگه کد کتگوری توی اموزش توسط مدل ها تاثیر داده نشه مشکلی نیست
-> "
"-> با سلام در خصوص تاوابع فعال ساز غیر خطی نوشته شده Stacking of network is possible این به چه معناستاین همون حالت saturate شدن موقع مشتق گیری هست و به اون ربط داره
-> سلام اینها رو کجا دیدید لطفا لینک بدید
-> "
"-> سلام من تو بخش کد faster rcnn که میخوام عکس رو به همراه باکس هاش نشون بدم تصویری که نشون میده سفیده چیکلر باید بکنم این رقع بشه خود عکس هم نشون نمیده بدون باکس
-> "
"-> S Ashrafi سلام اول نصب پایتون مثلا نسخه 37 دوم نصب کودا مثلا نصب cuda 102 میتونید از سایتهای داخلی این فایل رو دریافت کنید سوم نصب cudnn یک نسخه cudnn متناسب با نسخه کودا دانلود کنید مثلا آخرین نسخه cudnn متناسب با cuda 102 چهارم از صفحه اصلی سایت پایتورچ دکمه install رو انتخاب کنید و لینک مربوط به فایل نصب پایتورچ رو کپی و در cmd پیست کنید بله پکیج cudnn شامل چند فایل dll lib و h هست اینها رو باید در کودایی که نصب کردید کپی کنید بخش ویندوز لینک زیر گفته در چه مسیرهایی کپی کنید من نسخه متناسب با ویندوز 64 بیتی برای cudnn پیدا نکردمفقط از سایت پایتورچ نسخه کودای ۱۱۱ را مجدد قبلا پایتورچ روی cpu موجود بود نصب کردمدرقسمت ترمینال پایچارم الان وقتی تایپ می کنم Torchcudais_available نتیجه True هست ایا باز هم باید روی کارت گرافیک تغییری ایجاد بشه مطابق با لینک فوق باید پیش بروم
-> 
-> ممنون پیدا شد ولی نوشته بود windowsx86 وقتی کلیک کردم برای دانلود نسخه ۶۴ بیتی نوشته بود خیلی ممنون از راهنمایی تون"
"-> سلام و وقت بخیر من طبق آموزش نحوه نصب پایتورچ در وبلاگ هوسم نسخه cpu را قبلا نصب کرده ام الان نسخه cuda111 را دوباره نصب کنم مشکلی پیش نمیادداخل خود پایچارم و روی ترمینالش میشه نصبش کرد
-> سلام نسخه cpu و gpu باهم فرق دارن وقتی کودا نصب کردید بعدش باید حتما نسخه gpu پایتورچ رو نصب کنید"
"-> سلام وقت بخیر در فیلم پردازش تصویر بخش فرکانس فرمودید فاز اطلاعات مکان اشیا رو به ما می دهند من نتونستم این مورد رو در کتاب گونزالس ببینم آیا مرجع تون از همین کتاب بود یا مقاله ای در این زمینه وجود داره
-> سلام میتونید دقیقتر آدرس بدید تا بررسی کنم اسم ویدئو و زمان
-> بله براتون ارسال میکنم
-> Chapter 3 _ filtering in Frequency Domain تئوری حدودا یک ساعت و بیست و هشت دقیقه ۱۲۸ تا پنج دقیقه بعدش فرمودید فاز اطلاعات مهم تصویر مکان اشیا در خودش داره میخواستم بپرسم آیا این در کتاب گونزالس مطرح شده یا مقاله دیگه ای"
"-> سلام استاد میشه چند موضوع تو حوزه بینایی ماشین پیشنهاد بدین یا مقاله من برای تز ارشدم و موضوع سمینارم دنبال موضوع میگردم خیلی ممنون
-> 
-> ممنون استاد"
"-> سلام و خدا قوت اقای کریمی یه سوال داشتم فصل ۲ قسمت کد نویسی فکر کنم ناقص هست درسته چون برای من قسمت سوم که کد نویسی هست دو تا ویدءو هست و تا مبحث اماده سازی داده گفته شده
-> سلام امشب نگاه میکنم حتما لطفا منو منشن کنید چون ممکنه نبینم پیام تون رو الان اتفاقی دیدم
-> چشم تشکر
-> شما برای session7 باید دو تا فایل ویدیویی داشته باشید یکی حدود 2 ساعت و 22 دقیقه و دیگری حدود 49 دقیقه اگه این دو تا فایل رو نداشتید بفرمایید من هماهنگ بکنم براتون ارسال بشه ما تا همین جا رو کار کردیم چون موارد بعدش یا حل تمرین ها بوده یا یه سری کدهای مخصوص نسخه های قدیم کتاب البته اگه به حل تمرین ها علاقه داشتید خیلی خوبه که حداقل چند تاشون رو حل کنید و جواب تون رو با کدها چک کنید و اگه سوال داشتید بپرسید من در خدمت تون هستم
-> خیلی ممنون بله دو قسمت رو دارم"
"-> سلام جلسه چهارم دوره بینایی ماشین کی رو سایت قرار میگیره
-> سلام انشالله امروز یا نهایتا فردا مجبور شدیم بخشی از ویدئوها رو دو بار ادیت کنیم و متاسفانه طول کشید البته خوشبختانه تموم شده
-> بله ممنونم"
"-> سلام من دچار یک ابهام در طبقه بندی شدم وقتی که ما با شبکه های دیپ فیچر ها رو استخراج می کنیم و در نهایت قصد داریم اون ها رو به n کلاس دسته بندی کنیم من دو نگرش رو به این مساله شاهد بودم یکی این که ابتدا فیچرها استخراج شده و سپس بردار ویژگی وارد یک شبکه fully connected شده و سپس از یک لایه با یک تک نورون عبور داده می شوند و با محاسبه مقدارعبوری از activation function که یک خروجی احتمال بین صفر و یک است و تعیین مقدار آستانه توسط طراح الگوریتم دسته بندی می شوند رویکرد دوم بعد از استخراج ویژگی توسط شبکه های مثلا کانولوشنی بردار ویژگی وارد قسمت fully connected شده و به تعداد کلاس ها نورون در این شبکه وجود داره و مقدار آستانه ای توسط طراح تعیین نمی شه و سپس با اعمال داده های تست معیارهای ارزیابی مورد بررسی و تحلیل قرار می گیرن کدوم یکی از این دو راهکار درست هست و کدوم اشتباه اصلا شاید هر دو اشتباه باشن یا هر دو درست من به شخصه دچار تناقض فکری شدم
-> سلام رویکرد اول مربوط به دوکلاسه هست Binary classification برای دوکلاسه یک نورون و یک سیگموید یا تانژانت هایپربولیک میذاریم که خروجی 0 تا 1 تولید کنه اگر خروجی به 0 نزدیک باشه کلاس 0 و اگر خروجی نزدیک 1 باشه کلاس 1 هست اما باید با یک ترشولد تعیین کنیم که مرز این دو کلاس کجاست رویکرد دوم هم دستهبندی چندکلاسه هست بیشتر از دو کلاس هر نورون یک کلاس داره و هر نورونی که بزرگتر باشه ورودی به اون نورون کلاس تعلق داره برشما واجب هست دیدن دوباره تمامی ویدئوهای جلسه صفر و یک
-> خیلی ممنونم استاد بله حتما ویدیوها رو مرور می کنم"
"-> یه سوال داشتم چرا بعضی وقتها برای تبدیل به تنسور از دستور Torch tensor استفاده می کنیم و بعضی وقتها از دستور زیر Torchvisiontransformsfunctionalto_tensor آیا فرق خاصی دارند
-> سلام اگر یک آرایه نامپای یا لیست داشته باشیم میتونیم با دستوری مثل torchtensor اون رو تبدیل به یک تنسور کنیم اما دستور to_tensor یک کاربرد خاص داره تصویر خونده شده با pil رو تبدیل به تنسور میکنه به مسیر این دستور دقت کنید از کتابخونه torchvision هست
-> ممنون استاداتفاقا در ادامه ویدیو faster rcnn سوال یکی از دوستان بود که همونجا پاسخ داده بودید من تازه بهش رسیدم واقعا سوالات دوستان در کلاس بسیار کاربردی و مهم هستندممنونم"
"-> سلام من خروجی لایه های رزنت رو گرفتم و دادم به لایه fc همه رو در نهایت دقت بشدت اومد پایین دلیلش چی میتونه باشه چه راهکاری پیشنهاد میدین که درسش کنم ممنون
-> سلام لایههای پایینتر ویژگیهای سطح پایینی دارن بنابراین وقتی در دستهبندی مستقیم به لایه فولیکانکتد متصل بشن ممکن هست چندان اطلاعات جدیدی اضافه نکنن و طبیعتا افزایش دقت هم نداشته باشیم البته معتقدم همین کار هم اگر درست پیادهسازی بشه افت شدید ایجاد نمیکنه بلکه تقریبا برابر با همون دقت بدون شاخههای فرعی میشه من راهکار خاصی نمیتونم بدم چون اهداف شما و تسک شما رو نمیدونم اما بهصورت کلی پیشنهاد میکنم مقاله بخونید و از ایدههاشون الگو بگیرید ایدههای ابتکاری خوب هستن اما ممکن هست پشتوانه علمی ضعیفی داشته باشید همه پشتوانه ضعیفی داریم و نتیجه این بشه که روی یک ایده بدون خروجی و حتی اشتباه وقت بگذارید مطالعه مقاله درصد رخدادن این اشتباهها رو کم میکنه
-> ممنون کاملا حق با شماست باید مقالات دیگه هم بررسی کنم که نقص روش رو رفع کنم"
"-> پارمترهای f و map_location رو میشه بگین چی هستن
-> ffile nameوmap_location رو نمیدونم م"
"-> مگه میشه
-> load اشتباه تایپ کردین
-> اوه درسته"
"-> سلام من هر چقدر جستجو میکنم نمونه مثال یا کدی که دو تا لایه lstm داشته باشهلایه با تعداد نرون های متفاوت پیدا نمیکنم کسی میتونه تو گیتاب یا کولب یا هر جای دیگه ای لینک بده تشکر دوستان
-> استاد همچین مثالی سراغ ندارید در یک کلاس lstm staked ولی با سایز لایه های متفاوت نوشته شده باشه
-> سلام معمولا لایههای lstm رو پشت هم بدون تغییر تعداد نورونها میذارن به نظرم دنبال کار پرزحمتی هستید حتما میخوایید انجام بدید
-> بله اگر کد مشابهی دارید ممنون میشم معرفی کنید خیلی گشتم پیدا نکردم کلاسی که نوشتم رو کجا میتونم بفرستم ببینید
-> سلام اگر برای کراس بخوای من دارم خودم انجامش دادم
-> نه کراس مشکلی ندارم
-> 
-> اوکی تا نیم ساعت براتون پی وی میفرستم امیدوارم مفید باشه"
"-> سلام آقای اشرفی شما شبکه ResNet رو در دوره پایتورچ آموزش نمیدید من باید از این شبکه برای تزم استفاده کنم هنوز دوررو کامل ندیدم اگر توی دوره نیست امکان هست منبعی برای یادگیریش بهم معرفی کنید و کدی که resnet رو ایپلیمنت کرده باشن هم اگر توی گیتهابتون هست ممنون میشم لینکش رو بهم بدید
-> سلام ما درمورد شبکههای پریترین در دوره صحبت کردیم و پروژه هم انجام دادیم نیازی به پیادهسازی شبکه رزنت نیست چون معماری و وزنهای آموزشدیده این شبکه در پایتورچ موجود هست اگر در سطح حرفهای نیاز به سورس کدهای رزنت دارید و میخوایید تغییراتی در دل شبکه ایجاد کنید بازهم از سورسکدهای رزنت در گیتهاب پایتورچ بخش torchvision استفاده کنید
-> ممنونم از لطفتون ولی استاد من گفتن استراکچر نتوورک رو تغییر بدم و از رزنت استفاده کنم هنوز دوررو تموم نکردم اوایل دوره هستم یه تعدادی ویدئو دیدم یوتیوب ولی تعداد چنل و خیلی از پارامترهایی که برای ترین رزنت استفاده میکرد رو متوجه نمیشدم چی هستن و برای دیتای خودم باید اونارو چطوری تغییر بدم یه اینتروداکشنی که با پارامترهای مختلف رزنت آشنا بشم که بتونم بهتر تغییرشون بدم مثل in channel Out channel Number of channels Identity downsamples که بتونم برای دیتای خودم بزنمح
-> شما به جزئیات داخلی شبکه رزنت نیاز دارید ما در پایتورچ نگفتیم دوره آموزشی ندیدم که درباره چنین موردی توضیح داده باشه نهایتا در یوتیوب اطلاعاتی بتونید کسب کنید درهرصورت من پیشنهادم رو خدمت شما میگم اول تئوری رزنت رو کامل یاد بگیرید دوم سورسکدهای رزنت رو از پایتورچ بردارید و در قالب یک مساله دستهبندی قرار بدید سوم با تریس و دیباگ و مقایسه کدها با تئوری میتونید به تمامی جزئیات کد رزنت مسلط بشید اگر یوتیوب یا مواردی که خدمت شما گفتم کمکتون نکرد قاعدتا باید از کسی بخوایید که به کدها مسلط باشه و بهتون خط به خط کد رو توضیح بده هرچند این راه سریعتر هست اما توصیه نمیکنم
-> خیلی ممنونم از راهنماییتون آقای اشرفی لطف کردید"
"-> سلام ببخشید برای قسمت اول سوال این راه حل درست هست ولی جوابش 09 میشه در صورتی که در محاسبه 05 میشه
-> سلام مهندس کریمی رو منشن میکنم که جواب بدن
-> فکر می کنم اشکال در elif باشه اون هم باید if باشه چون ما هم تعداد اول دختر رو می خوایم هم تعداد هر دو دختر پی نوشت شما در حال حاضر دارید تعداد هر دو دختر رو درست حساب می کنید اما عدد دومی که حساب می کنید تعداد اولی دختر و دومی پسره این نسبت 09 که به دست اومده با تقریب خوبه چون باید یک به دست می اومد
-> "
"-> بازم نشد
-> کمی با همین دستور کار کنید و آدرس رو تغییر بدید مشکل حل میشه مثلا مسیر ذخیره رو در همون کانتنت بذارید و ببینید ذخیره میشه یا نه برای مدلتون پسوند pth بذارید
-> فقط قسمت اخر ادرس sec_saved_model رو گذاشتم و بقیش و پاک کردم درست شد پسوند هم اضافه نکردم بهش مهندس"
"-> سلام وقت به خیر ایراد کار کجاست
-> آدرس رو در نگذاشتید"
"-> سلام وقتتون بخیر دوستان ایا میشه کودای ورژن ۱۱ رو در ابونتوی ۱۸۰۴ با موفقیت نصب کرد یا بایستی ابونتو رو هم به ورژن ۲۰ اپدیت کرد
-> سلام بله میشه
-> برای اینکه کودای ده رو به یازده اپدیت کنیم چه مراحلی رو باید طی کرد که کل سیستم خراب نشه
-> اینجور گفته شده که پیشنهاد نمیشه اینکار باید نسخه قبلی را پاک کنید بعد نسخه جدید تر را نصب کنید
-> معمولا مشکل این هست که بعضی فیچر ها نیازمند برخی از ورژن ها خاصی هستن یک روش برطرف کردن این مشکل استفاده از داکر هست ولی خب داکر user interface نداره و مشکل ساز میشه و روش دوم اپدیت کردن سیستم هر چند ماه یک بار میشه کسی تجربه ای در این رابطه داره
-> خب اگر در این سطح کار می کنید میشه چند نسخه کنار هم داشت باید سرچ کنید اینکار جزئیات داره"
"-> سلام من از طریق سایت ایرانیکارت حساب پرو گوگل کولب رو خریداری کردم
-> ممنون چه خوب قابل توجه دوستان
-> سلام وقت شما بخیر پس چرا نوشته که امریکا و کانادا
-> شما مبلغ رو واریز میکنید اونها خرید رو انجام میدن
-> آدرس سایت رو محبت میکنید بفرستید
-> الان اوکیه دیگه براتون محدودیتی نداره واسه ای پی از ایران
-> به آی پی گیر نداده دیگه ازم نمیپرسه رباتم یا نه جی پی یو هم خوب میده رم هم ۲۵ گیگ در اختیارم میذاره رفیقم میگه من ۴ تا نوتبوک با هم ران گذاشتم ولی اون از ترسش با ویپیان وصل میشه
-> 
-> سلام مهندس وقت بخیر این مدت چطور بوده بخاطر آی پی اصلا اذیت نکرده
-> سلام نه فعلا مشکلی نداشتم
-> شکر خدا بعد برای خرید لازم کاری خاصی انجام بدیم
-> به ایرانیکارت باید جیمیل و پسوردت رو بدی ثبت نام کن و امپراتور سوال کن راهنمایی میکنند
-> خیلی ممنون"
"-> سلام آقای اشرفی بله من وارد صفحه پرداخت میشم می تونیم امتحان کنیم
-> خوبه پس شاید بتونید پرداخت کنید"
"-> اگر کسی تمایل داشته باشه که خرید کنه و امتحان کنه من می تونم پرداخت رو انجام بدم البته اگر پرداخت محدود به این کشورهایی که گوگل لیست کرده نباشه
-> سلام ممنون آقای زارع عزیز فکر میکنم فعلا کشورها محدود هست قبلا فقط آمریکا و کانادا بود البته شاید با کردیت کارت بشه"
"-> با سلام و وقت بخیر برای نصب پایتورچ رو gpu چه مراحلی رو باید انجام داد ممنون میشم راهنمایی بفرمایید
-> سلام اول نصب پایتون مثلا نسخه 37 دوم نصب کودا مثلا نصب cuda 102 میتونید از سایتهای داخلی این فایل رو دریافت کنید سوم نصب cudnn یک نسخه cudnn متناسب با نسخه کودا دانلود کنید مثلا آخرین نسخه cudnn متناسب با cuda 102 چهارم از صفحه اصلی سایت پایتورچ دکمه install رو انتخاب کنید و لینک مربوط به فایل نصب پایتورچ رو کپی و در cmd پیست کنید
-> ممنون بابت راهنماییتون cudnnحالت نصبی نداره فقط یه فایل زیپه که چندتا فایل داره
-> بله پکیج cudnn شامل چند فایل dll lib و h هست اینها رو باید در کودایی که نصب کردید کپی کنید بخش ویندوز لینک زیر گفته در چه مسیرهایی کپی کنید"
"-> با سلام برای رفع مشکل دسترسی محدود به GPU گوگل کولب باید چکار کنیم
-> سلام متاسفانه اخیرا سختگیریهای کولب زیاد شده فعلا یک راه چند داشتن اکانت هست اگر کسی در خارج از کشور میشناسید مثلا فرانسه آلمان کانادا و آمریکا میتونید بگید برای شما کولب پرو تهیه کنند ماهیانه 10 دلار
-> ممنون از پاسخگویی تون شما می دونید میزان محدودیتش چقدره
-> محدودیت کولب معمولی دوبرابر کولب پرو هست"
"-> با سلام آقای اشرفی من دوره پایتورچ رو خریدم ولی فصل اول که مربوط به تنسورها و دستورات جبرخطی هست دانلود نمیشه همه فصلهای دیگه دانلود میشه علاوهبراین در فصل صفر گفتید که google colab رو توضیح دادید ولی من توی ویدئوها پیداش نکردم ایمیل زدم گفتن توی گروه از خودتون سوال کنم ممنونم ازتون
-> سلام پیام دادم که در اسرع وقت مشکل شما رو حل کنند درمورد گوگل کولب در بخش وبلاگ سایت یک پست کامل درمورد گوگل کولب داریم همچنین یک وبینار سه ساعته هم برگزار کردیم که الان لینک این ویدئو رو در گروه فوروارد میکنم حین مشاهده دوره هر سوالی داشتید در گروه مطرح کنید در خدمت شما هستم"
"-> سلام ببخشید در بخش اول یادگیری ماشین روشی در کنار موارد دیگه گفته شد خوب متوجه نشدم تلفظش این بود بیک وی سوپروایزد یا بیت وی سوپروایزد ممنون میشم دقیقش رو مطرح کنید
-> سلام Weakly Supervised یعنی برچسبهایی لیبلها که در دسترس هستند 100 درصد دقیق نیستند و خطا دارند
-> ممنون"
"-> سلام اسناد ایا مبحث segmentation با keras یا pytorch هم آموزش داده ایدمن توی سایت ندیدم
-> سلام خیر انشالله از هفته آینده به مدت دو هفته درمورد سگمنت تئوری و کدنویسی پایتورچ در دوره بینایی کامپیوتر صحبت میکنیم
-> سپاس امکانش هست فقط توی این دوره شرکت کنیم
-> با پشتیبانی صحبت کنید
-> ممنون"
"-> سلام روز بخیر من یک دیتاست دارم که ۸۰۰۰ تا تصویر positive داره به معنی که حداقل یکی از اشیایی که من میخوام رو داره و در این دیتاست حدود یک میلیون تصویر negative هست که به معنی اینکه هیچ کدوم از اشیایی که من دنبالشم تو این تصاویر نیست حالا سوال من اینه که برای ترین شبکه به چه تعداد از negative ها باید سمپل بردارم یا اصلا به این negative ها نیاز هست
-> سلام به این بستگی داره که تصاویر شما شامل چی هست اگر تصاویر مثبت نشون دهنده چند شی مشخص هستند به تصاویر منفی نیازی نیست کار به کلاسبندی چندکلاسه تبدیل میشه اما اگر تصاویر مثبت فقط یک شی هست یا یک شی در حالتهای مختلف مثلا انسان در ژستهای مختلف هست پس مساله یک کلاسبندی دوکلاسه هست به اندازه نمونههای مثبت نمونه منفی انتخاب کنید عدم تعادل در مثبت و منفی کار برای آموزش رو مشکل میکنه البته در کارهای دوکلاسه از تکنیک hard example mining هم برای ایجاد تعادل بین مثبت و منفی استفاده میشه
-> بله شامل چند شی خاص هست برای آموزش شبکه u net پس با همین تصاویر مثبت کافیه نیاز نیس منفی هم بدم
-> به نظرم نیازی نیست
-> مرسی"
"-> سلام من میخوام یه ماتریس 0و1 تصادفی درست کنم که تعداد یک هاش هم قابل کنترل باشه دقیقا همچین کاری رو با sparserandom پکیج scipy میشه کرد میخوام بدونم میشه اینکار رو مستقیم با pytorch بکنم اگه کسی میدونه لطفا راهنمایی کنه
-> سلام تعداد یکها به چه شکلی کنترل میشه لطفا یک نمونه مثال بزنید
-> سلام مثلا میخوام 35 درصد درایه ها 1 باشه و به صورت تصادفی توی سطر و ستونا پخش باشه
-> من نمیدونم دستور آمادهای در پایتورچ وجود داره یا نه ولی دو راه حل به ذهنم میرسه اول اگر scipy به شما یک نامپای میده آیا همون قابل تبدیل به تنسور هست torchtensorscipy_array دوم ماتریس رو reshape کنید که بردار بشه 35 درصد از طول بردار میشه تعداد یکها n حالا n عدد رندوم randperm انتخاب کنید بجای این n عدد در بردار 1 بذارید درنهایت بردار رو به همون ماتریس اولیه reshape کنید
-> راه دوم رو درست متوجه نمیشم من میخوام 35 درصد درایه هام 1 باشه و سطر و ستون یک ها تصادفی باشه"
"-> سلام استاد بسته اموزش پنجاه ساعته یادگیری عمیق پایتورچ شامل پیاده سازی برای lstm نیست برای lstm کد زده نشده
-> سلام در فصل پنجم و ششم هم در مورد دستوراتش توضیح داده شده هم یک نمونه کد اجرا و بررسی شده
-> ممنون دوباره نگاه میکنم من تازه پایتورچ شروع کردم توش خیلی به مشکل خوردم
-> سلام به نظرم از ابتدا ویدئوهای پایتورچ رو مشاهده بفرمایید و همزمان تمرین کنید تمرین خیلی مهم هست اگر هم سوال داشتید در گروه مطرح کنید
-> تشکر استاد"
"-> سلام آموزش object detection با deep learning با tensor flow کسی داره
-> از کی
-> استادش خیلی مهم نیست ولی ترجیحا فارسی باشه و پروژه محور باشه"
"-> y yreshape256512 RuntimeError shape 256 512 is invalid for input of size 40960 pythonBaseException من هیچ لایه ای را فریز نکردم فقط می خوام خود رزنت18 را با لیست کردن دوباره اجرا کنم یایز قبلی می گفت ۱۳۱۰۷۲ هست با ابعاد ۲۵۶۵۱۲۱۱ که باریسایز کردنش اصلاح شد و ایپاک ۱ را رد کرد ولی دوباره در همان خط اجرا در ایپاک ۲ هستم ولی سایز ۴۰۹۶۰ داره نمی دونم چرا
-> عدد 256 احتمالا بچ سایز هست بچ سایز رو عدد ثابت نگذارید این عدد رو از ورودی x بگیرید بعد اول x برابر با بچ سایز هست اگر حل نشد در خصوصی پیام بدید تا بیشتر بررسی کنیم
-> بله ۲۵۶ بچ سایز هستبررسی می کنمممنونو عذرخواهی از همه بزرگواران که پیامهای داخل گروه زیاد شدند"
"-> سلام لینک گروه رفع اشکال آموزش پایتورچ را حذف کرده اید که نمیشه وارد نمی شهدر انتهای سرفصل ها لینک قرار داده شده ولی وارد نمی شود
-> سلام همین گروه هست
-> ممنون در فصل پنجم یک تمرینی دادین که شبکه رزنت را با دستور لیست تیکه تیکه کردین بعد گفتید حالا با همین تیکه ها دوباره رزنت را پیاده سازی کنیدمن هر کاری میکنم ایراد میگیره و در قسمت ترین وارد نمیشه میشه راهنمایی کنید Base models resnet18pretrained True Layers listbasechildren l5 Layers5 lenLayers l6 Layers6 l7 Layers7 l8 Layers810class listNetnnModule def __init__self superlistNetself__init__ selfL5 nnSequentiall5 selfL6 nnSequentiall6 selfL7 nnSequentiall7 selfL8 nnSequentiall8 selfL8fc nnLinearin_features512 out_features10 biasFalse def forwardselfx y selfL5x y selfL6y y selfL7y y selfL8y y selfL9y returny من همین لیست را در قالب کلاس گذاشتم
-> این رو ببنید احتمالا کمکتون میکنه
-> ممنونبررسی کردم ایرادی که از من می گیره اینه که سایز خروجی از اوریج پولینگ که بررسی کردم ۲۵۶۵۱۲۱۱ هست با سایز fc که ۵۱۲۱۰ هست نمی خونهدرحالیکه نباید مشکلی داشته باشه
-> سلام باید فلت کنید تا ابعاد 1 از بین بره flatten reshape view squeeze با یکی از دستورات بالا این کار انجام میشه
-> استاد من همین کار را انجام دادم و جالب اینه که اینبار ایپاک ۱ را انجام میده ولی به ایپاک ۲ باز خطای ناهمسانی سایز می زنه
-> لطفا خطای جدید رو هم بفرستید احتمالا مشکلی از جای دیگهای هست"
"-> سلام وقت بخیر ببخشین فیلم های جلسه دوم بینایی ماشین کی قرار میگیره رو سایت
-> سلام امروز انشالله
-> خیلی ممنون"
"-> با عرض سلام و ادب و احترام استاد جلسه امروز بسیار آموزنده بود ولی الان یک سوالی برای من ایجاد شد در ابتدای جلسه درباره self supervised صحبت فرمودین و برداشت من این هست که این نوع از الگوریتم ها احتمالا نیازی به لیبل ندارن اما زمانی که به جای کانولوشن از ترنسفورمر استفاده می شه داده ها طبیعتا باید لیبل داشته باشن درسته و این که بین selfsupervised و ترنسفورمر ها چه ارتباطی وجود داره لیبل ها در ترنسفورمر چه وضعیتی پیدا می کنن شاید هم سوال من اشتباه باشه راستش ابهام در ذهنم ایجاد شد عذرخواهی می کنم اگر سوال نادرست می پرسم
-> سلام بین خودناظر و ترنسفورمر ارتباطی وجود نداره ترنسفورمر یک شبکه عصبی هست و خودناظر یک روش یادگیری در ابتدای جلسه گفتیم علاوه بر تحقیقات در شبکههای کانولوشنی و یادگیری نيمهنظارتی و خودناظر این روزها در شاخه دیگری هم تحقیقاتی انجام میشه که ترنسفورمرها هستند طبیعتا ترنسفورمرها رو هم مثل کانولوشنی میتونیم به روشهای مختلف یادگیری باناظر نيمهنظارتی و خودناظر آموزش بدیم"
"-> سلام از همه عزیزانی که برای مسابقه وقت و انرژی گذاشتند بسیار ممنونم بسیار انرژی گرفتم و خوشحال شدم مسابقه تا جمعه هفته بعد ادامه داره حالا میتونید از تکنیکهای گفته شده در جلسه 1 و 2 هم استفاده کنید شبکه گفته شده در تسک رو تغییر ندید اما در فرآیند آموزش وزنهای پریترین و اضافه کردن ماژولهایی به شبکه اصلی آزاد هستید
-> آیا روی این دیتاست دقت خوب توسط دیگران با روشهای معمول گرفته شده است
-> بله با نکاتی که تا اینجا گفته شده میتونید به راحتی به دقت بالای 80 درصد برسید"
"-> سلام دوستان من به یک مشکل با گوگل کولب خوردم اونم اینکه یکبار که کانکت میشه و runtime گرفته میشه بعد دیس کانکت میشه و دیگه کانکت نمیشه اینو من با گوگل اکانت افراد دیگه چک کردم مشکلی نداشتن
-> سلام شاید زیاد ازش استفاده کردین با حساب جدید وارد بشید برای منم پیش اومده"
"-> سلام دوستان وقت بخیر دوستان این summary شبکه هست این منفی یک که اول همه پرانتزها گذاشته شده چیه ممنون
-> سلام بچسایز
-> این ینی اندازه بچ سایز منفی یک بوده
-> خیر یعنی سایز بچ مهم نیست چون این عدد هایپرپارامتر هست و طبیعتا متغیر سایز بچ هرعددی میتونه باشه بههمین خاطر بجای بچسایز 1 گذاشته
-> آهان جالب بود ممنون"
"-> دوستان گرامی درود من تو پای چارم اجرا میکنم و خطای زیر رو میگیرم کسی تجربه داره RuntimeError Input type torchcudaFloatTensor and weight type torchFloatTensor should be the same
-> مدل رو روی جیپییو نبردید اگر هم خطای حافظه میگیرید بچسایز رو کوچکتر کنید"
"-> با عرض سلام روزتون بخیر استاد فایل کولب آماده شد
-> سلام خیر"
"-> با سلام میخواستم بپرسم تو کاستم دیتاست برای متد getitem باید از چه آدرسی عکس teuren شود یعنی Index ما چیه دقیقا چون یک سری مثلا فولدر داخل train هست ۲۰۰ تا و داخل هر کدام از فولادها یه سری عکس داخل فولدر Images قرار دارد حالا مثلا عکسهای داخل فولدر اول یا دوم به ازای چه Index هایی باید لود شوند و به همراه Target مربوطه return شوند سوال دوم آیا len ما تعداد کلاسها است یا تعداد کل عکسهای موجود در فولدر مثلا train ویا val
-> سلام امروز یک کاستوم دیتاست مینویسم"
"-> سلام من تو سیستمم نمی تونم pyplot رو نصب کنم هرچند matplotlib را نصب کرده ام ولی pyplot رو نمی شناسه قبلا خطای ورژن pip می دادالان دو تا خطا می زنه Error could not find a version that satisfies the requirement pyplot Error No matching distribution found for pyplot ممنون میشم راهنمایی کنید
-> سلام احتمال داره متپلات درست نشده باشه چون pyplot نیازی به نصب نداره from matplotlibpyplot as plt
-> من هم از همین تعجب کردم چون دیدم در قسمت پکیج ها یک pyplot جدا نوشته فکر کردم شاید جدا باید نصب بشه الان حذف کردم و مجدد نصب کردم طبق گفته شما درست شد ممنون از راهنمایی تون"
"-> لایه آخر رزنت رو به صورت netfc nnLinear2048 200 نوشتم هر نود الان داره اسکور مربوط به 200 کلاس رو مشخص میکنه از طرفی من لیبلها رو اعداد بین 0 تا 199 تخصیص دادم من چون معمولا با تنسورفلو کار کردم لیبلها رو وان هات میکردم و لایه آخر مدل سافت مکس میگذاشتم اینجا لایه آخر سافتمکس نداریم و لیبلها هم وان هات نیستند من برای محاسبه لاس و هم بعد کردن خروجی مدل و لیبلها بایستی چه کاری انجام بدهم
-> ویدئوی بخش کدنویسی جلسه صفر رو مشاهده کردید جواب سوال شما در ویدئو و کدهای جلسه صفر هست
-> راستش مراحل رو درست پیش رفته بودم و بهم ارور دایمنشن میداد با کد شما چک کردم دیدم دایمنش لیبلهای من ۱۳۲ است و دلیل خطا ۱ بود سکوییز کردم حل شد"
"-> روشهای خوبی استفاده کردید Saeedtguserid403630037 Nasimtguserid106717485 روشهای متنوعی وجود داره با pil از همه سادهتر هست
-> وقتی عکس ها سیاه و سفید هستند میشه گفت که اطلاعات کمتری دارند
-> بله میشه گفت جلسه قبل فیلترهای لایه اول رزنت رو به تصویر کشیدیم به نظر میرسید بعضی فیلترها فقط شامل رنگ بودند درواقع فیلترهای حساس به رنگ بودند اگر رنگی نداشته باشیم احتمالا اون فیلترها کارآیی خودشون رو از دست میدن
-> "
"-> سلام مجموعه داده MNIST که به صورت سه فولدر جدا از هم validationtesttrain هست که در ویدیوهای فصل سوم یادگیری عمیق استفاده شده را از کجاباید دانلود کنم از این فایل چگونه باید تصویر استخراج بکنیم t10kimagesidx3ubyte
-> 
-> بله لطفا کدش را بفرستید جون خودم هم میخواهم تصاویر mnist را داشته باشم و از این فولدرها بتوانم خود تصویر را ببینم"
"-> در مورد دیتاستی که برای چالش کلاس بینایی ارائه کردید توضیح میدهید تکست فایلها بیانگر چه چیزی هستند اسم هر فولدر بیانگر اسم کلاس است
-> سلام پوشه train هر پوشه در پوشه train یک کلاس هست 200 پوشه معادل 200 کلاس نام هر پوشه معادل با نام اون کلاس توجه به فایلهای تکست در پوشههای train توجه نکنید مهم نیست پوشه val لیبل همه تصاویر val در فایل تکست موجود هست در فایل تکست جلوی هر تصویر نام کلاس نوشته شده بازهم سوال یا مشکلی بود در خدمت شما هستم لطفا همه در مسابقه شرکت کنید تا مفاهیم تدریس شده رو خوب یاد بگیرید رفته رفته جلسات مشکل میشه و از الان باید تمرين خوبی داشته باشید تا جلسات پیشرفته رو از دست ندید
-> ممنونم
-> تا چه روزی برای این چالش تایم داریم
-> تا پایان روز جمعه انشالله چهارشنبه با پنجشنبه کمی درباره مسابقه صحبت میکنیم البته الان هم اگر سوالی دارید بپرسید
-> ممنون
-> استاد سلام وقت تون به خیر می خواستم بپرسم درباره custom dataset و کد نویسی این مبحث در آموزش دیپ لرنینگ صحبت شده و اگر صحبت شده کدوم قسمت رو باید ببینم چون فایل یا ویدیوی مشخصی به این اسم ندیدم
-> 
-> استاد برای یک datasetدر هر کلاسلزوما image ها باید channelیکسان داشته باشند یعنی یا رنگی باشند یا سیاه و سفید یا الزامی ندارد
-> سلام لینکی که آقای Saeedtguserid403630037 فرستادند خوب هست
-> یعنی لازم است preparation روی اینگونه دیتاست ها انجام شود
-> همه تصاویر ورودی رو سه کاناله درنظر بگیرید حتی اگر سطح خاکستری هستند با سه بار تکرار پشت هم میتونید تصویر سه کاناله داشته باشید شبکهها اکثرا با تصاویر سه کاناله ترین شدند و اگر بخوایید ورودی تک کاناله بدید باید تغییر در کرنل کانولوشنی لایه اول ایجاد کنید که توصیه نمیشه
-> سلام استاد ممنونم از شما
-> سلام قربان خیلی لطف کردین ممنونم از محبت شما
-> 
-> PILImageopenimage_pathconvertRGB من با این دستور سه کاناله کردم
-> خواهش میکنم
-> tensor_img torchsqueezetorchstacktensor_img3 axis10 من هم با این دستور البته دستور طولانی تریه"
"-> سلام دوستان وقتی که از شبکه های بازگشتی استفاده میکنیم برای پردازش متن و همچنین از لایه ی embedding استفاده میکنیم وبا فرص اینکه zero padding روی جملات صورت گرفته ایا راهی هست که لایه ی embedding صفر ها رو در نظر نگیره
-> 
-> خیلی ممنون استادسوال دیگه اینکه اگر در صورتی که بخواهیم لایه ی embeddimgرو هم train کنیم در این صورت به zero padding نیاز داریم یا خیر
-> خیر نیازی به zeropad نیست"
"-> سلام وقت همگی بخیر یک سوال در مورد مفهوم time step در شبکه های بازگشتی دارم در step10 منظور این هست که شبکه بعد از هر ده ورودی که دریافت میکنه خروجی داشته باشه و برای پیش بینی خروجی n ام از داده صفر تا n_1 استفاده میکنه یا این که برای هر خروجی فقط از ده تا داده ی قبلی کمک میگیره و در هر timestep از اطلاعات step های قبلی استفاده نمیکنه
-> سلام خروجی در هر تایم استپ مثلا n تاثیر گرفته از تایم استپ های 0 تا n1 هست همچنین ورودی n هم در خروجی تاثیر میذاره به عنوان نمونه به دو خروجی Y1 و yn نگاه کنید فلشهای قرمز و سبز نشون میدن که هریک از این خروجیها از کدوم قسمتها تاثیر میگیرن
-> متشکرم
-> به نظر میرسید شبکههای بازگشتی در بحث پردازش زبان طبیعی بتونن مشکلات رو حل کنن مخصوصا مشکل Long distance dependency مثلا اگر فاصله بین اول تا آخر جمله زیاد بشه شبکههای بازگشتی تقریبا نمیتونن بفهمن که آخر اون sequence درست هست یا نه یا نمیتونن حدس بزنن کلمه محتمل چیه ما حدود ۳ تا ۴ سال با این شبکههای کار کردیم برای sequence های زیر ۸ کلمه بسیار عالی هستن اما طول sequence که بیشتر میشه بسیار ضعیف عمل میکنن و دقت الگوریتم بع شدت کم میشه برای حل این مشکل ترنسفرمرها پیشنهاد شد که انصافا Bert و gpt خیلی کمک کننده هستن مدلهایی که برای پردازش زبان فارسی استخراج کردیم واقعا عالی هستن و به نظرم انقلابی صورت گرفته ببخشید پرحرفی کردم یه نکته کوچیک بود که گفتم بگم خدمت دوستان
-> چقدر لطف کردین جناب رحمانی عزیز واقعا استفاده کردم ممنونم از شما
-> اطلاعات مفیدی بودممنون
-> خواهش میکنم چیز خاصی نگفتم
-> درود بر شما"
"-> سلام ویدئوها کدها سوال مسابقه در بخش فهرست مطالب در آدرس زیر قرار گرفت چند نکته سوالات درسی رو در همین گروه مطرح کنید مدت زمان مسابقه 1 هفته اگر مشکلی در ورود به اکانت و دریافت فایلها داشتید لطفا با پشتیبانی آنلاین سایت مطرح کنید تلاش میکنیم جلسههای بعدی رو زودتر در سایت قرار بدیم
-> "
"-> سلام فیلم های دوره کی قرار میگیره رو سایت
-> سلام در حال آپلود هست انشالله تا نیم ساعت دیگه ویدئو کد فایل ارائه سوال مسابقه در سایت قرار میگیره انجام شد در گروه پیام میدم
-> ممنون"
"-> سلام فیلم های دوره بینایی ماشین تقریبا کی قرار میگیره چون نمیتونم امشب شرکت کنم و میخوام تا هفته دیگه ببینم بتونم با کلاس بیام جلو
-> انشالله امروز ویدیوها کدها فایل ارائه و مسابقه در اختیار همه قرار میگیره
-> خیلی ممنون استاد"
"-> با سلام آیا نیاز هست که خودمان کلاس را ضبط کنیم یا بلافاصله بعد از برگزاری کلاس قابل دانلود خواهد بود
-> سلام جلسه کامل ضبط میشه و در سایت آپلود میشه و میتونین دانلود کنین"
"-> سلام سلام امروز ساعت 19 شروع دوره بینایی کامپیوتر
-> "
"-> سلام این قابلیت اسمش چیه پایچارم این قابلیت این که آرگومان های متدها را موقع نوشتن کد معرفی کنه و یه توضیح مختصری از کارکرد اون متد به ما بده را نداره مثل این تصویری که فرستادم
-> سلام داره هم autocomplete داره هم help معمولا بعد از نوشتن دستور و گذاشتن پرانتز با یک مکث کوتاهی این پنجره هلپ رو نشون میده ازطرفی اگر روی دستور مدنظر کلیک کنید با شورتکات ctrlq میتونید هلپ برای اون دستور رو مشاهده کنید
-> تشکر از شما"
"-> سلام دوستان من یه دیتاستmat دارم که اندازش به شکل ۱۲۸۰سطر و ۸ستون و بعد ۴۶۰ توی متلب به شکل 12808460 خونده میشه اما وقتی فایل رو توی پایتون میخونیم به دلیل تفاوت نحوه نمایش فایل ها بعد دیتاست ۱۲۸۰ در نظر گرفته میشه برای این که بتونم فایل هارو درست بخونم چیکار باید بگنم
-> دوستان لطفا اگر کسی اطلاع داره راهنمایی کنه
-> سلام در اینجور موارد میتونید فایل mat رو در متلب باز کنید و در فرمت مناسبی مثل CSV txt و غیره ذخیره کنید یا اینکه از دستورات لود mat در نامپای استفاده کنید و بخونید معمولا صرفا استفاده از دستور لود جواب نمیده و باید کمی بعد از لود شدن وقت بذارید و مشکل رو پیدا کنید سوال شما خاص پروژه خودتون هست یک راه حل عمومی نداره و ما هم دسترسی به فایل و پروژه نداریم و به همین خاطر نمیتونیم بیشتر از این کمک کنیم
-> ممنون استاد"
"-> سلام وقت همگی بخیر دوستان من تازه شروع به کار با پایتورچ کردم میخواستم خواهش کنم راهنمایی کنید تا این ارور رو برطرف کنم ممنون
-> احتمالا از dataloader باشه
-> بله قطعا از dataloader هست وقتی میخوایم دیتا های خودمون رو بدیم بهتره dataloader رو چطور تعریف کنیم
-> 
-> خیلی ممنونم استاد لطف کردید
-> استاد دیتا های من به این شکل هست که ورودی ها اندازشون 38412808 و لیبل هایش 3843 لیبل هارو خودم ساختم الان نمیدونم باید چطور به دیتا لودر این هارو بدم در کراس بصورت دستی اینارو شافل میکنم و به عنوان ورودی و لیبل به سیستم میدم بدون هیچ مشکلی
-> دوستا خواهش میکنم اگر کسی سمپل مشابهنوشتن کلاس برای همچین دیتاستی دارن لطفا ارسال کنند بسیار ممنونم"
"-> جلسه اول بینایی کامپیوتر 11 فروردین در اولین جلسه میخواهیم درباره موارد زیر صحبت کنیم بخش تئوری شبکه عصبی کانولوشنی معماریهای شناختهشده تکنیکهای ترین شبکههای کانولوشنی بخش کدنویسی پیادهسازی صفر تا صد یک پروژه دستهبندی انجام آزمایشهای مختلف روی شبکه برای بهبود دقت و تفسیر آن درصورتی که میخواهید پیشمطالعه داشته باشید مشاهده ویدئوهای دو فصل کانولوشنی فصل 3 و 4 دوره یادگیری عمیق همراه با تمرین کدنویسی پایتورچ پیشنهاد میشود
-> سلام ببخشید موضوعی که گفتید جلسه ۱۲ فروردین رو هم پوشش میده با توجه به اینکه دو جلسه پشت سر همن اگه موضوع هر دو رو میتونید لطفا بگید
-> سلام 11 فروردين بخش تئوری 12 فروردین بخش کدنویسی
-> "
"-> سلام جناب استاد اشرفی عزیز و دوستان گرامی سال نو مبارک
-> سلام سپاس سال نو مبارک"
"-> الان باید این لینکو کپیکنم
-> توی command line پیست کنید اجرا کنید
-> توی cmd پیست کردم اما erro داد"
"-> سلام وقتتون بخیر فکر میکنم شیوه نصب اشاره شده تو مجموعه هوسم طبق فرمت های قدیم سایت هستش من الان برای نصب پایتروج به مشکل خوردم
-> این ویسو گوش کنید شاید مشکلتون حل شد"
"-> سال نو رو خدمت دکتر اشرفی عزیز و تیم فوق العاده هوسم تبریک عرض میکنم
-> سلام ممنون سال نو بر شما و همه عزیزان حاضر در گروه مبارک امیدوارم سال خوبی برای همه باشه"
"-> سلام وقتی میخوام با image folder و data loader در pytorch تصویر را بخونم و لود کنم خطای variable not defined میگیرم کسی از دوستان میتونه کمک کنه
-> سلام لطفا یک اسکرین شات بفرستید طوری که هم کدتون مشخص باشه و هم خطا رو بتونیم ببینیم
-> حل شد"
"-> سلام خدمت دوستان عزیز دوستان کسی تابحال loss function روش های معروف ابجکت دیتکشن رو تغییر داده مثلا loss یولو رو کسی تغییر داده و اینکه چه جوابی گرفته ازش بخصوص رو دیتاهایی که bounding box های زیادی در هر تصویر داره و overlap هم بین ابجکت ها زیاده اگر کسی جوابی گرفته و یا اطلاعاتی تو این مورد داره ممنونم میشم راهنمایی کنه و تجربیات خودش رو بگه
-> تو خود مقاله یولو سه یه بخشی در مورد همین موضوع بحث کرده و حتی نتایج رو هم آورده"
"-> خیلی ممنونم از شما و پاسخ سریعتون
-> ممنون از شما برای اعتماد به مجموعه هوسم"
"-> عرض ادب و احترام من کلاس بینایی ماشین حرفه ای رو ثبت نام کردم و مبلغ رو واریز کردم تاییدیه نگرفتم فقط تو سایت درج شده که ۱ دوره ثبت نام کرده اید ولی مشخص نمیکند که کدام دوره در ضمن اطلاع رسانی جهت شروع کلاس و نحوه برگزاری کی و چگونه است با تشکر
-> سلام افرادی که با لینک موجود در گروه ثبتنام میکنند توسط پشتیبانی به دوره بینایی کامپیوتر اضافه میشن به دلیل فرآیند اقساطی و همچنین درنظر گرفتن تخفیف بیشتر نسبت به افراد خارج از این گروه این تصمیمات گرفته شده اسم شما ثبت شده و نگران نباشید شروع کلاس از یازدهم فروردین هست سایر موارد مهم درباره دوره در گروه و سایت اطلاعرسانی میشه"
"-> سایت برای خوندن مقالات جدید تو حوزه هوش مصنوعی معرفی کنید ممنون
-> سلام پیشنهاد میکنم هرروز سایت paperswithcodecom رو چک کنید
-> خیلییی ممنون و تشکر فراوان از دوره درجه یک یادگیریی عمیقتون خیلیی خوب آموزش دادید بازم ممنون و خدا قوت
-> خوشحالم که از آموزش راضی هستید خدا رو شکر"
"-> سلام جناب آقای اشرفی شما در ویدئو ها پردازش تصویر گفتین در اصل sampeling به دوربین بستگی داره و نمیشه با کد نویسی تغییر داد اما در بعضی منابع با درونیابی این کار رو انجام دادن آیا این غلطه
-> سلام بله sampling به دوربین مرتبط هست با درونیابی امکانپذیر هست که تغییر بدیم اما میتونیم اسم این کار رو sampling نرمافزاری بگذاریم در دوربینهای دستگاههای مختلف مثل لپتاپ و غیره این کار sampling نرمافزاری بسیار رایج هست اما طبق توضیحات در درس این کار در بخش سختافزار و تبدیل سیگنال به روشنایی انجام میشه
-> ممنون از پاسخگویی تون بله پس کوانتیزه کردن هم نرم افزاری هست چون تعداد رنگ های دوربین مربوط به سخت افزار دوربین میشه"
"-> سلام و وقت بخیر من از دوره های سایت دوره پایتورچ را تهیه کردم نسبت به محتوای آموزشی دوره دیپ لرنینگ آیا نیاز هست که این دوره هم تهیه بشه یا مطالب همین دوره پایتورچ کفایت می کند و اون را هم پوشش میده
-> سلام اگر به تئوری مباحث دیپ لرنینگ مسلط هستید دوره پایتورچ کافی هست اما اگر تئوری و کدنویسی رو باهم میخوایید دوره دیپ لرنینگ مناسب هست اگر پایتورچ خریدید اما دیپ لرنینگ رو میخوایید با پشتیبانی صحبت کنید تا برای شما عوض کنند
-> من ایده ای نسبت به دیپ ندارم ولی سوالی که دارم اینه که اساسا برای کار با دیپ و پایتورچ نیازی به دونستن اون مباحث تئوری و زمینه ای هست چون توو کار با ماشین لرنینگ که خیلی به اون مباحث تئوری نیاز پیدا نمی شد
-> از نظر بنده تئوری حتما نیاز هست حتی در یادگیری ماشین هم نیاز هست نمیدونم در یادگیری ماشین چه کارهایی انجام دادید اما صرف استفاده از فریمورکهایی مثل سایکیت کافی نیست درهرصورت پیشنهاد میکنم آموزش پایتورچ رو شروع کنید تا ببینید چقدر به تئوری نیاز پیدا میکنید اگر متوجه شدید و مشکلی نداشتید پس با همین آموزش ادامه بدید
-> ممنون از توضیحاتتون ولی با توجه به توضیحات شما مبنی بر نیاز به تئوریات میشه گفت که بهتره اگر میخام یکبار وقت بزارم یکدفه دیگه واسه همون دوره دیپ وقت بزارم تا اینکه شاید بخام برگردم و اونیکی رو مطالعه کنم
-> بله حرف شما منطقی هست و اتلاف وقت ممکن هست پیش بیاد شخصا فکر میکنم با دوره دیپ شروع کنید منطقیتر هست از دانستن تئوری ضرر نمیکنید
-> حتما همینطوره که شما میگید ممنون از وقتی که گذاشتید"
"-> سلام حضور دوستان گرامی و جناب استاد اشرفی
-> سلام خوش آمدید"
"-> سلام دوستان وقت بخیر این ۳ تا چه فرقی با هم دارن Image enhancment Image restoration Image reconstruction
-> سلام ببین در حدی که من میدونم بخوام بگم اولی یه چیزی مثل برطرف کردن نویز بالا بردن کیفیت و کنتراست تصویر مثه روش متعادلسازی هیستوگرام سومی کاربردش تو مواردی مثه کیس های تاریخی که تصویر خیلی قسمتهاش مشخص نیست و شما باید ازون تصویر مرجع که خیلی داغونه اصل تصویر رو با کیفیت بالاتر دربیاری یا مثلا در مواردی که نویز خیلی خراب کرده تصویر رو شما باید تصویر با کیفیت تر رو درست کنی از روی این تصویر خرابدومی رو من کار نکردم اطلاعی ندارم
-> ممنونم از شما
-> بقیه دوستان نظری ندارند
-> 
-> خیلی ممنونم
-> سلام من اطلاعات کمی در این مورد دارم امیدوارم که به دردتون بخوره توی image enhancement تصویر نسبتا خوب هست و ما میخوایم که یه کمی بهترش کنیم در image restoration تصویر توسط یک عاملی تخریب شده و هدف ما این هست که تصویر رو برگردونیم برخلاف enhancement در restoration برای بهبود تصویر نیاز به مدل ریاضی تخریب هست اما image reconstruction با این دوتا متفاوت هست توی reconstruction هدف ساختن تصویر از دادههایی هست که منشاشون تصویر نیست مثل تصاویر اکو که از بازتاب سیگنالهای صوتی یک تصویر ساخته میشه کتاب گنزالس فصل 5 درموردش کامل توضیح داده و پی دی اف زیر هم خلاصه یه چیزایی در موردش گفته لینک
-> خیلی ممنونم از شما"
"-> من پکیج آموزش یادگیری عمیق پایتورچ جناب مهندس اشرفی را خریداری کردم 2 نکته اما این سری دقیقا 2 نکته 1 چه تفاوتهایی بین این دو دوره انتظار میره 2 اینکه امیدوارم با توجه به خرید قبلی پکیج آموزش یادگیری عمیق ما به ازای 50 مبلغش که به عنوان هدیه در نظر گرفتین رو روی سایر پکیج هاتون برام اعمال کنید
-> 1 فهرست مطالب دوره رو بخونین تفاوت با دیپ لرنینگ فکر کنم زیاده 2 دیگه یه هدیه ای برای پیش نیازا گذاشتیم دیگه اگه مورد دیگه ای از سایت میخوایین ایشالا تو تخفیف عید نوروز تهیه کنین تخفیفش خوبه"
"-> pltplotbin_edges01 255 histogram دوستان تو این خط کد اون قسمت اول که نوشته bin01 رو میشه توضیح بدین متوجه نمیشم چرا منفی یک گذاشت ممنون
-> یعنی از اندیس صفر تا یکی ماقبل آخر منفی یعنی از آخر به اول بیا پس 1 یعنی یکی باقبل اخر 2 یعنی دوتا ماقبل اخر مثلا اگه لیست شامل ۵ عنصر هست میشه عنصر اول تا چهارم و آخری شاملش نمیشه
-> سعی کن که عادت کنی جواب اینا رو اول تو اینترنت پیدا کنی"
"-> سلام دوستان کسی ویدیو آموزشی و یا کورسی در مورد شبکههای GAN برای image to image translation میشناسه به من معرفی کنه یا ویدیوای باشه که پروژه ای در این زمینه پیاده سازی میشه خیلی ممنون میشم اگر راهنمایی کنین
-> سلام میتونید از این سایت کمک بگیرید image to image translation Towards Data Science
-> خیلی ممنون"
"-> لایه lstm دومتون هم همینطور
-> یه دنیا ممنون الان خطام سر load_weights هست
-> خواهش میکنم"
"-> سلام من از این خطا سر درنمیارم کسی هست توضیح بده مشکل چیه
-> واسه لایه اول lstm باید return_sequence True بذارید تا خروجی لایه هم سیکوئنس باشه برای لایه بعدی"
"-> Emailing 1s20S0006349516307482mainpdf
-> The collected timelapse videos were analyzed using MATLAB The MathWorks Natick MA code developed inhouse Briefly the tracking consisted of three steps First a background correction was performed to remove any bright pixels associated with particles that adhered to the bottom of the channel Second the position and size of all particles were calculated using a gradientbased method The calculated particle size was used to eliminate outoffocus particles based on their apparent larger size Third the positions of all remaining particles were tracked as a function of time to calculate displacements and velocities A more indepth explanation of the image analysis is given in Supporting Materials and Methods
-> این بخشی از لین مقاله هست که من باید اموزششو یاد بگیرم راهنمایی بفرمایید ممنون میشم
-> گویا با تولباکس خود متلب انجام داده"
"-> سلام خدمت ادمین گرامی یک سوالی داشتم در خصوص دوره پردازش تصویر با متلب در این دوره اموزش میدید که چطور میشه سایز اشیا و ذرات را محاسبه کرد با تشکر
-> سلام شاید من خوب متوجه سوال شما نشده باشم فکر میکنم شما نیاز به تشخیص اشیا دارید بعد هم میخوایید سایز اشیا رو به پیکسل حساب کنید اگر درست متوجه شدم پردازش تصویر چنین مبحثی رو پوشش نمیده شما به آموزشی مثل تشخیص اشیا Faster RCNN نیاز دارید
-> سلام ممنون از پاسخگوییتون استاد اشرفی گرامی
-> استاد اشرفی عزیز راهنمایی بفرمایید کدام اموزش سایتتون به کارم میاد در خصوص این موضوع ممنون میشم"
"-> دوستان معنی این خط کد چیه index npwheredistance pixel_distance00
-> من دقیقا نمی دونم دستور distance pixel_distance چه شرطی رو بررسی می کنه ولی در کل خروجی تابع where در اینجا یکسری عناصر هست که شرط بالا رو برآورده می کنن خروجی دستور بصورت یک تاپل هست که عنصر اول تاپل یک آرایه است که اندیس سطرها را دارد و عنصر دوم ان یک آرایه است که اندیس ستون ها را دارد در کل خروجی دستور فوق شماره سطر اولین عنصری از ماتریس را می دهد که در شرص داخل دستور صدق می کند
-> ممنون از توضیحات خوبتون"
"-> سلام و وقت بخیر به همه ی دوستان ضمن تبریک بایت روز مهندس می خواستم بدونم در بین دوستانی که در زمینه ی پیش بینی کار کرده اند کسی هست که با Support Vector Regression svr باشد ای می توان مسایلی که برای پیش بینی multi variate multistep ahead هست را با SVR پیش بینی کرد چون ما یک دیتاست داریم که باید برای ۷ ویژگی سه گام بعدی شان را پیش بینی کند و این تبدیل به سه بعد می شود مثال X_trainshape 120951 10 ۷ y_trainshape 120951 3 ۷ ولی svr این خطا را می دهد ValueError Found array with dim 3 Estimator expected 2 جوری که متوجه شدم svr نمیتونه سه بعدی را پردازش کند ایا کسی تجربه این مشکل را دارد
-> زحمت پاسخ این سوال را به همه دوستان علی الخصوص کسانیکه با استاد کریمی کلاس داشتند بکشند لطفا
-> سلام به نظرم انتظار سنگینی از svr دارید ورودی شما ده در هفت هست که ده نشان دهنده زمانه خروجی مورد انتظارتون سه در هفت هست که سه نشان دهنده زمانه به نظر من svr اصلا توانایی گرفتن ورودی سری زمانی رو نداره ضمن اینکه من در اینکه پیاده سازی سایکیت لرن بتونه چند خروجی آموزش ببینه هم شک دارم پیشنهاد کلی من استفاده از شبکه های عصبی بازگشتی مثل lstm هست
-> سلام سپاسگزارم خیلی عالی و واضح پاسخ دادید راستش کسی نمیتونست پاسخ بده تشکر بابت همراهی شما
-> خواهش میکنم"
"-> سلام جناب اشرفی وقتتون بخیر من میخوام به کمک اتوانکدر روی گراف ها Embedding انجام بدم اتوانکدرم از لایه های Linear تشکیل شده منتهی چون گرافم داینامیکه در هر لحظه ممکنه سایز ورودی های اتوانکدر متغیر بشه فرض کردم فقط نودها اضافه میشوند ممنون میشم راهنمایی بفرمائید چطور می تونم با ثابت بودن لایه ها و وزن های گراف لحظه ی قبل فقط به لایه ورودی نود اضافه کنم
-> سلام نمیدونم اطلاعاتی در این زمینه ندارم
-> سپاسگزارم"
"-> سلام من یه دیتاست دارم که میخوام با cnnاجراش بگیرم ولی لپ تاپم جواب نمیده حجمه داده ها بالاس میشه یه سایت که بطور انلاین بشه کد deep Learning نوشت معرفی کنید
-> گوگل کولب
-> ضمن تشکر از اینجا برای گوگل کولب آموزش داریم"
"-> خیلی خوب می شد اگر یک پکیج کامل داده کاوی و ماشین لرنینگ با اخذ مدرک در گروه هوسم راه اندازی می شد
-> سلام انشالله در سال جدید چهار دوره جامع برگزار میکنیم بینایی کامپیوتر پردازش تصویر یادگیری ماشین یادگیری عمیق چند ماهی هست که مطالعه و جمعآوری مطالب یادگیری ماشین رو شروع کردیم با همون رویکرد همیشگی تئوری همراه با کدنویسی مثالهای ساده و متعددی از کاگل جمعآوری کردیم بهعنوان مینیپروژهها 60 درصد مطالب رو هم آماده کردیم اما باید کیفیت مطالب رو ارتقا بدیم هنوز رضایتبخش نیست تخمین ما این هست که پاییز برگزار بشه فعلا برنامهای برای ارائه مدرکگواهی نداریم اما باید به این مساله هم فکر کنیم و مدرکی بدیم که معتبر باشه ممنون برای پیشنهاد"
"-> برنامه شروع کلاسها همان فروردین هست انشاله چون ۱۵ ام اسفند تازه امتحان دکتری هست و یکم استراحت بعد امتحان بنظرم لازم هست ممنون
-> سلام بله فروردین شروع کلاس هست"
"-> من پیشثبتنام کردم
-> خوش اومدید به کلاس دوستان واقعا ظرفیت تکمیله الان دو نفر صندلی ندارن باید ایستاده تو کلاس حاضر شن من لینک پیش ثبت نام رو down کردم مرسی
-> آقا عالی
-> سلام ببخشید دوستانی که پیش ثبت نام کردند چه زمانی باید ثبت نام اصلی رو انجام بدن
-> سلام ما تو این هفته پیج ثبت نام رو بالا میاریم بعدش دیگه میتونین ثبت نام اصلی رو انجام بدین"
"-> دوستان ظرفیت 1 نفر لطفا دیگه ثبت نام نفرمایید
-> فک کنم نفر اول من پیش ثبت نام کردم خدا رو شکر تکمیل شد"
"-> سلام وقت بخیر شبکه دینامیک هم داریم یعنی هی مجبور نباشیم که اگه ابعاد دادهخروجی و ورودی عوض بشه داخل شبکه رو عوض کنیم
-> سلام الان دیگه اکثر شبکه های کانولوشنی راحت با هر سایز ورودی کار میکنن آخرش یه adaptive pooling دارن که براشون مهم نیست سایز فیچرمپ چقدر باشه"
"-> سلام من نمیتونم ورژن جدید pytorch رو برای gpu نصب کنم از دوستان کسی اطلاعتی در این باره داره
-> سلام چه خطایی نشون میده نسخه کودای مناسب رو نصب دارید گاهی با بالا بردن ورژن پایتورچ عدم سازگاری نسخه کودا و پایتون ممکن هست پیش بیاد"
"-> سلام من یک پروژه object detection را با پایتورچ انجام دادم الان میخوام خروجی رو در یک نرم افزار ارایه بدم مثلا توی یک وب سایت یا توی یک نرم افزار موبایلی وکسی میتونه کمک و راهنمایی کنه بهم ممنونم
-> خروجی به چه صورته
-> سلام با flask و Django و دیگر فریمورک های مشابه میتونید خیلی راحت یک وب اپلیکیشن از مدل نهاییتون درست کنیدو با API به دست اومده در موبایل سایت ویندوز و سمت کاربر خودتون رو کدنویسی کنید
-> سلام سعید جان نحوه خروجیتون رو مشخص کنیدتا بهتر بشه نظر داد"
"-> من دیتامو تو گوگل درایو اپلود کردم بعد از اونجا خوندمشدیتا ست من دو قسمت اموزش و تست عهبعد هر کدوم دو قسمت مثبت و منفی داره که داخل هر کدوم ازونا هم 1000 تا داده متنی عه ولی الان که من داده ها رو از گوگل درایو گرفتم و طولشون رو اندازه میگیرم نوشته 33 تااینو چه جوری درست کنم من
-> سلام قسمت خوندن دیتاتون روچک کنید ممکنه نسبت به ابعادوتایپ دیتاتون اوردری که شما توی کد تعریف کردی با ابعاد دیتا نمیخونه و اینکه باید ببینید گوگل درایو دیتا رو به همون ترتیب که شما اپلود کردید حفظ کرده
-> ترتیب ندارهفقط مهم تعدادشه الان یک کار کاملا ابتدایی دارم انجام میدم فقط میخوام بخونم اون یه چیزی بهم بگه منتها یه چیزی که هست اینه که الان با این اوصاف که چند قادر تو در تو هست توقع میره این چی به ما بده کاش استاد یه چیزی مثه این حل میکردن اونی که استاد تو ویدئو درس دادن یه جور دیگه بود ممنون میشم راهنماییم کنید"
"-> سلام استاد اشرفی عرض ادب ابتدای دوره پایتورچ فرمودید که یه لینک از آموزش پایتون دارید که مرحله به مرحله با انجام تمرین میشه پیش رفت امکانش هست لینک رو قرار بدید
-> سلام در آپارات یک آموزش مختصر از پایتون قرار دادیم جامع نیست اما برای شروع کافی هست این ویدئوها رو ببینید و بعد کمی تمرین پایتونی حل کنید
-> تشکر"
"-> دوستان سوالی مطرح شده که تو این سوال دو مدل شبکه رو دادن حالا میگن با این تفاسیر دقت و زمان آموزش رو بگین چه جوری من دقت و زمان آموزش مدل رو بدون پیاده سازی پیش بینی کنم کسی هست کمکم کنه
-> شاید راه بهتری باشه ولی من یه دیتا رندم به شیپی ک گفته جنریت میکنم بعدش مدل رو پیاده سازی میکنم
-> به نظرتون پیاده سازی چقدر وقت میبره یک روز بیشتر وقت نیست که جواب بدم
-> دیتاست خودش دادهولی چه جوری میشه بدون پیاده سازی گفت دقت مدل چقدرهزمانشم تازه بگیم که چقدر طول میکشه
-> سلام غیرممکن هست سوال هم کاملا اشتباه هست
-> وقتی استاد میفرمایند پس کاملا غیرممکن هست
-> ممنون استاد
-> ممنون"
"-> اطلاع_رسانی ۶۰ گیگ اینترنت هدیه در انتظار دانشجویان وزیر ارتباطات به دلیل حجم زیاد شکایت دانشجویان برای رایگان نبودن استفاده اینترنت سایتهای آموزش مجازی با دستور رئیسجمهور بسته اینترنت ۶۰ گیگی به دانشجویان داده میشود دانشجویان و طلاب ۲ هفته وقت دارند برای دریافت این بسته در سامانه ictgiftsir ثبتنام کنند و ظرف ۲ هفته بسته برای آنها فعال میشود مرکز آموزش پایتون
-> تشکر بابت اطلاع رسانی
-> خواهش میکنم"
"-> آقا چجوری پیش ثبت نام کنیم
-> سلام"
"-> استاد اشرفی عزیز سلام می خواستم بپرسم که برای مطالعه مباحث تئوری بینایی ماشین کتاب ریچارد شلسکی بهتره یا کتاب دیوید فورسایت اگر کتاب بهتری خودتون میشناسین معرفی بفرمایین ممنون میشم
-> سلام ورژن دوم کتاب شلیسکی رو که نگاه میکردم خوب نبود اما بازهم هردو رو نگاه میکنم و در گروه پیام میدم
-> سلام استاد عزیز بابت اموزش های ارزشمندتون متشکرم نظرتون رو راجع به fast ai میفرمایید سپاس
-> سلام خواهش میکنم کتابخونه خوب و ارزشمندی هست اما درحال حاضر بهترین کار تسلط بر پایتورچ هست اکثر مقالات با پایتورچ پیادهسازی میشه و اگر با سایر فریمورکها کار کنید استفاده از پروژههای آماده پایتورچ رو از دست میدید همچنین درحال حاضر چندان نیازی نیست که فریمورک دیگهای رو در کنار پایتورچ یاد بگیرید اما برای پایتورچ کتابخونههایی کاربردی ساخته میشه که یاد گرفتن اونها ارزشمند هست مثلا apex از Nvidia
-> متشکرم
-> سلام استاد خیلی ممنونم از شما
-> 
-> سلام استاد خیلی ممنونم از توضیحات کامل و جامع شما"
"-> سلام دوستان کسی میتونه منبع و یا آموزشی به من معرفی کند که بتونم شبکه ای که مثلا با تنسور یا تورچ در محیط پایتون آموزش دیده رو در سی پلاس پلاس تست بگیرم خیلی ممنونم
-> من از آموزشهای پایتورچ استفاده کردم و تونستم این کار رو انجام بدم بخش C رو نگاه کنید"
"-> من کارشناسی ارشدم ریاضی محض هست و تا دلتون بخواد جبرخطی و بهینه سازی و خوندم ولی وقتی علم داده شروع کردم کار کنم تازه فهمیدم هر کدوم چه کاربردی دارن و مفهوم اصلی شون چیه بنظر من هم آدم با علم داده شروع کنه هر جاش رو لازم داشت بره بخونه یاد بگیره بهتره
-> خیلی ممنون"
"-> سلام شب تون بخیر ببخشید برای یاد گیری ماشین درس جبر خطی رو چقدر باید بلد باشیم درس بهینه سازی رو چطور منظورم اون ریاضیات و تئوری پشت الگوریتم هاست جبر خطی رو کامل فول باید باشیم به طور کلی قبل از اینکه از آموزش های دکتر اشرفی استفاده و شروع کنیم چه پیش نیاز هایی لازمه
-> سلام من معتقدم نیازی به گذراندن درسهایی مثل جبر خطی و بهینهسازی نیست اگر دانش اولیه در ماتریس و کار با اونها مستقگیری از توابع آمار و احتمال دارید میتونید درسهایی مانند یادگیری ماشین و یادگیری عمیق رو شروع کنید حین یادگیری هرجایی احساس کردید ریاضیات رو متوجه نمیشید از کتاب و اینترنت استفاده کنید و ریاضی اون بخش رو یاد بگیرید
-> خیلی ممنون دکتر اخه استاد ما میگه باید بهینه سازی رو حتما به عنوان بیس کار بلد باشین ایشون معتقده یادگیری ماشین ینی بهینه سازی برای همین میخواستم ببینم نظر شما و بقیه دوستان چیه که قبل از ورود به بحث های اصلی اینارو کامل مسلط باشم
-> صددرصد با آقای اشرفی موافقم اون فکر که اول برم جبر خطی و فلان و فلان رو مسلط شم بعد شروع کنم مثل باتلاقه ولی برعکسش درسته یعنی شروع کنین ماشین لرنینگ رو و هر جایی رو مباحث بیسیک به مشکل خوردین حتما پیگیری کنین اون قسمت رو و اون زمان سعی کنین مسلط شین تو این مبحث
-> خیلی ممنون از راهنمایی تون
-> کاملا درسته دقیق"
"-> سلام دوستان مشکل لاگین شدن اوبونتو رو چجوری باید رفع کرد
-> لطفا کل متن در یک پیام"
"-> سلام دوستان چطور میشه سرعت دتکشن رو بالا برد مثلا یولو پایتون بیس در جتسون خیلی کند هست من شنیدم از tensorrt یا onnx باید استفاده کرد با نظرتون کدوم بی دردسرتر هست اموزش مفیدی اگر سراغ دارید لطفا معرفی کنید چطور fps دوربین و سیستم رو تنظیم میکنید من زمانی که اجسام رو ترک میکنم گاهی ودیو فید شده سرعت fps خیلی بالاتر از سرعت پردازش دتکشن هست در نتیجه سیستم شمارش خیلی از فریم ها رو از دست میده چطور باید این دو رو synchronize کرد متشکرم
-> خود یولو حالتی داره که ورودی با ابعاد پایین تری هم میشه بهش داد با این کار سرعت افزایش پیدا میکنه نمودارش رو مثلا تو خود مقاله نسخه سه کشیده فقط دقت کمی ضعیف میشه
-> یه راه برای همزمان کردن اینه که گرفتن فریم و پردازش ها با هم سری باشن نه موازی اینطوری هر وقت پردازش یه فریم تموم شد بعدی رو میگیرد هر چند این کار باعث میشه ترکر به خاطر تنوع در نرخ فریم درست کار نکنه یه راه دیگه اینه که اگه تعداد فریم های گرفته شده از پردازش شده از یه آستانه بیشتر شد خود به خود چند تا فریم رو دور بریزه از ادامه شروع کنه
-> بله اون روش هم به سرعت مطلوب نرسید میخواستم ببینم کسی از دوستان tensorrtرو امتحان کرده به نظر میرسه به صرفه هست دقت هم خیلی زیاد پایین نمیاد ایا روش سرراستی هست برای پیاده سازی
-> سلام در مورد tensorrt و onnx اولی برای بهینه کردن از نظر سرعت مدل آموزش دیده شده هست یعنی شما با هر فریم ورکی که کار میکنید میتونید مدلتون رو به tensorrt تبدیل کنید مورد دوم برای معمولا بهینه ساز نیست یه اینرفیس واسطی هست که میتونید مدل هاتون رو از یک فریم ورک به فریم ورک دیگه exchange کنید در مورد fps دوربین دوربین ها یه ادرس دارن معمولا ۱۹۲۱۶۸۱۶۴ اینو تو web browser میزنید و وارد آدمین پنلش میشید و تغییرات مدنظر رو لحاظ میکنید در مورد سینک کردن دوربین و بخش پردازش هم اینکه میتونید fps دوربین رو به fps واحد پردازیشون نزدیک کنید
-> ممنون بابت توضیحتون به نظر شما بین این دو تا کدوم پایتون فرندلی هستن 1 Tensorrt tkDNN 2 Tensorrt Deepstream شما تا به حال ورژن opencvDNN یولو رو تو xavier ران کردید اگر بله چه سرعتی رسیدید ممکنه یه سری منبع خوب برای پیاده سازی tensorrt معرفی کنید
-> دومی یولو کلا رو جتسون کنده من ران نکردم منتها حتما بنچ مارکاش تو نت هست منابع تو اینترنت زیاده یه سرچ بزنید کلی مطالب میاره
-> شما در کارهاتون برای edge computing از چه نت ورک دتکشن استفاده میکنید که سرعت و دقت خوبی داشته باشه
-> معمولا مدل های ssd خوبن برای اینکار"
"-> با توجه به اینکه اگر دوره سه ماهه باشه اخر دوره با دوران امتحانات دانشجویان تداخل پیدا میکنه اگه به دو دوره یک و نیم ماهه تقسیم بشه که یکی از فروردین شروع بشه تا اواسط اردیبهشت و یکی مثلا اواخر تیر شروع شه بهتر نیست
-> سلام خیلی فاصله میفته و به نظرم این کار مثبتی نیست هم میتونیم در طول سه ماه یک یا دو هفته تعطیلی داشته باشیم هم اینکه ویدئوها آفلاین دردسترس هست اگر کسی به خاطر امتحان یک یا دو جلسه نبود میتونه از ویدئوها نگاه کنه رفع اشکال آنلاین هم میتونیم بذاریم"
"-> کورس حدودا چند ساعت میشه میشه از الان یه منبع معرفی کنین پیش خوانی کنیم استاد ما این سرفصل ها رو اوکی باشیم میشه نیم نگاهی هم به اپلای کردن انداخت
-> سلام پردازش تصویر و یادگیری عمیق پیشنیازها هستند همچنین توانایی کدنویسی در پایتورچ جز این سه مورد به مورد دیگهای نیاز نیست
-> بلد بودن این سرفصلها و گفتن اینکه کامپیوتر ویژن بلد هستید در رزومه میتونه امتیاز مثبتی باشه اما مهمتر از این برای اپلای کاردانشگاه سابقه کاری خوبمقاله هست"
"-> سلام زمان پیش ثبت نام را در این گروه اعلام می کنید
-> سلام بله"
"-> سلام آقای مهندس اشراقی هفته پیش فرمودین برای کورس بینایی کامپیوتر رای گیری میکنین ک برگزار بشه یا نه میخواستم بپرسم آیا کورس قطعا کنسل شد یا رای گیری برگزار میشه چون من یه تایم خالی دارم پیش رو از الان میخوام براش برنامه ریزی کنم و برام مهم هست کورس شما
-> سلام اقای اشرفی خوشحال شدم که دوره برگزار میکنید بفرمایید کاملا این دوره ریاضی هست پیش نیاز پردازش تصویر یا چیز دیگه میخاد
-> سلام وقت بخیر ممنون برای پیش ثبت نام به آی دی خودتون پیام بدیم
-> سلام ممنون دوره فقط تئوری نیست تقریبا بهصورت مساوی شامل تئوری و کدنویسی هست کدنویسی با پایتورچ هست پیشنیاز دوره پردازش تصویر و یادگیری عمیق تئوری و کدنویسی هست
-> سلام خیر قرار هست دوستان در سایت امکان پیشثبتنام رو فراهم کنند انشالله بهزودی آماده میکنند پیشثبتنام شامل کل هزینه نیست پرداخت یک هزینه جزئی صرفا برای اعلام آمادگی هست
-> خیلی ممنون بابت برگزاری دوره
-> خواهش میکنم انشالله اگر برگزار شد بتونیم دورهای در شان عزیزان حاضر در گروه برگزار کنیم
-> سلام و وقت بخیر ممنون از تصمیم بر برگزاری دوره بی صبرانه منتظریم
-> جسارتا بهتر نبود از پردازش تصویر شروع میکردین یا همون پردازش تصویر که در سایت هست کافیه جند روز پیش در voice فرمودید که قرار تمام این دوره ها برگزار بشه در سال جدید
-> ممنون لطف دارید
-> ممکنه یکیشون مثلا ریکاگنیشن رو بیشتر توضیح بدین منظور همون ریکگنیشن پردازش تصویر هست یا اینجا اون پیشفرضه و مثلا با دیپ اینجا پیش میریم
-> دوره یکساله کامپیوترویژن برنامه جدی ما هست و انشالله شروع میکنیم برای این دوره هم پردازش تصویر سایت کافی هست قرارشده افراد پیشثبتنامی بتونن پردازش تصویر رو رایگان و دیپ لرنینگ رو با 50 درصد تخفیف داشته باشن این شرایط فقط برای اعضای این گروه هست برای افراد خارج از گروه فعلا تصمیمی گرفته نشده
-> سرفصل Image Recognition شامل چهار جلسه حداقل 3 ساعته هست مجموعا 12 ساعت 6 ساعت برای تئوری 6 ساعت برای کدنویسی در 6 ساعت تئوری تمام جزئیات شبکه کانولوشنی همراه با معروفترین شبکهها تا 2021 توضیح داده میشن نحوه آموزش یک شبکه و ترفندهای لازم تشریح میشه در 6 ساعت کدنویسی کتابخونههای پایتورچی برای recognition معرفی میشن و نحوه استفاده ازشون توضیح داده میشه یک شبکه ساده مثلا موبایلنت از ابتدا پیادهسازی میشه یک پروژه تعریف میشه و از ابتدا تا انتها تمامی مراحل لازم برای image recognition پیادهسازی میشه از خواندن دیتاست تا محاسبه دقت و سایر معیارهای ارزیابی درپایان دستکاری یک مدل برای اینکه بتونیم بخشهایی از شبکه رو حذف کنیم و یا ماژولهایی بهش اضافه کنیم رو خواهیم دید نامنظم توضیح دادم اگر لازم هست در قالب یک وویس مطالب بالا رو واضحتر توضیح بدم
-> ممنونم
-> خواهش میکنم
-> سلام خیلی ممنونم من حتما پیش ثبت نام میکنم من خیلی براش برنامه ریزی کرده بودم مرسی که برگزار میکنین لطفا با همون متد_خودتون که توی ویس فرمودین برگزار کنین خیلی هم عالی بود
-> خواهش میکنم انشاالله به حد نصاب برسه با همان متد برگزار میکنیم
-> سلام برای فصل segmentation هم به همین صورت می فرمایید قرار چی کار انجام بشه ممنون
-> بالاخره انتظار به سر رسید حضور سر کلاس شما بیشتر از پیش ثبت نام نیاز به گزینش داره با افتخار ثبت نام میکنم
-> ببخشید در مبحث GAN بحث سوپر رزولوشن هم گفته میشه
-> سلام روال آموزش در هرفصل تقریبا شبیه به همان فصل image recognition هست در تئوری مرور مقالات از پایهها تا جدیدها در کدنویسی معرفی ابزارهای مهم و نحوه استفاده از اونها اجرای یک پروژه از 0 تا 100 آموزش دستکاری مدلها
-> خواهش میکنم شما همواره نسبت به بنده و هوسم لطف داشتید
-> هدفمون این هست که در GAN به سوپرزولوشن هم بپردازیم اما ممکن هست فرصت نشه"
"-> سلام آقای اشرفی بسیار ایده خوبی رو بیان کردید به نظرم اصل کار اینه که شما فرمودید پردازش تصویر و کامپیوتر ویژن و یادگیری عمیق رو ابتدا تئوری برگزار کنید هر دانشجویی هم برحسب اینکه چه بحثی رو لازم داره بیشتر وقت میزاره روی اون موضوع و بعدا اشکالات رو مطرح میکنه برای مثال دانشجویی که اصلا با پردازش تصویر آشنا نبوده و حالا در مرحله تعریف رساله استاد گفته از کاربرد شبکه های عمیق روی داده تخصصی استفاده کن واقعا نیاز داره پایه رو بلد بشه در زپینه شبکه های عمیق من که دانشجو هستم به علت اینکه وقت ندارم یا هر چیز دیگه میخام سریع برم سراغ اصل کار و این خودش کلی چالش برای منه دانشحو داره که گاهی میام اینجا سوال میکنم و شاید خیلی پرت باشه و شما به عنوان استاد میگی مشخصه اصول اولیه رو بلد نیس و کاملا هم درست میفرمایید پس همون روال خودتون رو پیشش ببرید و شک نکنید که واسه من دانشجویی که این تیوری ها رو بلد نیستم در اینده به شدت نیاز میشه موفق باشید
-> ممنون بابت توضیحات انشالله سال جدید این برنامه رو اجرا میکنیم
-> سلام آقای اشرفی اگر زحمت بکشین و نظرسنجی بزارین خیلی بهتره اینجا 139 نفر هستن و فقط چند نفر نظرشونو گفتن لزوما همه که الان دانشجو نیستن که متمرکز باشن به یه موضوع خاص من بعد از سرفصلی که گفتین بی صبرانه منتظر برگزاری دوره بودم
-> بله دقیقا
-> ضمن اینکه به نظر من کسانی که نظر دادن هدف شون این نبود که متد دوره عوض بشه بیشتر بحث این بود که سیلابس چیا باشه که در واقع خود شما هم خواسته بودین همه نظر بدن هر کی خواست شرکت می کنه و هر مبحثی رو که بیشتر به کارش میاد با تمرکز بیشتری یاد میگیره"
"-> ممنون از همه بازهم از همه عذرخواهی میکنم با اینکه معتقدم بهتر هست برگزار نشه اما پیشنهاد عزیز خوب بود یک نظرسنجی بگذاریم تا ببینیم چقدر علاقهمند به دورهای با این ساختار وجود داره بعد نظرسنجی بهتر میشه تصمیم گرفت
-> ممنون"
"-> توضیحاتی درباره دوره کامپیوترویژن
-> من بنظرم این روی کرد که با دیدن ویدیوهای مربوط به مبحث کامپیوتر ویژن تمام چالش های یک شخص برطرف بشه اشکال داره به هیچ وجه نمیشه تمام چالش ها را کاوریج کرد بنظرم دوره کامپیوتر ویژن را برگزار کنید و اگه کسی نتونست مشکلاتش رو برطرف کنه از امکان استاد مشاور بهره بگیره من به شخصه دنبال رفع کشلاتم در مرحله اول نیستم و بدنبال یادگیری این مباحث برای خودم هستم به عنوان کسی که حالا یه مدرک اکادمیک در این زمینه داره و این نیاز رو میبینم که مطالب رو باید بلد باشم حداقل در حدی که بتونم حرفی برا گفتن داشته باشم بعدش دنبال رفع مشکلاتم هستم حالا صرفا نظر منه
-> سلام و وقت بخیر داشتم گوش میدادم تا اونجایی که رسیدید گفتید دوره برگزار نمیشه یه آب سردی ریخته شد روم من و خیلی دیگه از اعضای این گروه از دوسال پیش تا الان تمام دوره های شما رو شرکت کردیم و به کیفیت دوره هاتون ایمان داریم خواهشا یکم در تصمیم گیری نهایی خودتون وقفه بندازید و حداقل قبلش یک نظر سنجی بزارید که آیا با برگزاری دوره به شیوه ای که خودتون از ابتدا مدنظر داشتید موافقیم یانه
-> سلام استاد اگر امکانش هست لطفا تجدید نظر کنید
-> سلام لطفا تجدید نظر کنید و طبق متد خودتون برگزار کنید من و خیلی از اعضای دیگه گروه همین الان هم سه کورس پردازش تصویر و یادگیری ماشین و یادگیری عمیق شما رو گذروندیم و با متد شما آشنا هستیم هر کی که کورس رو گذروند بر اساس نیازهای خودش بهره برداری ش رو می کنه من کلی تایمم رو خالی کردم و کارامو کنسل کردم که روی این کورس شما وقت بذارم شما کورس تون رو طبق متد خودتون لطفا برگزار کنین
-> "
"-> لیست مباحثی که تاکنون برای دوره یادگیری عمیق نهایی شده CNN Archtitecture Recognition Detection Segmentation Keypoints Detection Tracking Attention احتمال داره تعدادی سرفصل دیگه هم اضافه بشه مثلا موارد زیر Data Augmentation Action Recognition Texts in computer vision 3d computer vision هریک از مباحث بالا یک فصل هست در هر فصل به تشریح تعدادی از مقالات شناخته شده پرداخته میشه لطفا نظر بدید تا بتونم بهترش کنم
-> سلام میخواستم ازتون خواهش کنم که مواردی که فرمودین ممکنه اضافه بشه رو حتما اضافه کنین من پردازش تصاویر fMRI و PET و DTI کار می کنم و بشدت هر چهارتا رو توی کارم خیلی لازم دارم خیلی ممنونم
-> شبکه های GAN برای کاربردهای مختلف رادیولوژی برای ایجاد داده های جدید استفاده میشن که برای بهبود مراقبت های بالینی آموزش و تحقیقات استفاده میشن لطفا معماری انواع شبکه های GAN و پیاده سازی شون رو هم توی سیلابس تون قرار بدین
-> بله پیشنهاد خوبی هست ممنون
-> دینویزینگ مثل سوپر رزولوشن
-> بله ممنون اگر گن گفته بشه سوپرزولوشن هم میشه گفت
-> استاد کد های مقاله هم بررسی می شوند با action recognition از موضوعات احتمالی موافقم موضوعات semisupervide و نحوه کار با psuedolabel هم جالبه به نظرم و کاربردی
-> بخش کدنویسی هم داریم اما طبیعتا نمیشه کدهای همه مقالات رو بررسی کرد یا مثلا یک دیتکشن از ابتدا نوشت ولی قرار هست با پایتورچ کدهای آماده بعضی مقالهها رو اجرا کنیم کدهاش رو بررسی کنیم و به عبارتی تریس کنیم کتابخونههایی مثل mmdetection رو معرفی کنیم و نحوه استفاده ازشون رو آموزش بدیم دو موضوعی که معرفی کردید هم خوب هستند ممنون
-> مثلا متد مقاله را که بررسی میکنیم نحوه پیاده سازی متد اون که بررسی میکنیم
-> یک دنیا ممنون الان در حوزه پزشکی دیتاهایی که داره generate میکنه کم کم دارن تحلیل بیولوژیکی پیدا میکنه خصوصا برای تولید سیگنال های ECOG که برای تولید واقعی اونها باید مغز جراحی بشه و الکترودها در بافت های عمقی مغز کاشته بشه که روش تهاجمی هست و قطعا خیلی استفاده نمیشه روی انسان
-> من خودم به شخصه بیشتر چالش کد نویسی دارم و اینکه مثلا دوتا کد از دوتا مقاله را یک کد کنم و یک کار جدیدتر ارائه بدم به نظره من تئوری موضوع رو کسی که دوره اول یادگیری عمیق رو دیده باشه کافیه میتونه خودش در ادامه مقالات متعدد بخونه و تئوری کار رو یادبگیره خیلی از مقالات کدهاشونو تو گیتهاب میذارن ولی اینکه بشه اون کدهارو بهینه کرد یا روش بهتری نسبت به همون کد ارائه کرد چالش داره اگه بخشی از دوره به این صورت تدریس بشه برای افرادی که میخوان خروجی مقاله داشته باشن خیلی عالی میشه
-> لطفا کاربرد یادگیری_تقویتی در بینایی کامپیوتر رو هم پوشش بدین در موارد زیر خیلی کاربرد دارن filtering extracting image features localizing objects in scenes Action detection Object recognition
-> 
-> علاوه بر کار با تصاویر لطفا درباره کار با ویدیو که البته احتمالا در مبحث tracking هم هست صحبت بفرمایید ممنون می شم
-> ممنون سیلابس این دوره با الگوگیری از دورههای کامپیوتر ویژن دانشگاهها و موسسههای شاخص آماده شده من رویکرد شما رو در این دورهها ندیدم آنچه شما به دنبالش هستید در قالب یک دوره آموزشی نمیگنجه شما در کدنویسی چالش دارید بنابراین باید تمرین کنید و مینیپروژه انجام بدید شما در توسعه یا بهبود یک مقاله چالش دارید بنابراین باید تحت نظر یک سوپروایزر با دانش کار کنید تا شما رو هدایت کنه تا به اهدافتون برسید
-> ممنون هدف ما پوشش دادن مباحث مادر در کامپیوتر ویژن هست ترکینگ یکی از این زمینههاست بسیار مهم و البته کاربردی هست
-> دوره بینایی کامپیوتر احتمالا در دو فاز تدریس میشه احتمالا در این فاز اول از یادگیری تقویتی صحبت نکنیم
-> سلام و عرض ارادت خدمت استاد عزیز عالی میشه اگر امکان گنجاندن شبکه های کپسول هم باشه
-> عرض سلام و خسته نباشید لطفا شناسایی عمل رو هم توی این دوره بگنجونید و این که به نظر بنده اگه قرار باشه کد هم کار باشه بهتره که کد ها اماده باشه و از روی اونها توضیح بدین و این که لطفا یه مبلغ دانشجویی برای این دوره لحاظ کنید
-> با سلام و خسته نباشید سپاس از تلاشی که می کنید و باعث میشه شما را از سایر همکاران متمایز کند لطفاا تبدیل متن به عکس و بالعکس را بگذارید در لیست"
"-> Advanced Computer Vision with TensorFlow
-> سلام استاد اشرفی شب تون بخیر فرمودین برای کورس بینایی کامپیوتر تا هفته بعد سیلابسش رو اعلام میکنین برای تشکیل کلاس میخواستم ازتون خواهش کنم سیلابس این کورس کورسرا رو هم تووش بگنجونین اگه ممکنه خیلی ممنونم ازتون
-> سلام بسیار ممنونم حتما استفاده میکنم درحال آمادهسازی سیلابس هستیم قبل از نهایی شدن حتما در گروه به اشتراک میذارم که نظر دوستان حاضر در گروه رو بدونم فعلا فصلهای Recognition Detection و Segmentation نهایی شده سرفصلهای زیادی رو انتخاب کردیم اما نهایی نشدن شاید امروز در گروه بذارم تا ببینید و نظر بدید"
"-> سلام وقت بخیر استاد روی شبکههای ردیابی بر اساس شبکههای سیامی کار میکنم میخوام این شبکه رو با الگوریتم hog ترکیب کنم برای بالا بردن دقت ایا امکان این هست الگوریتم پردازشی hog با شبکه عمیق ترکیب بشه اگه ترکیب بشه دقت رو بهبود میبخشه چون الگوریتم hog روی کارهای ردیابی با بینایی ماشین جواب خوبی داده به نظر شما ترکیب hog که برای تشخیص لبه هست با ترکیب روی شبکه های عمیق مانند resnet دقت رو بهبود میبخشه و شدنی هست در واقع الگوریتم hog پیشپردازشی روی تصویر ورودی قبل از ورود به شبکه عمیق داشته باشه و بعد از تشخصیص لبههای تصویر شبکه آموزش ببینه اگر لبه های تصویر قبل از ورود به شبکه عمیق تشخیص داده بشه یعنی در واقع بخش ابتدایی شبکههای عمیق که مربوط به یادگیری اطلاعات سطح پایین هست قبل از قبل پیدا شده و این میتونه دقت رو بهبود ببخشه یا کلا ربطی نداره
-> سلام بهنظرم ترکیب الگوریتمهای کلاسیک با دیپ خیلی ایده خوبی نیست من چنین تقسیمبندی برای الگوریتمهای کلاسیک و دیپ درنظر گرفتم 1 اگر کار تجاری هست ایده جدید داشتن چندان لزومی نداره برای یک کار تجاری باید ملاحظات رو بسنجیم و بعد روشی رو طراحی کنیم که شاید اصلا جدید نباشه دیپ نباشه یک الگوریتم سبک باشه و به عنوان مثال پروژه شمارش کارتن که در کانال هوسم دمو شد اصلا بر پایه دیپ نبود 2 اگر کار ریسرچ هست پس تمرکز رو روی روشهای دیپ بذارید کلاسیکها رو فعلا کنار بگذارید در ده سال اخیر مهندسی ویژگی جای خودش رو به یادگیری ویژگی داده اما اگر به دنبال ایده هستید اتنشن ماژولها ایدههای جذابی هستند ساختار معلمدانشآموز هم ایده جالب و پرطرفداری هست ترکیب کانولوشن و ترنسفورمر هم خوب هست ترنسفورمرهای سبک هم داریم مثلا reformer حالا برید بررسی کنید از اینها میتونید در ردیابی استفاده کنید فکر میکنم از اتنشن در ردیابی استفاده شده و حتی ردیابی با ترنسفورمر هم مقاله وجود داره که بسیار ایده خوبی بود درآخر این نظر من هست که ترکیب کلاسیک و دیپ کار خوبی نیست ممکن است افرادی نظر مخالف داشته باشند
-> سلام استاد من یه مقاله خوندم و الان میخوام پیاده سازی کنم ابتدا اینجوریه که 2000 تا داده به صورت رندوم انتخاب میکنه و برای همه الگوریتم ها به accuracyدر 40 میرسه من چجوری باید 2000 تا داده مو انتخاب کنم کهaccمن بیشتر از40 نشه
-> بسیار ممنونم برسی کردم پیشنهادات عالیای بود تشکر
-> سلام نمیدونم متاسفانه"
"-> یک ماه
-> بدون محدوديت استفاده
-> ی نگاه بندازید
-> مرسي پوريا جان
-> خواهش میکنم موفق باشید
-> ارادت همچنين"
"-> راه حل اين داستان چيه بنظر شما واسه اينكه بشه تايم بيشتري از gpu استفاده كرد
-> البته اینم بگم این بلاک شدن دائمی نیست اوایل و بعد از چند روز درست میشه ولی اونم زیاد طول نمیکشه راه 1 من خودم چند اکانت گوگل داشتم و همچنین هر چند ایتریشن که شبکه آموزش میدید در گوگل درایو وزن ها و رو ذخیره میکردم راه 2 راه معقول تر اینه که گوگل کولب پرو تهیه کنید که من زیاد روش وقت نذاشتم چون گویا محدودیت های خرید از ایران هست و باید با افراد و یا جاهایی صحبت کنید که این خرید براتن انجام بدن 10 دلار
-> ١٠ دلار براي چ مدت استفاده"
"-> سلام دوستان عزیز میدونید google colab چند ساعت GPU به هر اکانت اختصاص میده
-> سلام ساعت مشخصی نداره ولی عموما اکانتتون اگه تازه شروع کرده باشه به استفاده از جی پی یو حدود 6 ساعت با اینکه نهایت داده هاتون تا 12 ساعت نگه میداره این 6 ساعت ممکنه بعد از 2 الی 3 ساعت استفاده ی ثانیه قطع کنه و شبکتون که توی train هست قطع بشه و در ادامه هرچی استفادتون بالاتر میره این ساعت و زمان interrupt دادنش کمتر میشه تا در نهاست به صورت کامل بلاکتون میکنه
-> منظورتون از بلاك كردن چيه اكانت اجازه استفاده از gpu رو نداره
-> دقیقا"
"-> با تعداد نرونها و هایپر پارامترها بازی کنید
-> این کارو نکردم
-> خیلی ممنون از توجه و دقتتون"
"-> هشتاد به بیست اوکی هست برای تست و ترین
-> بله این نتیجع مربوط به ۸۰ به بیسته"
"-> دیتا چقدر هست
-> ۱۵۰هزار خط"
"-> چرا ۵ تا ایپاک
-> تا ۲۰۰ ایپاک رفتم"
"-> خوب دقت ولیدیشن ثابت هست بخش ولیدیشن رو چجوری جدا کردید
-> هم از طریق time series generator جدا کردم هم train test split"
"-> یه چیز دیگه داده ای که دارید چی هست
-> داده ی تخلفات ه برای ایران نیست پیش بینی توع وسیله نقلیه است توی پنج دسته از جمله شخصی تاکسی ون و"
"-> نرخ یادگیری چنده
-> تغییر دادم از ۱ ده هزارم تا ۱ صدم"
"-> سلام دوستان کسی تجربه ای در این زمینه داره ImportError Failed to import pydot You must pip install pydot and install graphviz for pydotprint to work اینارو با pip install و به صورت فایلی در محیط ویندوز نصب کردم ولی خطا پا بر جاست path هم اضافه شده
-> سلام من درست خاطرم نیست ولی فکر میکنم که باید یه بار سیستم restart بشه"
"-> توضیحی بدم تشخیص گفتار رو به زودی به صورت افلاین ارائه خواهیم کرد بخشهای ویرایشی رو هم با شبکههای عصبی آماده کردیم گذاشتیم مدلهای خوبی استخراج شده هر روز هم بهتر میشه
-> ممنونم"
"-> دوستان ما در شیراز و در مجموعه ویراویراست یک سیستم تشخیص گفتار رو تهیه کردیم که حجم کمی داره دقت خوبی هم داره ویراویراست یه سامانه آنلاین ویرایش متن فارسی هست لطفا سر بزنید بهش اما در مورد تشخیص گفتار باید بگم که خیلی راحت امکان ران شدن سیستم روی رزبری رو داره
-> اگر یه لینکی بفرستید ممنون میشم
-> Viravirastcom"
"-> سلام اقای اشرفی شبتون بخیر شما فرمودید در مورد self attention در کدوم دوره مطالبی رو بیان کردید
-> سلام در آخرین جلسه یادگیری عمیق جلسه نهم
-> سلام خیلی ممنون من میخام چند لایه attentiin در شبکه هم قرار بدم اول شبکه میخام ببینم با گرفتن اطلاعات global نتیجه کارم بهتر میشه یا نه چیکار باید کنم با پایتورچ هم کار میکنم
-> متاسفانه نمیدونم
-> سلام استاد وقتتون بخیر بنده میخوام دیتاستی متشکل از ۱۵۰ تا گراف که به صورت ماتریس مجاورتی هس را به صورت ورودی به شبکه بدم ولی نمیدونم چطوری در نظر بگیرم و به عنوان ورودی بدم ممنون میشم راهنمایی کنید
-> سلام نمیدونم متاسفانه
-> کجا میتونم در موردش اطلاعات بدست بیارم
-> برای کارهای مرتبط با گراف اکثرا از پکیج networkx در پایتون استفاده میشه این لینک رو مطالعه کنید به دردتون میخوره
-> ممنون بله با networkx کار کردم قبلا ولی هدفم این ماتریس مجاورتی رو بدم به عنوان ورودی به شبکه
-> خب هدف تون چی هست شبکه چیکار کنه دسته بندی کنه
-> فعلا روی ابتدای کار مشکل دارم اونم دادن ورودی هس وگرنه مدل رو بلدم میخوام تشخیص ناهنجاری انجام بدم اگه ورودی حل بشه مدل رو میسازم
-> 
-> سلام بنظرم تلاش کن ورودیت رو به فرمت متداولی که تو شبکه ها استفاده میشه تبدیل کنی
-> بله اینم خوندم در کل میگن به صورت بردار بدیم به شبکه که زیاد درست به نظرم چون بین سطر ها و ستون های ماتریس ارتباط وجود داره ممنون بابت وقت ای که گذاشتین
-> بله همچین کاری باید بشه گفتم شاید دوستان تجربه اش رو داشته باشن ممنون"
"-> سلام وبینارgan چی شد
-> سلام متاسفانه فعلا نمیتونم برگزار کنم فرصت کافی برای این کار نداشتم فرصت کنم اولین وبینار همین gan خواهد بود اگر عزیزان منتظر بودند این مدت عذرخواهی میکنم"
"-> سلام آقای مهندس اشرفی یه بار که بحث کورس جدید شده بود فرمودین پردازش تصویر پیشرفته برگزار میکنین که در ادامه کورس قبلی هست میشه این کورس رو قبل عید شروع کنین که توی عید هم وقت بذاریم تمریناتش رو حل کنیم چون تعطیلات هست و تایم بیشتری داریم ضمن اینکه بشدت هم توی تحقیقات جاری من و مطمئنا بقیه بچه ها مورد نیاز هست همچنین اگه سرفصل ش رو هم بفرمایین برای پیش خوانی خیلی عالی میشه خیلی ممنونم ازتون
-> سلام بله قبل از عید شروع کنیم خوب هست انشالله نهایتا تا 2 هفته آینده فهرست مطالب و زمان پیشنهادی رو برای دوره کامپیوترویژن در گروه میذاریم
-> خیلی از لطف تون ممنونم
-> سلام بیصبرانه منتظر دوره های جدیدتون هستیم"
"-> خروجی برای ۶۴ سطح
-> نتایج اشتباه هست با تریس و دیباگ مشکلات رو پیدا کنید کلیات راه حل درست هست اما احتمالا نکاتی جزئی رو رعایت نکردید
-> خودمم حس کردم نتیجه درست نباشه باشه چشم ممنون از راهنمایی شما
-> سلام میشه هر موقع که تونستید مجددا بررسی کنید کدم رو درست نوشتم من ۳ سطح کوانتیزه در نظر گرفتم
-> سلام بهنظر میرسه به دو سطح کوانتیزه شده برای دو سطح نتیجه منطقی هست"
"-> سلام سوال من درخصوص تمرین اول پردازش تصویر هست اونجا که میگین کد رو طوری تغییر بدیم که بصورت پویا بتونیم تصویر به تعداد سطوح مختلف کوانتیزه کنیم من چیزایی که به ذهنم میرسید و سرچ کردم اما متاسفانه هنوز به نتیجه نرسیدم میشه بیشتر راهنمایی کنید تا به جواب برسم
-> سلام خوب نوشتید اما if مقایسه ناقص هست کوچکتر از مقدار q حاضر و بزرگتر از مقدار q قبلی
-> ممنوم از راهنماییتون لطفا مجددا کد رو بررسی کنید بینید درست نوشتم این خروجی برای ۳ سطح هستش و جالبه خیلی با ۳۲ سطح تفاوتی نداره اما در ۶۴ سطح کمی متفاوت هستش این کم بودن تفاوت درسته"
"-> سلام وقت بخیر برای تریس کد توی پایچارم میشه نتایج تریس رو تا اون لحظه ذخیره کرد و بار دیگه نتایج تریس قبلی رو لود کرد و تریس رو ادامه داد
-> 
-> بعد از خوندن این مطلب کار تموم نیست اما همین میتونه هدایت تون کنه به جواب
-> ممنونم"
"-> بهترین تعریفی که از متغیر تصادفی دیدم بخشی از فصل ۳ کتاب پردازش تصاویر با متلب کاربرد در پزشکی و بیولوژی نوشته اومر دمیرکایا
-> خیلی تعریف خوبی بود بله تابع هست و در کتاب دکتر بهبودیان هم همینجور خوب تعریف شده"
"-> ببخشید استاد اینجا آدرس پایتون رو ک میزنم اجازه انتخاب نمیده
-> سلام باید فایل Pythonexe رو انتخاب کنید"
"-> سلام استاد خسته نباشید من بر اساس ویدئوها pytorch و pycharm رو نصب کردم و torch رو هم install کردم اما وقتی import میکنم این ارورو رو میگیرم دوبار پاک کردم و نصب کردم باز ارور هست ممنون میشم راهنمایی کنید
-> تو ویس بالا جوابش هست"
"-> سلام استاد خسته نباشيد يك سوال داشتم اگر در سيستم كنترلي حمله رخ بده مثلا سيستم چهار تانك و خلل در عملكرد سيستم ايجاد بشه اگر بخوايم حمله تنها تشخيص داده بشه و در مراحل بالاتر بتونيم حمله رو كنترل كنيم در واقع منظوزم سايبر اتك هست چه شبكه عصبي و چه كليد واژه هايي رو پيشنهاد ميكنيد سرچ كنم ممنونم
-> سلام متاسفانه هیچ اطلاعاتی درمورد کار شما ندارم
-> ببینید شما باید توزیع آماری پکت های شبکه تون رو در حالت معمولی بدست بیارید مثلا یه توزیع نرمال میشه با میانگین و واریانس مشخص زمانی که حمله سایبری میشه تبادلات پکت ها دیگه از این توزیع پیروی نمی کنند بنابراین شما تشخیص آنومالی یا اختلال در شبکه میدین آنومالی دتکشن رو سرچ کنید و در موردش تحقیق کنید
-> خيلي ممنونن
-> متشكر"
"-> سلام دوستان چطور میتونم مدل ترین شده را بصورت real timeتوی وب کم تستش کنم مدل ترین شده در رابطه با object detection هست
-> 
-> درود بر شما استاد ممنون بابت توضیح هاتون بله من با پایتورچ کار کردم و اینکه با opencv کار میکنم با همون دستور videocapture میتونم وب کم لپ تاپ را باز کنم فقط نمیدونم مدلی که ترین کردم را چطور بارگذاری کنم سپاسگزارم
-> model define model modelload_state_dicttorchloadmodel_weight_path modeleval torchset_grad_enabledFalse کدها دقیق نیست خودتون اصلاح کنید
-> سپاسگزارم استاد"
"-> سلام یک سوال داشتم اگر در پیش بینی متن از dropout استفاده کنیم چه مشکلی پیش میاددر شبكه هاي حافظه ايا شدني هست
-> سلام مشکلی پیش نمیاد اتفاقا کار رایجی هم هست
-> متشكرم ميشه بپرسم چه تاثيري داره دقيقا استاد و چه ويژگي ايجاد ميكنه دقيقا
-> به صورت خلاصه دراپ اوت برای جلوگیری از اورفیت در شبکه های عصبی عمیق پیشنهاد شده بنابراین در شبکه ای مثل lstm که پارامتر زیادی داره میتونه عملکرد مثبتی داشته باشه
-> سپاس"
"-> سلام وقتتون بخیر دوستان استاد در بخش شبکه های gan میفرمایند یک مقاله هست برای اینکه دیسکریمنتیور میتواند به عنوان یک اینیشالیزیشن استفاده میشود نویسنده مقاله معرفی کردند
-> سلام خوش آمدید باید دوباره به ویدئوها نگاه کنم خاطرم نیست کدام مقاله منظورم بوده
-> خیلی ممنونم استاد ممنون استاد حدود 10300 از توضیحات میگذره آنجا داشتید کاربرد این دوتا شبکه به طور جدا را میگفتید
-> سلام استاد یه سوال داشتم هر دیتاست که cnnرو بخوایم روش اجرا کنیم مهم که cnn چه تعداد لایه هایی داشته باشه منظورم اینه که مثلا روی دیتاستMNIST با دو تا لایه CNNو یه لایه MAXPOOLINGاجرا بگیریم میشه رو دیتاستCIFAR10هم از همین CNNاستفاده کنیم و جواب خوبی بگیریم
-> 
-> 
-> صرفا برای اینکه با اون معماری در CIFAR10 به دقت بالاتری بتونه برسه"
"-> مسابقه تحلیل داده و یادگیری ماشین کارگزاری مفید زمان جمعه ۱۲ دی ۹۹ ساعت ۱۰ ثبتنام
-> سلام اگر وقت دارید در این نوع مسابقات شرکت کنید شرکت در این چالشها دستاوردهای بزرگی داره که جایزه مالی کوچکترین و کمارزشترین دستاوردش هست"
"-> با سلام خدمت اساتید گروه regression is a row of data without sequence sequence prediction or sequence regression are the same kind of thing You cannot accurately refer to sequence prediction or sequence regression as regression as an LSTM cannot be used for the latter but can be used for the former در اموزشی خونده بودم که رگرسیون یک ردیفی از داده بدون توالی است regression is a row of data without sequence منظور از این جمله چیه
-> سلام منظور این هست که رگرسیون به دادههای دنبالهای مثلا سری زمانی بورس فعالیت گسلها مربوط نمیشه برای اینها الگوریتمها و شبکههایی مثل lstm مناسب هست اما برای رگرسیون شبکهای مثل lstm کاربردی نداره
-> مثل همیشه عالی و ساده و روشن توضیح دادید سپاسگزارم"
"-> سلام وقت شما بخیر من میخواستم با کاربرد های پکیج پایتورچ در پردازش موازی آشنا بشم اگه ممکنه راهنمایی بفرمائید
-> سلام به گروه خوش اومدید من تخصصی در حوزه پردازش موازی ندارم نمیدونم پایتورچ چه کاربردی در این حوزه داره شاید عزیزان حاضر در گروه اطلاعات داشته باشند"
"-> سلام کسی هست روی تصاویر لیبل گذاری کرده باشه
-> سوال شما چی هست در مورد لیبل گذاری"
"-> سلام وبینار gan تون چی شد
-> سلام تو مرحله آماده سازیه مطالب هست بچه ها حوصلشون سر اومده دیگه ببخشید واقعا
-> بخدا"
"-> من همون کدی رو که در اموزش گفتید ران میکنم الان میخوام تست کنم و میخوام عکس تست بدم و خروجی رو بگیرم
-> من کد رو دوباره چک میکنم و دقیقتر به شما میگم لطفا پیامها در قالب یک پیام بنویسید ممنون
-> ممنونم"
"-> سلام به همگی جهت تست کردن شبکه یولو به خطای زیر میخورم بعد از ران کردن این کد load_ext tensorboard tensorboard logdir log The tensorboard module is not an IPython extension UsageError Line magic function tensorboard not found ممکنه راهنمایی کنید
-> "
"-> فقط اینکه فایل اکسل هست یعنی seprateor نداره
-> اگه فایلت csv هست یعنی با کاما از هم جدا شدن
-> اکسل هست xlsx با هر دوتاش تست کردم همین خطا را میداد"
"-> سلام چرا زمان خواندن فایل csv با panda خطای زیر را میده AttributeError TextFileReader object has no attribute head
-> کد رو بزار احتمال خطای سینتکسی داره کدت
-> data pdread_csvdatacsv dtypestr chunksize100 encodingISO88591 error_bad_linesFalse iteratorTrue printdatahead"
"-> سلام دوستان کسی تجربه انتخاب دوربین برای کاربرد در کامپیوتر ویژن رو داشته ممنون میشم با من در اشتراک بزاره
-> سلام برای چه پروژه و کاربردی ما برای چند پروژه تجاری از دوربینهای تحت شبکه اکسیس استفاده میکردیم
-> جهت نصب در کارگاه ساختمانی برای شناسایی وسایل ایمنی همراه کارگر مثل هارد هت عینک دستکش
-> دوربینهای تحت شبکه برای این کار گزینه مناسبی هست ارتباط دوربین با کامپیوتر از طریق شبکه هست ما از دوربینهای دو مگاپیکسلی استفاده میکردیم که احتمالا برای کاربرد شما هم مناسب باشه دوربینهای تحت شبکه مثلا اکسیس از قابلیت کنترل میزان روشنایی محیط هنگام تصویربرداری رو دارن
-> خیلی ممنون بابت پاسخ حتما بررسی میکنم البته الان دارم یه سری دوربین رو بر اساس استاندارد EMVA 1288 مقایسه میکنم اما به هرحال هیچی جای تجربه عملی رو نمیگیره
-> سلام استاد ممکنه توضیح بدید چطور میزان روشنایی محیط رو از طریق این نوع دوربین کنترل میکنید ایا روشنایی در بعضی از این دوربین ها قرار داده شده"
"-> یه سوال فنی و غیر مرتبط دارم دوستان وقتی مرورگرهامو باز میکنم سی پی یو کامپیوترم صد در صد کار میکنه ایا راهی هست بفهمم وقتی به اینترنت وصل میشم یا مرورگرهامو باز میکنم کامپیوترم به چه سایتهایی وصل میشه چون احتمال میدم یه ویروس منو وصل به یه سایت میکنه مثل سایتهای استخراج ارز دیجیتال
-> کروم استفاده میکنید
-> تمام مرورگرهام مشکل دارن کروم بیشتر مشکل داره
-> با دستور netstat توی پاورشل یا cmd میشه چک کرد یه tool هم هست به اسم CurrPorts اونم میشه باهاش چک کرد
-> ممنونم چک میکنم"
"-> سلام دوستان وقت بخیر من تازه دارم دیپ لرنینگ رو شروع میکنم طبق اموزش استاد پیش رفتم برای نصب پایتورچ ولی باین خطا در محیط cmdمواجه میشم کسی میتونه راهنمایی کنه ERROR Could not find a version that satisfies the requirement torch170cpu ERROR No matching distribution found for torch170cpu
-> نسخه های دیگه ای از پایتورچ رو امتحان کن
-> ممنون"
"-> استاد یه راه حلی میخواستم بگم برای سوال خودم نمیدونم اصلا حرفم درسته یا نه میشه مثلا دو تا شبکه رو هر کدوم جداگونه رو اون دیتاست اصلی عه اموزش بدیم بعد مدلها رو ذخیره کنیم بعد بیاییم مدلهای ذخیره شده رو قبل از قسمت ensemble لود کنیم حالا شبکه جدید رو که حاصل ترکیب اون دوتاست رو اونم بیاییم رو دیتاست اصلی اموزش بدیم
-> استاد میشه نظرتون رو در مورد این راه حل بگین این کار درسته ممنون
-> این اموزش را مطالعه فرمایید"
"-> سلام عزیزان گوگل کولب سرویس خوبی هست تلاش کنید از این سرویس برای کارهای دانشگاهیتون استفاده کنید اگر به جایی رسیدید که محدودیت زمانی کولب کار رو برای شما سخت کرده بهصورت جدی به گوگل کولب پرو فکر کنید این سرویس پولی هست ماهانه 10 دلار اگر خارج کشور از خانواده یا دوستان کسی رو دارید میتونید از اونها بخوایید که براتون کار خرید رو انجام بدن دقت کنید کولب پرو هم محدودیت زمانی نداره و هم پردازندههای قدرتمندتری داره شاید حتی با یکی دیگر از دوستانتون بتونید در هزینه شریک بشید و هردو استفاده کنید من شرکتی در داخل میشناسم که کارهای تجاری خودشون رو با کولب پرو جلو میبرن اگر به فضای ذخیرهسازی بیشتری نیاز دارید و 15 گیگ گوگل درایو برای شما کافی نیست میتونید از گیفت کارت گوگل پلی استفاده کنید و تا 100 گیگ با هزینه کمتر از یک میلیون تومن ارتقای حجم بدید ما این مورد رو تست نکردیم اما بررسی کردیم و بهنظر میرسه با گیفت کارت شدنی هست اگر در مورد کولب پرو و گوگل درایو اطلاعاتی دارید لطفا حتما در گروه به اشتراک بگذارید این روزها دوستان داخل کشور با مشکلات زیادی مواجه هستند باید به هم کمک کنیم
-> خیلی ممنون استاد از اطلاعات مفیدتون فک میکنم کولب پرو در حال حاضر امریکا و کانادا رو ساپورت میکنه من چند بار خواستم خرید کنم ولی همیشه با ارور روبرو میشدم اگر واقعا لینکیراهی برای ثبت نام کردن باشه ممنون میشم بفرمایین
-> سلام بله به نظر میرسه فعلا فقط برای کانادا و آمریکا دردسترس هست البته خرید باید از طریق این دو کشور انجام بشه ولی برای استفاده هرجای دنیا میشه استفاده کرد
-> سلام شاید بشه از طریق شرکت های واسط مثل پیمنت ۲۴ و خرید کرد ضمنا در کولب پرو هر ران کاری حداکثر تا ۲۴ ساعت میشه انجام داد و cpu و رم هم دوبرابر هستش البته شاید بشه یک crawler نوشت که هر ۱۲ ساعت که زمان کولب معمولی تمام شد فایل وزن ها را بطور اتوماتیک در کولب بارگذاری کند و فرایند اموزش را دوباره از سر گیرد تا اموزش تمام شود"
"-> سلام استاد وقتتون بخیر استاد وقتی دو تا شبکه رو endemble میکنیم باید اول هر کدوم رو جداگونه اموزش بدیم و در نهایت بعد از ترکیب و ایجاد یه مدل جدید حاصل از ترکیب اینا باید مدل جدید رو اموزش بدیم خواستم بدونم تو قسمت اول که دارم مدلها رو جدا اموزش میدم باید اونا رو هم رو همون دیتاست اصلی اموزش بدم چون دیتاست من ۸۰۰۰ تا تصویره بخوام اینکارو بکنم خیلییی زمان میبره خواستم مطمئن شم ایا این شکلی عه واقعا ممنون
-> تکنیکهای ensemble زیاد و متنوع هست یکی از این تکنیکها همون توضیحات شماست بله باید روی کل دیتاست آموزش بدید البته میتونید با دادههای کمتری آموزش رو انجام بدید نتایج رو بررسی کنید و درصورت مثبت بودن آموزش وقتگیر رو بذارید
-> الان درصدم عالی شده استاد منتها دو تا شبکه پری ترین رو باید قبلش اموزش بدم با همون ۸۰۰۰ تا بعد تازه بره سراغ اصلی فک کنم با گوگل کولب بخوام کار کنم نتونم تا اپوک بالا برم جلو با این حجم و با این محدودیت ران تایم راه اسونتری وجود نداره واقعا
-> پس باید روی کل داده آموزش بذارید یک راه استفاده از گوگل کولب پرو هست
-> رایگانه استاد"
"-> سلام دوستان چجوری میشه بعد یه لایه کانولوشن به داده ها دسترسی پیدا کرد
-> منظورتون اینه که خروجی اون لایه رو داشته باشین
-> بله داده ها رو بعد اینکه یک بار اون لایه روش اعمال شد
-> طبق چیزی که استاد گفتن باید از لایه ها لیست بسازین بعد ماژول بسازین ازشون که بتونین بعد از ورودی دادن خروجی هر لایه رو بدست بیارین
-> احسنت
-> ممنون
-> 
-> "
"-> نتایج دقتهای ولید و ترین رو توی یک ورد ذخیره کردم استاد برای هر اپوک ذخیره کردم نتایج رو
-> پس هیچی احتمالا نمیتونید معیارهای دیگر رو بدست بیارید
-> ینی باید دوباره ران بگیرم و کد رو درست کنم خیلی طولانیه ده ساعت طول میکشه
-> باید وزنهای بهترین نتایج رو ذخیره میکردید این کار ضروری هست
-> حق با شماست استاد باید این کار رو میکردم"
"-> سلام دوستان دوستان من یک شبکه ای رو ران ران کردم و این ران طولانی مدت بود الان نتایج ران رو دارم ایا راهی هست که بدون دوباره ران کردن و با داشتن نتایج معیارهای confusion و recall precision رو حساب کنم ممنون میشم اگه بلدین راهنمایی کنین
-> "
"-> استاد چشم انتظار وبینار GAN تون هستیمواقعا خیلی نیازهایشالا بزودی خبر برگزاریشو بدین ممنون
-> سلام بله مشغول جمعآوری مطالب یک کورس هستیم به همین خاطر هنوز وقت نشده تلاش میکنم در آذرماه برگزار بشه
-> ان شالاممنون از زحماتتون استاد"
"-> سلام استاد وقت بخیر من هرکاری میکنم تورچ درپایچرم نصب نمیشه واین خطا را نشان میده
-> سلام تصاویر رو نگاه کردم شما از دستور pip install torch برای نصب استفاده کردید اگر این کار رو انجام دادید باید بگم که راه درستی نیست به سایت pytorchorg برید و در همون صفحه اول گزینه install رو انتخاب کنید بعد دقیق سیستم عامل و سایر گزینه های مهم رو انتخاب کنید تا درنهایت به شما یک دستور برای نصب بده باید در نصب نسخه پایتون هم دقت کنید چون اگر نسخه های جدید پایتون مثلا 39 نصب کرده باشید ممکن هست نصب نشه درهرصورت به این روش نصب پایتون رو آزمایش کنید اگر مشکل حل نشد پیام بدید"
"-> سلام استاد وقت بخیر استاد وبینار Gan برگزار شد
-> سلام خیر"
"-> سلام استاد مباحث مربوط به memory neural network هم در كلاس يادگيري عميق پوشش دادع شده است
-> شبکههای بازگشتی در دو فصل توضیح داده شده
-> ممنونم"
"-> دوستان ایا شدنی عه رو جی پی یوی کولب مثلا دیتاست دو هزارتایی بذاریم ران بشه یا کولب از یه حدی بیشتر رو اجازه نمیده چون من امروز تا حدود 200 تا رو راحت ران کردماما بیشتر از اون رو نمیتونه ران کنهو جوابای عجیب غریب میده
-> به دیتا داده ارتباطی نداره محدودیت زمانی داره در یک شبانهروز حداکثر 12 ساعت که بنا به حجم استفاده و کار کشیدن از سختافزار ممکن هست از 12 ساعت کمتر بشه
-> اهانپس ممکنه بخاطر همین این اتفاق افتاده چون من از یه کد ساده پری ترین استفاده کردمبا 100 تا تصویر درست کار کرد دقت خیلی خوبیم دادولی وقتی دیتاست رو به دو هزار تا افزایش دادم سیستم بسختی کار کرد اخرشم جوابای خیلی بدی داددقتش به یک رسیددر حالیکه برای دیتاست اول که 200 تا بود حدود 96 درصد شد
-> خیر اگر زیاد استفاده کنید به شما gpu داده نمیشه نه اینکه نتایج اشتباه بده
-> من خیلی استفاده کردمطی چند روز گذشته تقریبا از صبح تا شب"
"-> انگار با انکر باکس حل میشه اگر امکان داره توضیح بدید ممنون میشم
-> لطفا در یک پیام متنها رو بنویسید چهار پیام شما در یک پیام میتونست نوشته بشه پیامهای متوالی برای اعضای گروه آزاردهنده هست بله با آنکورباکس این مشکل حل شده متاسفانه توضیحش در قالب پیام مشکل هست بهتر هست به آموزش یولوی هوسم یا پستهای معتبر خارجی مراجعه کنید
-> خیلی ممنون چشم حتما رعایت میشه از این به بعد"
"-> سلام ببخشيد كسي ميدونه براي داده هايي كه تداخل دارند در الگوريتم yolo_v2 چه راه كاري پيشنهاد ميشه
-> سلام تداخل یعنی چی
-> ینی مراکز عکس ها روی هم قرار بگیرند"
"-> سلام کارت گرافیک geforce gtx 1070 دارم ویژوال استادیو نصب کردم کودا را هم نصب کردم ولی وقتی دستور torchcudais_available را میزنم false بر میگردانه چکار دیگه ای لازم است انجام بدم
-> سلام برای نصب پایتورچ نیازی به نصب ویژوال استودیو نیست اگر هم بهعنوان IDE میخوایید ازش استفاده کنید پیشنهاد میکنم از vscode یا pycharm نصب کنید اگر gpu شما رو پیدا نمیکنه موارد زیر رو باید مدنظر داشته باشید آیا نسخه کودای پایتورچ رو نصب کردید آیا ورژن کودا در نسخه پایتورچ با ورژن کودای نصبی روی سیستم شما یکی هست آیا cudnn نصب کردید cudnn لایبرری شرکت Nvidia برای شبکههای عصبی هست باید این رو هم نصب کنید در انتخاب cudnn دقت کنید که با نسخه کودای نصبی رو ی سیستم عامل شما سازگار باشه"
"-> 
-> منواین لینکو دیدم جالب بود واسم انگار جایگزین کانولوشن میخاد کنه
-> این لینک منظورش ترنسفورمر بجای کانولوشن هست ترنسفورمرها در nlp تحول بزرگی ایجاد کردند در یک ماژول ترنسفورمر بخش مهمی بنام self attention وجود داره در یک سال اخیر در مقالاتی دیده شده که ترنسفورمرها وارد حوزه ویژن هم شدن سوالهایی مثل پایان عصر کانولوشن با ترنسفورمر مطرح هست احتمال داره این اتفاق بیفته لینک هم عالی بود ممنون در جلسه 9 دوره یادگیری عمیق خیلی خلاصه درباره ترنسفورمر توضیح داده شده
-> خواهش میکنم چون منم ویژن کار میکنم البته رگرسیون نه سگمنتالبته به نوعی سگمنت هم میشه اطلاق کرد استادم گفت این موضوع self attention به نظرم روی بهبود کارت تاثیر گزار هست ممنون از توضیحات شما حتی گاهی ترنسفر ها برابر با self attention در نظر گرفته میشه
-> اینم جالبه"
"-> سلام وقت دوستان بخیر Self attention چیه و چه کارایی داره جایگزین cnnهست یا میشه در بین لایه های کانولوشنی استفاده کرد
-> سلام یک ماژول هست جایگزین لایهای نمیشه بین لایهها لایه آخر و بهصورت خلاصه هرجایی که محقق صلاح بدونه هدف ماژولهای اتنشن کمک به شبکه برای توجه به نقاط مهم و متمرکزتر شدن روی نواحی برجسته هست
-> خیلی ممنون اثر گدار هم هست مثلا در بخش بندی و بدست آوردن جزئیات بیشتر
-> بله اثرگذار هست در عین اضافه کردن مقدار کمی پارامتر عملکرد شبکه رو بهتر میکنند در حوزه سگمنت به مقالات باید نگاه کنید دیدم که در سگمنت هم از اتنشن استفاده میشه
-> سپاسگزارم"
"-> و توی این شکل تعداد پارامترهای سمت راست اوکیه ولی شکل سمت چپی از کجا اومد این عکس صفحه ی بعد عکس قبلیه امیدوارم متوجه شده باشید
-> 
-> ممنون بابت توضیحات کامل تون متوجه شده بودم تعداد چنل هارو و داخل ویدیو ها هم کامل گفته شده بود مشکلم همون عکس اولی بود که فرمودین معادل نیستن و بقیه مشکلاتم هم حل شد سپاس"
"-> سلام دوستان من توی این تصویر متوجه نمیشم چطور شکل سمت راست با شکل سمت چپ معادل همند توی سمت چپی اگه اشتباه نکنم 64 تا و سمت راستی 256 تا صفحه خروجی داریم چطور معادل هم شدند
-> "
"-> سلام وبینار gan برگزار شد
-> سلام برگزار نشده هنوز
-> ممنون ببخشید مبخواستم بپرسم استاد قبل از faster rcnn وبیناری به اسم رگرسیون داشتن ممنون
-> وبینار آشنایی با پایتورچ داشتیم کل آموزش با رگرسیون خطی بود ویدیو تو کانال هست"
"-> سلام دوستان من یه سوال داشتم ممنون میشم کمکم کنید من میخوام دیتاست mnist که load کردم و کلاسیفایر میکنم جای داده ها ثابت باشن مثلا اگه داده x_test0 3 هست دفعه بعدی هم که ران میکنم 3 بمونه چیکار باید بکنم
-> random seed برای seed یک عدد ثابت درنظر بگیرید تا همیشه یک مجموعه اعداد تصادفی ثابت تولید بشه
-> سلام استاد وقت بخیر ببخشید میشه در مورد seedبیشتر توضیح بدین من اینجا خوب متوجه کد nprandomseed selfseed نمیشم که چرا در مقدار دهی اولیه ازش استفاده میکنیم با توجه به این کد ی که من دارم
-> خیلی ممنون استاد"
"-> سلام استاد استاد من با یک تعداد ثابت و محدود دیتاست وقتی مثلا میام و لایه fc یه شبکه رو تغییر میدم مثه کاری که شما تو ویدیو انجام دادین درصد دقتم خوبه ولی وقتی میام دو تا شبکه رو ترکیب میکنم درصد دقت میاد پایین ایا این فقط بخاطر اینه که تو حالت دوم شبکه پیچیده تر شده و اون تعداد محدود داده که تو حالت اول استفاده شد برای حالت دوم کمه که این اتفاق میافته یا اصلا نه چیز دیگه ای که اینجوری میشه ممنون میشم پاسخ بدین
-> دو شبکه با معماری مختلف رو روی یک دیتاست آموزش میدید و خروجی رو ترکیب میکنید فقط در مرحله ارزیابی ترکیب میکنید یا در مرحله آموزش هم اینکار رو میکنید چطوری ترکیب میکنید
-> دو شبکه رو توی قسمت کلاس خروجیهاشونو کانکت میکنم مثه ویدئو که شما یاد دادین بعد این خروجبهای کانکت شده که سایز هردوشون ۵۱۲ هست رو تو همون کلاس به یه لایه fc میدم که به دو کلاس تبدیل کنه بعد بیرون کلاس این مدل رو به یک مدل نهایی به اسم model میدم و بعدش این مدل نهایی رو اموزش میدم رو همون دیتاست کمی که دارماین دقتش به نسبت حالتی که مثلا میام فقط لایه fc یه شبکه رو عوض میکنم دقت کمتری داره دقت تستشم خب به نسبت خیلی کمتره توقعم چیز دیگه ای بود فک میکردم چیز بهتری میشه"
"-> داره کم میشه ولی با سرعت کم ولی هر دو LOSS در خال کم شدن هستن فک کنم باید LR رو بیشتر کنم
-> با lr schedule کار کنید
-> ممنون بله بله"
"-> آموزش epoch 026 train loss 698 ppl 1075e03 lr 25e01 clip 01 num_updates 3250 ولیدیشن epoch 026 valid loss 7027 ppl 1127e03 lr 25e01 clip 01 num_updates 3250 الان شبکه من در حال آموزش دیدن هست و الان داره خوب آموزش میبیه امیدوارم تا آخرش همینجوری پیش بره بتونیم مدل اصلی رو از حدود 10 گیگ متن استخراج کنیم درود
-> سلام نمودار لاس رو از ابتدا ندارید لرنینگ ریت همش 025 هست سرعت کاهش لاس چقدر هست
-> جناب اشرفی این کد کد اصلی نیست روی حدود 200 مگ دیتا داره کار رو انجام بده البته تو کد اصلی باید با شرط میزان LR رو متغیر کنیم نمودار لاس رو نکشیدم ولی از epoch اول میزان loss حدود 9 بود و روند کاهشی رو داره و دارم رصد میکنم"
"-> سلام دوستان برای یادگیری ماشین کتاب ساده خودآموز میتونید معرفی کنید لطفا
-> Hands on machine learning V2
-> تشکر این کتاب تو ایران چاپ شده ترجیجا کتاب چاپی میخوام"
"-> البته نتونستم رو سیستم خودم ران کنم چون رم جواب نمیداد دارم منتقل میکنم رو کولب
-> حواستون باشه کولب با تنسورفلو 2 هست احتمالا لازم هست روی کولب تنسورفلو 1 نصب کنید
-> اقای اشرفی الان مشکلم اینه که تو کولپ فایل وزن ها اپلود نمیشن من هموز موفق نشدم خروچی کد رو ببینم تو اپک اول رم میره رو صددرضد
-> فردا ساعت 9 شب باهم صحبت تلفنی داشته باشیم که ببینم مشکل چی هست
-> خیلی ممنونم ازتون"
"-> File EAI_2YOLOhosam3Codeskerasyolo3yolo3modelpy line 394 in yolo_loss _ ignore_mask Kcontrol_flow_opswhile_looplambda bargs bm loop_body 0 ignore_mask AttributeError module kerasbackend has no attribute control_flow_ops سلام دوستان تو اجرای کد yolo تشخیصابجکت راکون به این ارور برخوردم ممنون میشم راهنمایی کنید
-> سلام نسخه تنسورفلو و کراس رو لطفا بگید
-> این ارور ممکن مال نسخه کراس باشه یه تجربه که داشتم این بود من با نسخه قدیمی collections کار کردم و بعد کدی که قدیمی بود رو آوردم روی پایتون جدید اجرا کردم مثل همین خطا بعضی از attribute ها عوض شده بود باید یه سرچ کنید و ببینید تو نسخه جدید این attribute به چه اسمی تغییر نام داده
-> سلام اقای دکتر مورد این بود
-> اره دقیقا
-> الان ورژن پیاتون و تنسورفلو و کراستون چی هستش
-> ورژن پایتون 37 هست
-> سلام برای اجرای کد trainpy در الگوریتم yolo من تونستم این ارورها را برطرف بکنم توی آناکوندا اول باید یک env با پایتون نسخه ی 35 درست بکنید که برای من ورژن پایتونم 356 هستش بعد تنسورفلو نسخه ی 190 رو نصب کنید و بعد کراس نسخه ی 222 رو نصب کنید بعد از این کار به اروری مربوط به تداخل نسخه ی numpy و تنسورفلو برمیخوریم که در این مرحله باید اول numpy رو حذف کنید و بعد نسخه ی 1164 رو نصب کنید برای اجرای این کد همچنین باید دو کتابخانه ی pillow و yolo3 را نیز در این env نصب کنید در این مرحله کار تمومه و کد بدون هیچ اروری اجرا میشه
-> سلام ممنون از راهنمایی تون amin کد توی model Py بجای control_flow_ops عوض کردم با tf while_loop درست شد ولی سخت افزار سیستم من نمیتونه بکشه"
"-> سلام به همگی دوتا سوال از بخش سوم یادگیری عمیق دارم ۱ در بخش تئوری ها گفتین انداره padding بر اساس اندازه kernel تعیین میشه اینجا اندازه کرنل ۳ هست و طبق فرمول 1 213 چرا اندازه padding دو گرفتین ۲ با در نظر گرفتن padding دو سایز تصویر ۲۸۲۸ بوده که میشه ۳۰۳۰ به خاطر stride دو اندازه تصویر در خروجی لایه اول ۱۵۱۵ میشهدر لایه دوم به خاطر padding دو اندازه تصویر میشه ۱۷۱۷ و به خاطر stride دو ابعادش نصف میشه برابر ۸۵ این 88 چه جوری بدست میاد
-> 
-> خیلی ممنون متوجه شدم"
"-> سلام استادوقتتون بخیر استاد خواستم بپرسم امکانش هست بعد از وبینار GAN یه وبینار هم برای بحث دیتا آگمنتیشن بذارین من به شخصه خیلی با این قضیه آشنا نیستم و فقط میدونم خیلی الان برای کار با دیپ لرنینگ اکثرا نیاز دارن و مقاله های زیادیم در این زمینه داده شده ممنون میشم اگه وقتشو داشتین برای این موضوع هم وبینار ارائه بدین تشکر
-> سلام پیشنهاد جالبی هست برای برگزاری وبینارها نیاز به کمک داریم تهیه مطالب زمانبر هست همچنین باید از موضوع استقبال هم بشه لطفا مقالههایی که دیدید رو بفرستید ممنون
-> چشم استادمیفرستم اگه برای وبینار کمکی از دست من برمیاد بفرمایید خوشحال میشم بتونم کمکی کرده باشم که شما هم بتونید تو زمان کمتری وبینار این موضوع رو هم ارائه بدین چون واقعا مهمه ممنون
-> سلام وبینار کی هست
-> سلام زمانش هنوز مشخص نیست فعلا در مرحله تهیه مطالب هستیم
-> استاد مقالات دیتا اگمنتیشن رو براتون ارسال کردم
-> ممنون"
"-> یه سوال دیگه
-> لطفا اینطوری تکه تکه پیام ندید بچههای گروه کلافه میشن ممنون بله امکانش هست
-> چشم یعنی اینکه تمام سولات رو تو یک مثن فقط تو ی گروه قرار دهم
-> بله چت کردن در گروه باعث میشه بچهها گروه رو mute کنن و دیگه کسی گروه رو چک نمیکنه
-> چشم ببخشید دوستان"
"-> سلام دوستان در رابطه با اموزش yolov3 مربوط به راکون ها سوالی داشتم
-> سلام بله وزنهای pretrain رو دانلود کنید طبق آموزش یولو پیش برید"
"-> سلام امروز به یک مشکلی در فهم rnn برخوردم منابعی که مطالعه کردم هیچ کدوم به روانی تدریس استاد اشرفی نبودکاملا مشکلم حل شد
-> اقای اشرفی کارش درسته
-> سلام الحمدلله ممنون
-> 
-> واقعا ایشون آدم شریفی هستن
-> لطف دارید"
"-> سلام وقتتون بخیر من در اجرای فایل trainpy برای الگوریتم یولو به ارور هایی خوردم که بعد از جستجو متوجه شدم این ارور ها برای عدم تطابق ورژن پایتون و keras و tensorflow هستش لطفا راهنمایی بفرمائید که از چه ورژنی از اینها استفاده بکنم
-> سلام آموزش براساس کراس سازگار با تنسورفلو 1 تهیه شده تنسورفلو 2 هنوز بیرون نیومده بود الان نسخه پیش فرض تنسورفلو 2 هست نسخهای از کراس رو نصب کنید که با تنسورفلو 1 سازگار باشه در آموزش جایی نشون ندادم که نسخه کراس چند هست کسی از دوستان نسخه کراس رو در آموزش یولو میدونه
-> خیلی ممنون لطف کردید یعنی دقیقا از tf با ورژن 100 استفاده کنم
-> مهم نیست دقیقا 100 باشه نسخههای مختلفی از 10 وجود داره مثلا 110 رو تست کنید درهرصورت نسخه 20 نباید باشه چون نسخه 20 با 10 تفاوتهای زیادی داره
-> خیلی ممنون"
"-> سلام وقت بخیر میخواستم بپرسم اگه بخواییم از شبکه های pretrained که عمدتا ورودیشون سه کاناله هست برای تصاویر سطح خاکستری که تک کاناله هستن استفاده کنیم چه راه حلی پیشنهاد مبشه که بشه این نوع تصاویر رو هم بتونیم به این شبکه ها بدیم ممنون
-> تصویر رو به روشی که در گروه پردازش تصویر گفتم سه کاناله کنید و به شبکه بدید
-> اها دقبقا دنبال همین بودم استاد ممنونفک کردم شاید من استنباطم اشتباه باشه ممنون"
"-> دوستان من دارمروی پروژ تشخیص مدل خودرو برند رنگ و تعداد ان در تصویر توسطyolo کار میکنم اگر کسی ازدوستان تو اینزمینه فعالیت میکنه خوشحال میشم بتونیم باهمپیش ببریم کار رو
-> سلام دیتاست دارین
-> سلام واقعیت هنوز پیگیرشنشدم ولی اینجوری که پرسیدم واسه ماشین های خارجی دیتا ست هست"
"-> سلام من امین هستم خیلی ممنون از دعوت تون به گروه خوب تون
-> سلام خوش اومدید"
"-> سلام به همگی من یه سوالی داشتم کتابخونه های مورد نیازمو با cmd نصب کردم اما موقع import کردن تو پایچارم کلا نشون نمیده که نصب شده وباید از ترمینال پایچارم مجددا دستورات pip install رو بزنم تا کتابخونه ها رو بتونم import کنم و برای هر پروژه جدیدی که تعریف میکنم باید اینکار انجام بدم راهی وجود داره که هر بار اینکار انجام ندم
-> سلام چند تا پیام بالا رو بخونید توی تنظیمات پایچارم و تعریف interpreter اشتباهی رخ داده"
"-> Shell which python داخل کد که با پایچارم اجرا می کنید printsysexecutable اگر خروجی این دو دستور متفاوت هستند باید اینترپرتر پایتون توی پایچارم رو تغییر بدید
-> ممنونم از راهنمایی تون دقیقا مشکل همین بود ک شما فرمودین خیلی لطف کردین"
"-> سلام و وقت بخیر اگر بخواهم شبکه عصبی را جوری ترین کنم که نسبت به تشخیص اشتباه بیمار به سالم حساس تر باشه ولی نسبت به تشخیص اشتباه سالم به بیمار حساسیت کمتری داشته باشه منظورن بایاس کردن هست چجوری باید کد را بنویسیم داریم عکسهای کسانی را تشخیص میدیم که بیماری پوست دارند یا ندارند اگر کسی در زمینه بایاس کردن نتیجه به کمک لاس فانکشن تجربه ای دارد ممنون میشم با من به اشتراک بگذارد
-> سلام من در زمینه بایاس کردن تجربهای ندارم شما یک مدل آموزش دیده دارید که کار کلاسبندی در دو کلاس رو انجام میده اهداف شما با تکنیکهای ارزیابی یادگیری ماشین هم قابل حصول هست خروجی کار یک امتیاز بین 0 تا 1 هست به سمت 0 یعنی بیماری نداره و به سمت 1 یعنی بیماری داره اما آستانه کجاست 05 در نظر گرفتید این آستانه رو باید طبق معیارهای خودتون تنظیم کنید نیاز هست نگاهی به معیارهای ارزیابی recall precision بندازید این ارزیابیها در فاز تجاری خیلی دقیق و جدی انجام میشه
-> ببخشید این آستانه منظورتون احتمال خروجی شبکه عصبی هست مثلا Dnn class10۴ Dnn class 2 0۶ جواب کلاس دو است چون احتمال بیشتری دارد اما شما منظورتون این است که آستانه را بیشتر از نیم بگذاریم مثلا بگذاریم بزرگتر از ۰۶
-> بله اما باید این آستانه رو طی ارزیابی رو دادههای مختلف بدست بیارید مثلا ممکن هست که این مقدار برای شما حتی تا 08 انتخاب بشه منبعی ندارم اما کمی سرچ کنید
-> من روی uncertainty و شبکه های بیزین کار می کنم یکی از روش هایش تعیین استانه برای تشخیص نمونه uncertain هست خیلی ممنون شما همیشه علی رغم وقت کمی که دارید پشتیبان من بودید"
"-> سلام دوستان روی نسخه 364 نصب شد ممنون از آقای Amin rah mani
-> خواهش میکنم"
"-> امتحان کنید فک نکنم مشکلی داشته باشه
-> شما امتحان کردین
-> آره من پایتورچ رو روی ۳۶۴ نصب کردم مشکلی نداره
-> ممنون الان امتحان میکنم
-> 
-> ممنون
-> سلام آقای اشرفی ممنون از راهنماییتون میشه آناکوندا در کنار پایتروچ نصب کرد
-> بله
-> ممنون اخه یه سری کارهای نیمه تموم دارم که با اناکوندا کار کردم
-> اینکار انجام بدین
-> ممنونم
-> سلام من طبق ویس استاد پیش رفتم ولی باز هم نشد نمی دونم درست مراحل رو انجام دادم یا خیر
-> اینجا این قسمت ابی رو کپی کردم توی مرورگر
-> ببخشید میشه ببینید مشکلم کجاست طبق عکس ها پیش رفتم سیستمم هم 64 بیتی هست"
"-> روی ۳۶
-> بله ممکن هست به نسخه پایتون ربط داشته باشه هرچند میبینم که باید روی 36 و 37 و 38 نصب بشه 39 قطعا نصب نمیشه
-> ممنون"
"-> از مدیر خواهش میکنم یه ویدیوی برای حل کردن این مشکل تهیه بفرماین
-> نسخه پایتون نصبی رو لطفا بگید
-> هم با نسخه داخل ویدویو ۳۷۱ امتحان کردم و هم با آخرین نسخه ۳۹"
"-> پردازش گفتار هم اگر در برنامه قرار بدین عالیه
-> واقعیت این هم جز برنامه هست اما در اولویت پایینتری نسبت به cv و nlp هست بابت پیشنهادهای ارزنده از همه عزیزان تشکر میکنم"
"-> خیلی عالیه من به شخصه منتظر بینایی ماشین هستم جز بینایی ماشین کورس های دیگری هم هست استاد و اگر هست چه کورس هایی هست احتمالا
-> سلام شاید پردازش متن هم برگزار بشه توکل بر خدا
-> سلام دوره nlp هم بتونین برگذار کنین خیلی خوبه
-> بسیار عالی اگر پردازش متن باشه خیلی عالی هست
-> ممنون انشالله بتونیم دورههای با کیفیت برگزار کنیم
-> شما جزو معدود اساتیدی هستید که کلاسها را باکیفیت برگزار می کنید و بیشتر به کیفیت دقت میکنید تا به کمیت اینکه اگر دوره ای برگزار بشه مطمین هستم عالی میشه باز هم تشکر از همکاری شما
-> من که همین الان پولشو گذاشتم کنار
-> ممنون لطف دارید
-> توکل بر خدا
-> "
"-> من هم میخوام شرکت کنم ولی از مصاحبه میترسم چون دوره های قبلی را هنوز خوب کار نکردم
-> آهان نکته همینجاست بچهها در دورهها شرکت میکنند اما خروجی معلوم نیست دوره آنلاین یا حضوری باید برای کسانی باشه که کامل روی دوره تمرکز کنند و وقت بگذارند اگر کسی علاقه به مطالعه خودخوان داشته باشه طبیعتا پکیج آفلاین منابسش هست
-> اینحوری شما هم مثل اون جلسه قبل از GAN ناراحت و دلخور نمیشید ازاینکه چرا با وحود زحمات شماما کارنکرده بودیم
-> بله متاسفانه البته تعداد افراد هم باید کم بشه که در جریان افراد مختلف در طول هفته باشیم"
"-> سلام وقت بخیر استاد یه بار فرمودین قرار هست کورس پیشرفته پردازش تصویر رو برگزار کنین این کورس برگزار نمیشه
-> اگه بشه عالیهمنم میخوام
-> سلام ممنون استقبال شما دلگرم کننده هست احتمالا بینایی کامپیوتر منظور شما هست انشالله سال جدید 1400 دورههای جدید بهصورت آنلاین برگزار میکنیم انشالله اولین مورد بینایی کامپیوتر هست ولی سبک برگزاری کلاسها مثل دورههای آنلاین گذشته نخواهد بود فعلا تصمیم بر این هست که با ظرفیت بسیار کم 10 نفر برگزار بشه و با هر فردی قبل شرکت در کلاس مصاحبه بشه باید مطمئن باشیم افرادی که در دورههای آنلاین شرکت میکنند زمان کافی برای وقت گذاشتن در هفته و کمی دانش اولیه دارند باید انقدر دورهها رو با کیفیت برگزار کنیم که افراد حاضر در دورهها با گذروندن چند دوره در شرکت خوبی شغل پیدا کنند البته مشتاق شنیدن پیشنهادها و انتقادهای شما هستیم
-> حالا با این شرایط فکر نمیکنم کسی شرکت کنه
-> ببخشید بنظرم باید اموزش برای همه نوع افراد باشه نه اینکه گلچین بکنید چون شاید بعضی ها نخواهند شرکت کار کنند و برای مقاله و تز دانشگاه شون نیاز داشته باشند بنظرم کمی عمومی تر ببینید بهتره باتشکر از همراهی همیشه شما
-> حالا اگه قبول نشدیم ویدیوهاشو بهمون نمیدین
-> ناقص گفتم منظورم فقط شرکت نبود منظورم دانشگاه چه داخل چه خارج و سایر مسائل مرتبط هست
-> قبول نشید آنلاین نمیتونید شرکت کنید اما خب طبیعتا دوره آفلاین روی سایت قرار میگیره و میتونید خودتون تهیه کنید و برید جلو ولی طبیعتا افرادی که در دوره آنلاین باشن مزایای بسیار بیشتری خواهند داشت به همین خاطر تعداد رو کم در نظر میگیریم
-> بسیار عالیزمانشم خیلی خوبهتااونموقع فرصت هست تمرین کنیم قبول شیم تومصاحبه
-> معیارهای مهم مصاحبه 1 زمان کافی در طول هفته برای دوره 2 دانش اولیه مثلا در کورسی مثل کامپیوتر ویژن نباید با کدنویسی ساده پایتون مشکل داشته باشه یا نتونه خطای نصب پایتون پایچارم رو رفع کنه 3 الزام به حضور منظم در کلاسها بخوابم بعدا آفلاینشو میبینم نداریم 4 ما واقعا درحال برنامه ریزی و تنظیم چنین قوانینی هستیم
-> خیلی هم عالی استاد یه تایم دقیق میگین که ما از الان دوباره کورس قبلی رو مرور کنیم و خودمون رو برسونیم
-> انشالله از اردیبهشت شروع میکنیم زمان زیادی هم نمونده خوبه که بحثهای دیپ و پردازش تصویر مسلط شید انشالله در 1400 دوره جامع متنوع زیاد خواهیم داشت"
"-> شبیشو تو این مقاله دیدم
-> مقاله مربوط به همین پایان نامه است"
"-> بنظرم جالب اومدمقایسه فیلدهای مختلف رو یک جا با ترسیم شکل نشون داده
-> سلام ممنون مقالهاش رو هم دارید
-> سلام استادپایان نامه استمقاله نبود
-> چه بهتر امکان ارسال وجود نداره"
"-> سلام دوستان وقتتون بخیر دوستان من به روش ویئوی 5 استاد یه شبکه جدید درست کردم که قبل از fc خروجی 512 تایی دو شبکه پری تریند vgg19 و resent18 رو میگیره با هم کانکت میکنه و میده به لایه fcدرصد ولیدیشن خیلی خوبه 100 ولی تو تست درصد بسیااار پایینه50 درصدینی به هیچ دردی نخوردخواستم نظرتون رو بدونم دلیلش چیه من فک میکردم این کار میتونه ایده خوبی برای بالا بردن دقت باشه ولی بدتر شداینم اضافه کنم البته که دیتاستم کمهمثلا در حدود 40 تا تصویرممکنه علتش این باشه
-> سلام بنطرم هم تعداد عکسهای پایگاه داده شما کم هست و یا داده های تست با داده های ولیدیشن تفاوت زیادی دارند
-> سلام بله بنظرم از روش data augmentation استفاده کنید شاید بهتر شه نتیجتون اینطوری داده های train بالا میره و شبکه اموزش بهتری میبینه
-> دیتای اصلیم بیشتر از اینهمنتها گفتم با یه دیتای کم شروع کنمکد اولیه با همین تعداد دقتش درصد بالایی شدولی وقتی خودم شبکه رو تغییراتی بهش دادمو از ترکیب خروجی دو شبکه پری تریند استفاده کردم به ۶۰ درصد رسیدو این برام سوال بود که با یه دیتاست مشخص یه کد انقدر درصد خوبی داد یکی هم انقدر بد
-> ممنون"
"-> سلام دوستان وقتی کولب پیام لیمیت GPU میده دیگه اون حساب کارش تمومه
-> سلام برای همون روز آره ولی فرداش باز gpu بهتون میده اگه کارتون ضروریه یه حساب گوگل دیگه باز کنین و با اون وارد بشین
-> ممنون"
"-> سلام من یه سوالی داشتم اگه من لایه های یه شبکه پری تریند رو به لایه های یه شبکه پری تریند شده دیگه الحاق کنم باید مدل نهایی بدست اومد از این اتصالات جدید رو دوباره ترین کنم ینی خودم باید از اول وزنهای مدل شبکه جدید رو بدست بیارمیا چون از شبکه پری تریند دارم استفاده میکنم لازم نیست دیگه
-> سلام بله باید train بشه"
"-> سلام اینکه در مقالات یک روش پیشنهادی را با چندین روش موجود دیگر مقایسه میکنند اون کدهای روشهای دیگر را از کجا میشه پیدا کردتا این مقایسه ها را با تصاویر پایگاه داده خودمان پیاده سازی کنیم
-> لازم نیست شما کد مقالات دیگه رو داشته باشید تا مقایسه کنیدمقایسه بین مقالات از طریق نتایج بخش Results انجام میشهکافیه جداول و نمودارهای مقالات دیگه رو چک کنیدمعیاراشونو و پارامترهای مختلفی که بر اساس اونها نتایجشون رو اعلام کردنشمام خروجی و نتایج کارتون رو با اونها مقایسه کنید همین
-> خیلی ممنونم از اینکه وقت گذاشتید و پاسخ دادید ولی اگر کد را در اختیار داشته باشیم بنظرم راحتتر میشه ایده های جدید را روی کد نوشته شده پیاده سازی کرد بالاخره بقیه که مقاله ای را منتشر کرده اند حتما کد را خودشان ننوشته اندچون کار بسیار وقت گیری است
-> خب این حرفتون الان چیز دیگه ایه اره خب این یه چیز کلی عه اگه کد اماده باشه میتونیم راحتتر و تو زمان کمتری ایدمونرو پیاده سازی کنیمو البته با دیتاست خودمون"
"-> سلام در الگوریتم شبکه عصبی کانولوشن اون فیلتر چه جوری طراحی میشه اگه بخوام منم فیلتر طراحی کنم چیا باید یاد بگیرم آیا هر تسکی فیلتر خاص خودشو میخواد
-> پست آموزش شبکه عصبی کانولوشنی توی سایت رو خوندید فکر کنم جواب سوالاتونو بگیرین توی وبلاگ هست
-> داشتم فیلم استاد اشرفی رو می دیدم به همینجا رسیدم گفتم سوال بپرسم الان میرم میخونم الان که ویدیو رفت جلو فک کنم جواب این باشه که فیلتر ها رو میایم وزن میذاریم و خودش در فرایند پس انتشار خطا میاد وزنها آپدیت میشه و فیلتر مناسب ساخته میشه درسته الان اون پستم میبینم
-> پس دیگه وبلاگ رو نخونید دقیقا درست متوجه شدین
-> خیلی خیلی ممنونم از راهنمایی تون"
"-> اگر هنوز کنجکاو هستین و ابهام کامل برطرف بشه
-> خیلی خیلی ممنون از لطفتون"
"-> خب ابهامتون چیه
-> ممنون تشکر حل شد"
"-> فیلم های کلاس پایتورچ رو دیدینقشنگ این قسمت هارو توضیح دادند استاد اشرفیبنظرم یه نگاه به جلسه ۴و ۵ پایتورچ بندازین
-> بله دیدم ابهام من چیز دیگه ای که اینو پرسیدم بازم ممنون میشم کسی میدونه راهنمایی کنه"
"-> من یه کدی که خودش در واقع از قبل یه شبکه خیلی ساده داشت رو ران کردم درست جواب دادشبکه هم از pretrainedvgg19 استفاده میکردبعد که من خواستم جای اون شبکه یه شبکه cnn بذارم تو این دو تا عکس به خطا خوردمچه جوری رفعش کنم
-> از این تصاویر نمیشه قطعی نظر داد خطا نشون میده که ابعاد بردار ویژگی در شبکه جدید با لایه فولی کانکتد سازگار نیست
-> پس ینی باید سایز تصویر رو تغییر بدم یا پارامترهای شبکه رو"
"-> سلام دوستان وقتتون بخیر دوستان چه جوری میتونم یه شبکه ای تعریف کنم که برای هر سایز ورودی قابل اجرا باشه و در ادامه بخشهای دیگه کد با خطا مواجه نشم ممنون
-> اصولا شبکههای کانولوشنی مستقل از اندازه ورودی کار میکنن ولی باید بدونید که فیچرمپ خروجی شبکه رو میخوایید چیکار کنید چون سایز سطر و ستون فیچرمپها متفاوت متفاوت میشه
-> ینی میشه تو ورودی عدد مشخصی نداد"
"-> اگر اندازه دیتا ست بزرگ باشه بهتره روی سی پی یو لود بشه بعد حین ترینینگ روی جی پی یو منتقل بشه اگر از پایتورچ استفاده می کنین و از کلاس دیتا لودر سوییچ truepin_memory رو اگر ست کنین این انتقال سریع تر انجام میشه
-> ممنون از توضیحاتتون تشکر"
"-> data datatocuda Check the device using printdatadevice
-> هومماوکیممنون"
"-> من الان هر چی تلاش میکنم تو اون قسمت خط قرمز رنگ تصویری که خودم تو کولب قسمت Files دارم رو بهش بدم همش خطای 28 رو میده الان چی کار کنم
-> با pwd ببینید current directory کجاست قاعدتا باید بنویسید content"
"-> مشکل خطای summary گرفتن از شبکه م چیه
-> نباید تنسور ورودی چهاربعدی باشه
-> استاد متوجه نمیشم دقبقا ینی چی چی کار باید بکنم
-> تصاویر ورودیتون ۳۲ در ۳۲
-> فکر میکنم ورودی باید به شکل 133233 باشه
-> from torchsummary import summary summaryyour_model input_sizechannels H W
-> شما یجا mismatch داری باید با دیباگ پیداش کنی و ورودی صحیح را بدی به summary
-> نه الان تغییرش دادم به 224224 ولی کلا قاطی کرد برنامهRAM هم پر شددیگه کار نمیکنه
-> ممنون استاد اگه بشه تست میکنم دوباره"
"-> سلام دوستان علت خطای این شبکه چیه چرا ایراد داره
-> ویرگول آخر ماکس پولینگ نگذاشتید"
"-> سلام دوستان کسی تجربه کار کردن باpoint Cloud و BoNet رو داره
-> 
-> سلاممن با pointcloud کارکردمولی با شبکه pointnet"
"-> سلام و درود روش تبدیل کردن مدل پایتورچ به تنسرفلو چی هست آیا ممکن بعد از تبدیل دقت مدل هم تغییر کنه
-> سلام میشه از onnx یا mmdnn استفاده کنید شدنی هست اما معمولا کمدردسر نیست
-> با onnx میشود انجام داد ولی کم دردسر نیست به قول دوستمون در ضمن بدانید که ممکن است خروجی شما با خروجی تبدیل شده متفاوت باشد این هم بدلیل quantize نشدن مدل بعد از تبدیل می باشد
-> سپاس جناب اشرفی
-> درود"
"-> وقتتون بخیر دوستان از طریق یولو و ترکر SORT میخواستم اشیا رو ترک کنم ولی bounding box ها خیلی off هستند اصلا دقیقا در نمیاد کسی میتونه لطفا ایرادم رو بگه از torch استفاده میکنم
-> سلام یعنی چی off هستند
-> سلام باندین باکس ها دور اجسام نمیافتند دور تر هستند نمونه تصویر زیر لطفا به قسمت سمت چپ تصویر توجه کنید
-> به کدهاتون نگاه کردم متاسفانه ایرادی ندیدم چند تا مشکل رایجش این هست که مختصات xy رو برای کشیدن باکس برعکس به دستور opencv داده باشید عدد باکسها متناسب با تصویر ریسایز شده باشه و به درستی روی تصویر اصلی نگاشت نداره این قبیل مشکلات معمولا زیاد رخ میده باکس نسبت به سایز ماشین کوچکتر هست این نشون میده که مختصات باکس برمبنای یک تصویر دیگه ای بدست اومده
-> متشکرم بابت پاسختون"
